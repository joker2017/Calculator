{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Inception TL and cash fin 90%.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joker2017/Calculator/blob/master/Mobilenet_v2_KKKKKKKKKKKKKKKKKKK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "6BPCjWiVMPh4",
        "colab_type": "code",
        "outputId": "1c3accc6-090b-4593-83d0-41b2520f59d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/tensorflow/models"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'models' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rzNx01j6MWK1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/models/research/slim')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yl0_vUGm2U_x",
        "colab_type": "code",
        "outputId": "a5d09a1d-0fc4-47d9-9661-03a6a6f05ba5",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7Ci8vIE1heCBhbW91bnQgb2YgdGltZSB0byBibG9jayB3YWl0aW5nIGZvciB0aGUgdXNlci4KY29uc3QgRklMRV9DSEFOR0VfVElNRU9VVF9NUyA9IDMwICogMTAwMDsKCmZ1bmN0aW9uIF91cGxvYWRGaWxlcyhpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IHN0ZXBzID0gdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKTsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIC8vIENhY2hlIHN0ZXBzIG9uIHRoZSBvdXRwdXRFbGVtZW50IHRvIG1ha2UgaXQgYXZhaWxhYmxlIGZvciB0aGUgbmV4dCBjYWxsCiAgLy8gdG8gdXBsb2FkRmlsZXNDb250aW51ZSBmcm9tIFB5dGhvbi4KICBvdXRwdXRFbGVtZW50LnN0ZXBzID0gc3RlcHM7CgogIHJldHVybiBfdXBsb2FkRmlsZXNDb250aW51ZShvdXRwdXRJZCk7Cn0KCi8vIFRoaXMgaXMgcm91Z2hseSBhbiBhc3luYyBnZW5lcmF0b3IgKG5vdCBzdXBwb3J0ZWQgaW4gdGhlIGJyb3dzZXIgeWV0KSwKLy8gd2hlcmUgdGhlcmUgYXJlIG11bHRpcGxlIGFzeW5jaHJvbm91cyBzdGVwcyBhbmQgdGhlIFB5dGhvbiBzaWRlIGlzIGdvaW5nCi8vIHRvIHBvbGwgZm9yIGNvbXBsZXRpb24gb2YgZWFjaCBzdGVwLgovLyBUaGlzIHVzZXMgYSBQcm9taXNlIHRvIGJsb2NrIHRoZSBweXRob24gc2lkZSBvbiBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcCwKLy8gdGhlbiBwYXNzZXMgdGhlIHJlc3VsdCBvZiB0aGUgcHJldmlvdXMgc3RlcCBhcyB0aGUgaW5wdXQgdG8gdGhlIG5leHQgc3RlcC4KZnVuY3Rpb24gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpIHsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIGNvbnN0IHN0ZXBzID0gb3V0cHV0RWxlbWVudC5zdGVwczsKCiAgY29uc3QgbmV4dCA9IHN0ZXBzLm5leHQob3V0cHV0RWxlbWVudC5sYXN0UHJvbWlzZVZhbHVlKTsKICByZXR1cm4gUHJvbWlzZS5yZXNvbHZlKG5leHQudmFsdWUucHJvbWlzZSkudGhlbigodmFsdWUpID0+IHsKICAgIC8vIENhY2hlIHRoZSBsYXN0IHByb21pc2UgdmFsdWUgdG8gbWFrZSBpdCBhdmFpbGFibGUgdG8gdGhlIG5leHQKICAgIC8vIHN0ZXAgb2YgdGhlIGdlbmVyYXRvci4KICAgIG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSA9IHZhbHVlOwogICAgcmV0dXJuIG5leHQudmFsdWUucmVzcG9uc2U7CiAgfSk7Cn0KCi8qKgogKiBHZW5lcmF0b3IgZnVuY3Rpb24gd2hpY2ggaXMgY2FsbGVkIGJldHdlZW4gZWFjaCBhc3luYyBzdGVwIG9mIHRoZSB1cGxvYWQKICogcHJvY2Vzcy4KICogQHBhcmFtIHtzdHJpbmd9IGlucHV0SWQgRWxlbWVudCBJRCBvZiB0aGUgaW5wdXQgZmlsZSBwaWNrZXIgZWxlbWVudC4KICogQHBhcmFtIHtzdHJpbmd9IG91dHB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIG91dHB1dCBkaXNwbGF5LgogKiBAcmV0dXJuIHshSXRlcmFibGU8IU9iamVjdD59IEl0ZXJhYmxlIG9mIG5leHQgc3RlcHMuCiAqLwpmdW5jdGlvbiogdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKSB7CiAgY29uc3QgaW5wdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQoaW5wdXRJZCk7CiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gZmFsc2U7CgogIGNvbnN0IG91dHB1dEVsZW1lbnQgPSBkb2N1bWVudC5nZXRFbGVtZW50QnlJZChvdXRwdXRJZCk7CiAgb3V0cHV0RWxlbWVudC5pbm5lckhUTUwgPSAnJzsKCiAgY29uc3QgcGlja2VkUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBpbnB1dEVsZW1lbnQuYWRkRXZlbnRMaXN0ZW5lcignY2hhbmdlJywgKGUpID0+IHsKICAgICAgcmVzb2x2ZShlLnRhcmdldC5maWxlcyk7CiAgICB9KTsKICB9KTsKCiAgY29uc3QgY2FuY2VsID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnYnV0dG9uJyk7CiAgaW5wdXRFbGVtZW50LnBhcmVudEVsZW1lbnQuYXBwZW5kQ2hpbGQoY2FuY2VsKTsKICBjYW5jZWwudGV4dENvbnRlbnQgPSAnQ2FuY2VsIHVwbG9hZCc7CiAgY29uc3QgY2FuY2VsUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBjYW5jZWwub25jbGljayA9ICgpID0+IHsKICAgICAgcmVzb2x2ZShudWxsKTsKICAgIH07CiAgfSk7CgogIC8vIENhbmNlbCB1cGxvYWQgaWYgdXNlciBoYXNuJ3QgcGlja2VkIGFueXRoaW5nIGluIHRpbWVvdXQuCiAgY29uc3QgdGltZW91dFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgc2V0VGltZW91dCgoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9LCBGSUxFX0NIQU5HRV9USU1FT1VUX01TKTsKICB9KTsKCiAgLy8gV2FpdCBmb3IgdGhlIHVzZXIgdG8gcGljayB0aGUgZmlsZXMuCiAgY29uc3QgZmlsZXMgPSB5aWVsZCB7CiAgICBwcm9taXNlOiBQcm9taXNlLnJhY2UoW3BpY2tlZFByb21pc2UsIHRpbWVvdXRQcm9taXNlLCBjYW5jZWxQcm9taXNlXSksCiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdzdGFydGluZycsCiAgICB9CiAgfTsKCiAgaWYgKCFmaWxlcykgewogICAgcmV0dXJuIHsKICAgICAgcmVzcG9uc2U6IHsKICAgICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICAgIH0KICAgIH07CiAgfQoKICBjYW5jZWwucmVtb3ZlKCk7CgogIC8vIERpc2FibGUgdGhlIGlucHV0IGVsZW1lbnQgc2luY2UgZnVydGhlciBwaWNrcyBhcmUgbm90IGFsbG93ZWQuCiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gdHJ1ZTsKCiAgZm9yIChjb25zdCBmaWxlIG9mIGZpbGVzKSB7CiAgICBjb25zdCBsaSA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2xpJyk7CiAgICBsaS5hcHBlbmQoc3BhbihmaWxlLm5hbWUsIHtmb250V2VpZ2h0OiAnYm9sZCd9KSk7CiAgICBsaS5hcHBlbmQoc3BhbigKICAgICAgICBgKCR7ZmlsZS50eXBlIHx8ICduL2EnfSkgLSAke2ZpbGUuc2l6ZX0gYnl0ZXMsIGAgKwogICAgICAgIGBsYXN0IG1vZGlmaWVkOiAkewogICAgICAgICAgICBmaWxlLmxhc3RNb2RpZmllZERhdGUgPyBmaWxlLmxhc3RNb2RpZmllZERhdGUudG9Mb2NhbGVEYXRlU3RyaW5nKCkgOgogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAnbi9hJ30gLSBgKSk7CiAgICBjb25zdCBwZXJjZW50ID0gc3BhbignMCUgZG9uZScpOwogICAgbGkuYXBwZW5kQ2hpbGQocGVyY2VudCk7CgogICAgb3V0cHV0RWxlbWVudC5hcHBlbmRDaGlsZChsaSk7CgogICAgY29uc3QgZmlsZURhdGFQcm9taXNlID0gbmV3IFByb21pc2UoKHJlc29sdmUpID0+IHsKICAgICAgY29uc3QgcmVhZGVyID0gbmV3IEZpbGVSZWFkZXIoKTsKICAgICAgcmVhZGVyLm9ubG9hZCA9IChlKSA9PiB7CiAgICAgICAgcmVzb2x2ZShlLnRhcmdldC5yZXN1bHQpOwogICAgICB9OwogICAgICByZWFkZXIucmVhZEFzQXJyYXlCdWZmZXIoZmlsZSk7CiAgICB9KTsKICAgIC8vIFdhaXQgZm9yIHRoZSBkYXRhIHRvIGJlIHJlYWR5LgogICAgbGV0IGZpbGVEYXRhID0geWllbGQgewogICAgICBwcm9taXNlOiBmaWxlRGF0YVByb21pc2UsCiAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgYWN0aW9uOiAnY29udGludWUnLAogICAgICB9CiAgICB9OwoKICAgIC8vIFVzZSBhIGNodW5rZWQgc2VuZGluZyB0byBhdm9pZCBtZXNzYWdlIHNpemUgbGltaXRzLiBTZWUgYi82MjExNTY2MC4KICAgIGxldCBwb3NpdGlvbiA9IDA7CiAgICB3aGlsZSAocG9zaXRpb24gPCBmaWxlRGF0YS5ieXRlTGVuZ3RoKSB7CiAgICAgIGNvbnN0IGxlbmd0aCA9IE1hdGgubWluKGZpbGVEYXRhLmJ5dGVMZW5ndGggLSBwb3NpdGlvbiwgTUFYX1BBWUxPQURfU0laRSk7CiAgICAgIGNvbnN0IGNodW5rID0gbmV3IFVpbnQ4QXJyYXkoZmlsZURhdGEsIHBvc2l0aW9uLCBsZW5ndGgpOwogICAgICBwb3NpdGlvbiArPSBsZW5ndGg7CgogICAgICBjb25zdCBiYXNlNjQgPSBidG9hKFN0cmluZy5mcm9tQ2hhckNvZGUuYXBwbHkobnVsbCwgY2h1bmspKTsKICAgICAgeWllbGQgewogICAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgICBhY3Rpb246ICdhcHBlbmQnLAogICAgICAgICAgZmlsZTogZmlsZS5uYW1lLAogICAgICAgICAgZGF0YTogYmFzZTY0LAogICAgICAgIH0sCiAgICAgIH07CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPQogICAgICAgICAgYCR7TWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCl9JSBkb25lYDsKICAgIH0KICB9CgogIC8vIEFsbCBkb25lLgogIHlpZWxkIHsKICAgIHJlc3BvbnNlOiB7CiAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgIH0KICB9Owp9CgpzY29wZS5nb29nbGUgPSBzY29wZS5nb29nbGUgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYiA9IHNjb3BlLmdvb2dsZS5jb2xhYiB8fCB7fTsKc2NvcGUuZ29vZ2xlLmNvbGFiLl9maWxlcyA9IHsKICBfdXBsb2FkRmlsZXMsCiAgX3VwbG9hZEZpbGVzQ29udGludWUsCn07Cn0pKHNlbGYpOwo=",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "  \n",
        "print( os.listdir('/content') )"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-63548614-1e16-4de4-b38b-e694a1bc6711\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-63548614-1e16-4de4-b38b-e694a1bc6711\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving imagenet_class_names.txt to imagenet_class_names (1).txt\n",
            "User uploaded file \"imagenet_class_names.txt\" with length 31674 bytes\n",
            "['.config', 'mobilenet_v2_1', 'imagenet_class_names (1).txt', 'flower_photos', 'models', 'imagenet_class_names.txt', 'sample_data']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JeO02mGVr6VA",
        "colab_type": "code",
        "outputId": "1d7a935a-a459-4d56-f8cf-d25c845ef370",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#Загрузите последнюю предварительную модель Inception v3: контрольная точка доступна по адресу https://github.com/tensorflow/models/tree/master/research/slim . \n",
        "#писок имен классов доступен по адресу https://goo.gl/brXRtZ , но вы должны сначала вставить «background» класс.\n",
        "import os\n",
        "import re \n",
        "import sys\n",
        "import tarfile\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from random import sample\n",
        "from six.moves import urllib\n",
        "import matplotlib.image as mpimg\n",
        "import tensorflow.contrib.slim as slim\n",
        "#from tensorflow.contrib.slim.nets import inception\n",
        "from nets.mobilenet import mobilenet_v2\n",
        "from skimage.transform import rescale, resize, downscale_local_mean\n",
        "\n",
        "MODEL_NAME = \"mobilenet_v2_1.0_224.ckpt\"\n",
        "CLASS_NAME_REGEX = re.compile(r\"^n\\d+\\s+(.*)\\s*$\", re.M | re.U)\n",
        "DATASET_URL = \"http://download.tensorflow.org/example_images/flower_photos.tgz\"\n",
        "MODEL_URL = \"https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_1.0_224.tgz\"\n",
        "\n",
        "\n",
        "\n",
        "def download_progress(count, block_size, total_size):\n",
        "    percent = count * block_size * 100 // total_size\n",
        "    sys.stdout.write(\"\\rDownloading: {}%\".format(percent))\n",
        "    sys.stdout.flush()\n",
        "    \n",
        "\n",
        "def download_tgz(url):\n",
        "    file_name=os.path.basename(url)\n",
        "    name_parts = os.path.split(file_name)\n",
        "    _, name = name_parts\n",
        "    name_path = name.rsplit( \".\", 2 )[ 0 ]\n",
        "    path = os.path.join(\"/content\", name_path)\n",
        "    tgz_path = os.path.join(path, name)\n",
        "    if os.path.exists(path): #and os.path.isfile(tgz_path):\n",
        "      print(\"file and path exist \", tgz_path, \"Path is:\", os.listdir(path), \"!\")\n",
        "      return path\n",
        "    else:\n",
        "      print(os.path.exists(path), os.path.isfile(tgz_path), path, tgz_path)\n",
        "      os.makedirs(path, exist_ok=True)\n",
        "      urllib.request.urlretrieve(url, tgz_path, reporthook=download_progress)\n",
        "      tmp_tgz = tarfile.open(tgz_path)\n",
        "      tmp_tgz.extractall(path=path)\n",
        "      tmp_tgz.close()\n",
        "      os.remove(tgz_path)\n",
        "      print(\"path created:\", path)\n",
        "      return path\n",
        "    \n",
        "# Загружаем модель\n",
        "MODELS_PATH = download_tgz(MODEL_URL)\n",
        "# Загружаем датасет\n",
        "FLOWERS_PATH = download_tgz(DATASET_URL)\n",
        " \n",
        "\n",
        "def load_class_names():\n",
        "    with open(os.path.join(\"/content\", \"imagenet_class_names.txt\"), \"rb\") as f:\n",
        "        content = f.read().decode(\"utf-8\")\n",
        "        #sys.stdout.flush()\n",
        "        return CLASS_NAME_REGEX.findall(content)\n",
        "      \n",
        "load_class_names()    \n",
        "class_names = [\"background\"] + load_class_names()       \n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "file and path exist  /content/mobilenet_v2_1/mobilenet_v2_1.0_224.tgz Path is: ['mobilenet_v2_1.0_224.ckpt.meta', 'mobilenet_v2_1.0_224.tflite', 'mobilenet_v2_1.0_224.ckpt.index', 'mobilenet_v2_1.0_224_info.txt', 'mobilenet_v2_1.0_224_frozen.pb', 'mobilenet_v2_1.0_224_eval.pbtxt', 'mobilenet_v2_1.0_224.ckpt.data-00000-of-00001'] !\n",
            "file and path exist  /content/flower_photos/flower_photos.tgz Path is: ['flower_photos'] !\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CptxC-7lYhEg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "9d508bab-bf71-4f32-a060-4fd738f4139b"
      },
      "cell_type": "code",
      "source": [
        "kernel_size[0]"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-d2afed2835a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkernel_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'kernel_size' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "TV7umITUZQAZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "64721565-8d02-4390-ac0f-1469fb057b46"
      },
      "cell_type": "code",
      "source": [
        "5//1"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "metadata": {
        "id": "EaCnXGYLK0gX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import contextlib\n",
        "from functools import partial\n",
        "def _fixed_padding(inputs, kernel_size, rate=1):     \n",
        "  print(kernel_size)\n",
        "  kernel_size_effective = [kernel_size[0] + (kernel_size[0] - 1) * (rate - 1),\n",
        "                           kernel_size[0] + (kernel_size[0] - 1) * (rate - 1)]\n",
        "  pad_total = [kernel_size_effective[0] - 1, kernel_size_effective[1] - 1]\n",
        "  pad_beg = [pad_total[0] // 2, pad_total[1] // 2]\n",
        "  pad_end = [pad_total[0] - pad_beg[0], pad_total[1] - pad_beg[1]]\n",
        "  padded_inputs = tf.pad(inputs, [[0, 0], [pad_beg[0], pad_end[0]],\n",
        "                                  [pad_beg[1], pad_end[1]], [0, 0]])\n",
        "  return padded_inputs\n",
        "\n",
        "\n",
        "def _make_divisible(v, divisor, min_value=None):\n",
        "  if min_value is None:\n",
        "    min_value = divisor\n",
        "  new_v = max(min_value, int(v + divisor / 2) / divisor * divisor)\n",
        "  # Make sure that round down does not go down by more than 10%.\n",
        "  if new_v < 0.9 * v:\n",
        "    new_v += divisor\n",
        "  return new_v\n",
        "\n",
        "def expand_input_by_factor(n, divisible_by=8):                                     ##\n",
        "  return lambda num_inputs, **_: _make_divisible(num_inputs * n, divisible_by)\n",
        "\n",
        "\n",
        "\n",
        "def expanded_conv(input_tensor):\n",
        "  \n",
        "    num_outputs=320\n",
        "    expansion_size=expand_input_by_factor(6)\n",
        "    stride=1,\n",
        "    rate=1,\n",
        "    kernel_size=(3, 3),\n",
        "    residual=True,\n",
        "    normalizer_fn=None,\n",
        "    project_activation_fn=tf.identity,\n",
        "    split_projection=1,\n",
        "    split_expansion=1,\n",
        "    expansion_transform=True,\n",
        "    depthwise_location = 'output',\n",
        "    depthwise_channel_multiplier=1,\n",
        "    endpoints=None,\n",
        "    use_explicit_padding=False,\n",
        "    padding='SAME',\n",
        "    scope=None\n",
        "#  with tf.variable_scope(scope, default_name='expanded_conv') as s, \\\n",
        "#       tf.name_scope(s.original_name_scope):\n",
        "    prev_depth = input_tensor.get_shape().as_list()[3]\n",
        "    \n",
        "    if use_explicit_padding:\n",
        "      if padding != 'SAME':\n",
        "        #raise TypeError('`use_explicit_padding` should only be used with '\n",
        "                       # '\"SAME\" padding.')\n",
        "        print(\"`use_explicit_padding` should only be used with  SAME padding.\")\n",
        "      padding = 'VALID'\n",
        "    depthwise_func = partial(\n",
        "        slim.separable_conv2d,\n",
        "        num_outputs=None,\n",
        "        kernel_size=kernel_size,\n",
        "        depth_multiplier=depthwise_channel_multiplier,\n",
        "        stride=stride,\n",
        "        rate=rate,\n",
        "        normalizer_fn=normalizer_fn,\n",
        "        padding=padding,\n",
        "        scope='depthwise')\n",
        "    # b1 -> b2 * r -> b2\n",
        "    #   i -> (o * r) (bottleneck) -> o\n",
        "    input_tensor = tf.identity(input_tensor, 'input')\n",
        "    net = input_tensor\n",
        "\n",
        "    if depthwise_location == 'input':\n",
        "      if use_explicit_padding:\n",
        "        net = _fixed_padding(net, kernel_size, rate)\n",
        "      net = depthwise_func(net, activation_fn=None)\n",
        "\n",
        "    if callable(expansion_size):\n",
        "      inner_size = expansion_size(num_inputs=prev_depth)\n",
        "    else:\n",
        "      inner_size = expansion_size\n",
        "\n",
        "    if inner_size > net.shape[3]:\n",
        "      net = split_conv(\n",
        "          net,\n",
        "          inner_size,\n",
        "          num_ways=split_expansion,\n",
        "          scope='expand',\n",
        "          stride=1,\n",
        "          normalizer_fn=normalizer_fn)\n",
        "      net = tf.identity(net, 'expansion_output')\n",
        "    if endpoints is not None:\n",
        "      endpoints['expansion_output'] = net\n",
        "\n",
        "    if depthwise_location == 'expansion':\n",
        "      if use_explicit_padding:\n",
        "        net = _fixed_padding(net, kernel_size, rate)\n",
        "      net = depthwise_func(net)\n",
        "\n",
        "    net = tf.identity(net, name='depthwise_output')\n",
        "    if endpoints is not None:\n",
        "      endpoints['depthwise_output'] = net\n",
        "    if expansion_transform:\n",
        "      net = expansion_transform(expansion_tensor=net, input_tensor=input_tensor)\n",
        "    #Обратите внимание, что в отличие от расширения, у нас всегда есть проекция для получения желаемого выходного размера.\n",
        "    net = split_conv(\n",
        "        net,\n",
        "        num_outputs,\n",
        "        num_ways=split_projection,\n",
        "        stride=1,\n",
        "        scope='project',\n",
        "        normalizer_fn=normalizer_fn,\n",
        "        activation_fn=project_activation_fn)\n",
        "    if endpoints is not None:\n",
        "      endpoints['projection_output'] = net\n",
        "    if depthwise_location == 'output':\n",
        "      if use_explicit_padding:\n",
        "        net = _fixed_padding(net, kernel_size, rate)\n",
        "      net = depthwise_func(net, activation_fn=None)\n",
        "\n",
        "    if callable(residual):  # custom residual/ таможенный остаток\n",
        "      net = residual(input_tensor=input_tensor, output_tensor=net)\n",
        "    elif (residual and\n",
        "          # проверка шага гарантирует, что мы не добавляем остатки, когда пространственные измерения отсутствуют\n",
        "          stride == 1 and\n",
        "          # Depth matches/Глубина совпадений\n",
        "          net.get_shape().as_list()[3] ==\n",
        "          input_tensor.get_shape().as_list()[3]):\n",
        "      net += input_tensor\n",
        "    return tf.identity(net, name='output')\n",
        "\n",
        "\n",
        "def split_conv(input_tensor,\n",
        "               num_outputs,\n",
        "               num_ways,\n",
        "               scope,\n",
        "               divisible_by=8,\n",
        "               **kwargs):\n",
        "\n",
        "  b = input_tensor.get_shape().as_list()[3]\n",
        "\n",
        "  if num_ways == 1 or min(b / num_ways,\n",
        "                          num_outputs / num_ways) < divisible_by:\n",
        "    # Don't do any splitting if we end up with less than 8 filters\n",
        "    # on either side.\n",
        "    return slim.conv2d(input_tensor, num_outputs, [1, 1], scope=scope, **kwargs)\n",
        "\n",
        "  outs = []\n",
        "  input_splits = _split_divisible(b, num_ways, divisible_by=divisible_by)\n",
        "  output_splits = _split_divisible(\n",
        "      num_outputs, num_ways, divisible_by=divisible_by)\n",
        "  inputs = tf.split(input_tensor, input_splits, axis=3, name='split_' + scope)\n",
        "  base = scope\n",
        "  for i, (input_tensor, out_size) in enumerate(zip(inputs, output_splits)):\n",
        "    scope = base + '_part_%d' % (i,)\n",
        "    n = slim.conv2d(input_tensor, out_size, [1, 1], scope=scope, **kwargs)\n",
        "    n = tf.identity(n, scope + '_output')\n",
        "    outs.append(n)\n",
        "  return tf.concat(outs, 3, name=scope + '_concat')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b9npxX0lYp1k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "8a227c9b-2a23-4536-c910-d65c005b617f"
      },
      "cell_type": "code",
      "source": [
        "kernel_size[0]"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-d2afed2835a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkernel_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'kernel_size' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "i2T09-MCLOyx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def global_pool(input_tensor, pool_op=tf.nn.avg_pool):\n",
        "\n",
        "  shape = input_tensor.get_shape().as_list()\n",
        "  if shape[1] is None or shape[2] is None:\n",
        "    kernel_size = tf.convert_to_tensor(\n",
        "        [1, tf.shape(input_tensor)[1],\n",
        "         tf.shape(input_tensor)[2], 1])\n",
        "  else:\n",
        "    kernel_size = [1, shape[1], shape[2], 1]\n",
        "  output = pool_op(\n",
        "      input_tensor, ksize=kernel_size, strides=[1, 1, 1, 1], padding='VALID')\n",
        "  # Recover output shape, for unknown shape.\n",
        "  output.set_shape([None, 1, 1, None])\n",
        "  return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Jwe1UXvnqIK2",
        "colab_type": "code",
        "outputId": "096447ef-d9cf-437d-a070-c123b5744c45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        }
      },
      "cell_type": "code",
      "source": [
        "width = 224 #299\n",
        "height = 224 #299\n",
        "channels = 3\n",
        "import tensorflow as tf\n",
        "tf.reset_default_graph()\n",
        "n_epochs = 1000\n",
        "batch_size = 40\n",
        "best_loss_val = np.infty\n",
        "check_interval = 500\n",
        "checks_since_last_progress = 0\n",
        "max_checks_without_progress = 20\n",
        "best_model_params = None \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Каждый подкаталог flower_photos каталога содержит все изображения данного класса. Давайте получим список классов:\n",
        "flowers_root_path = os.path.join(FLOWERS_PATH, \"flower_photos\")\n",
        "flower_classes = sorted([dirname for dirname in os.listdir(flowers_root_path)\n",
        "                  if os.path.isdir(os.path.join(flowers_root_path, dirname))])\n",
        "\n",
        "#Давайте получим список всех путей файлов изображений для каждого класса:\n",
        "\n",
        "from collections import defaultdict\n",
        "image_paths = defaultdict(list)\n",
        "for flower_class in flower_classes:\n",
        "    image_dir = os.path.join(flowers_root_path, flower_class)\n",
        "    for filepath in os.listdir(image_dir):\n",
        "        if filepath.endswith(\".jpg\"):\n",
        "            image_paths[flower_class].append(os.path.join(image_dir, filepath))\n",
        "            \n",
        "            \n",
        "#Давайте сортируем пути изображения, чтобы заставить этот ноутбук вести себя последовательно на нескольких запусках:\n",
        "for paths in image_paths.values():\n",
        "    paths.sort() \n",
        "\n",
        "    \n",
        "#Для получения дополнительных функций манипуляции с изображениями, таких как вращения, проверьте документацию SciPy или эту приятную страницу .\n",
        "from scipy.misc import imresize\n",
        "def prepare_image(image, target_width = width, target_height = height, max_zoom = 0.2):\n",
        "    \"\"\"Zooms and crops the image randomly for data augmentation.\"\"\"\n",
        "\n",
        "    # First, let's find the largest bounding box with the target size ratio that fits within the image\n",
        "    height = image.shape[0]\n",
        "    width = image.shape[1]\n",
        "    image_ratio = width / height\n",
        "    target_image_ratio = target_width / target_height\n",
        "    crop_vertically = image_ratio < target_image_ratio\n",
        "    crop_width = width if crop_vertically else int(height * target_image_ratio)\n",
        "    crop_height = int(width / target_image_ratio) if crop_vertically else height\n",
        "        \n",
        "    # Now let's shrink this bounding box by a random factor (dividing the dimensions by a random number\n",
        "    # between 1.0 and 1.0 + `max_zoom`.\n",
        "    resize_factor = np.random.rand() * max_zoom + 1.0\n",
        "    crop_width = int(crop_width / resize_factor)\n",
        "    crop_height = int(crop_height / resize_factor)\n",
        "    \n",
        "    # Next, we can select a random location on the image for this bounding box.\n",
        "    x0 = np.random.randint(0, width - crop_width)\n",
        "    y0 = np.random.randint(0, height - crop_height)\n",
        "    x1 = x0 + crop_width\n",
        "    y1 = y0 + crop_height\n",
        "    \n",
        "    # Let's crop the image using the random bounding box we built.\n",
        "    image = image[y0:y1, x0:x1]\n",
        "\n",
        "    # Let's also flip the image horizontally with 50% probability:\n",
        "    if np.random.rand() < 0.5:\n",
        "        image = np.fliplr(image)\n",
        "\n",
        "    # Now, let's resize the image to the target dimensions.\n",
        "    image = imresize(image, (target_width, target_height))\n",
        "    \n",
        "    # Finally, let's ensure that the colors are represented as\n",
        "    # 32-bit floats ranging from 0.0 to 1.0 (for now):\n",
        "    return image.astype(np.float32) / 255\n",
        "\n",
        "\n",
        "def prepare_batch(flower_paths_and_classes, batch_size):\n",
        "    batch_paths_and_classes = sample(flower_paths_and_classes, batch_size)\n",
        "    images = [mpimg.imread(path)[:, :, :channels] for path, labels in batch_paths_and_classes]\n",
        "    prepared_images = [prepare_image(image) for image in images]\n",
        "    X_batch = 2 * np.stack(prepared_images) - 1 # Inception expects colors ranging from -1 to 1\n",
        "    y_batch = np.array([labels for path, labels in batch_paths_and_classes], dtype=np.int32)\n",
        "    return X_batch, y_batch \n",
        "  \n",
        "  \n",
        "def prepare_data(flower_paths_and_classes):\n",
        "    batch_paths_and_classes = flower_paths_and_classes\n",
        "    images = [mpimg.imread(path)[:, :, :channels] for path, labels in batch_paths_and_classes]\n",
        "    prepared_images = [prepare_image(image) for image in images]\n",
        "    X_data = 2 * np.stack(prepared_images) - 1 # Inception expects colors ranging from -1 to 1\n",
        "    y_data = np.array([labels for path, labels in batch_paths_and_classes], dtype=np.int32)\n",
        "    return X_data, y_data  \n",
        "  \n",
        "  \n",
        "#раннняя остановка\n",
        "def get_model_params():\n",
        "    gvars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
        "    return {gvar.op.name: value for gvar, value in zip(gvars, tf.get_default_session().run(gvars))}\n",
        "\n",
        "def restore_model_params(model_params):\n",
        "    gvar_names = list(model_params.keys())\n",
        "    assign_ops = {gvar_name: tf.get_default_graph().get_operation_by_name(gvar_name + \"/Assign\")\n",
        "                  for gvar_name in gvar_names}\n",
        "    init_values = {gvar_name: assign_op.inputs[1] for gvar_name, assign_op in assign_ops.items()}\n",
        "    feed_dict = {init_values[gvar_name]: model_params[gvar_name] for gvar_name in gvar_names}\n",
        "    tf.get_default_session().run(assign_ops, feed_dict=feed_dict)  \n",
        "  \n",
        "  #снова получить начальный график v3. На этот раз воспользуемся trainingзаполнителем\n",
        "  \n",
        "X = tf.placeholder(tf.float32, shape=[None, height, width, channels], name=\"X\")\n",
        "training = tf.placeholder_with_default(False, shape=[])\n",
        "\n",
        "with slim.arg_scope(mobilenet_v2.training_scope()): #is_training=False\n",
        "  logits, end_points = mobilenet_v2.mobilenet(X, is_training=training)\n",
        "\n",
        "#with slim.arg_scope(inception.inception_v3_arg_scope()):\n",
        "    #logits, end_points = inception.inception_v3(X, num_classes=1001, is_training=training)\n",
        "\n",
        "inception_saver = tf.train.Saver()\n",
        "\n",
        "\n",
        "#изучить входные данные выходного уровня:\n",
        "# logits.op.inputs[0].op.inputs[0]  или end_points\n",
        "#end_points[\"PreLogits\"]\n",
        "\n",
        "#отказаться от 2-го и 3-го измерений с помощью tf.squeeze() функции\n",
        "#prelogits = tf.squeeze(end_points[\"global_pool\"], axis=[1, 2])\n",
        "layer_17 = end_points[\"layer_17/output\"]\n",
        "\n",
        "\n",
        "#prelogits = end_points[\"global_pool\"]\n",
        "#добавить окончательный полностью подключенный слой поверх этого слоя:  \n",
        "n_outputs = len(flower_classes)\n",
        "\n",
        "#op(ops.expanded_conv, stride=1, num_outputs=320),\n",
        "#op(slim.conv2d, stride=1, kernel_size=[1, 1], num_outputs=1280)\n",
        "\n",
        "with tf.name_scope(\"new_output_layer\"):\n",
        "  \n",
        "    layer_18 = expanded_conv(layer_17)\n",
        "                 # num_outputs=320,\n",
        "                 # expansion_size=conv_blocks.expand_input_by_factor(6),\n",
        "                 # stride=1,\n",
        "                 # rate=1,\n",
        "                 # kernel_size=None,\n",
        "                 # residual=True,\n",
        "                 # normalizer_fn=None,\n",
        "                 # project_activation_fn=tf.identity,\n",
        "                 # split_projection=1,\n",
        "                 # split_expansion=1,\n",
        "                 # expansion_transform=None,\n",
        "                 # depthwise_location='output',\n",
        "                 #depthwise_channel_multiplier=1,\n",
        "                 # endpoints=None,\n",
        "                 # use_explicit_padding=False)\n",
        "                 # padding = 'SAME')\n",
        "    global_pool = mobilenet.global_pool(layer_18)\n",
        "    dropout = slim.dropout(global_pool, scope='Dropout', is_training=is_training)\n",
        "    flower_logits = tf.layers.dense(dropout, n_outputs, name=\"flower_logits\")\n",
        "\n",
        "    \n",
        "    Y_proba = tf.nn.softmax(flower_logits, name=\"Y_proba\")\n",
        "    \n",
        "\n",
        "y = tf.placeholder(tf.int32, shape=[None])\n",
        "\n",
        "with tf.name_scope(\"train\"):\n",
        "    print(\"logits shape\", flower_logits.get_shape().as_list(), \"label shape\", y.shape )\n",
        "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=flower_logits, labels=y)\n",
        "    #xentropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=flower_logits, labels=y)\n",
        "    loss = tf.reduce_mean(xentropy)\n",
        "    batch = tf.Variable(0, trainable=False)\n",
        "    \n",
        "\n",
        "    optimizer = tf.train.AdamOptimizer()\n",
        "    flower_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"flower_logits\") #flower_logits\n",
        "    training_op = optimizer.minimize(loss, var_list=flower_vars)\n",
        "    \n",
        "\n",
        "with tf.name_scope(\"eval\"):\n",
        "    correct = tf.nn.in_top_k(flower_logits, y, 1)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
        "\n",
        "with tf.name_scope(\"init_and_save\"):\n",
        "    init = tf.global_variables_initializer()\n",
        "    saver = tf.train.Saver() \n",
        "    \n",
        "\n",
        "    \n",
        "#Разделите свой набор данных на тренировочный набор и тестовый набор. Обучите модель тренировочному набору и оцените его на тестовом наборе.\n",
        "#Во-первых, мы хотим представить классы как int, а не строки:\n",
        "flower_class_ids = {flower_class: index for index, flower_class in enumerate(flower_classes)}\n",
        "\n",
        "\n",
        "#Будет проще перетасовать набор данных, если мы представим его как список пар filepath / class:\n",
        "flower_paths_and_classes = []\n",
        "for flower_class, paths in image_paths.items():\n",
        "    for path in paths:\n",
        "        flower_paths_and_classes.append((path, flower_class_ids[flower_class]))\n",
        "        \n",
        "#Затем давайте перетасовать набор данных и разделим его на обучающий набор и тестовый набор:\n",
        "\n",
        "\n",
        "np.random.shuffle(flower_paths_and_classes)\n",
        "a = flower_paths_and_classes\n",
        "flower_paths_and_classes_train = a[:int(len(a)*0.6)]\n",
        "flower_paths_and_classes_valid = a[int(len(a)*0.6):int(len(a)*0.8)]\n",
        "flower_paths_and_classes_test = a[int(len(a)*0.8):]\n",
        "\n",
        "n_iterations_per_epoch = len(flower_paths_and_classes_train) // batch_size\n",
        "\n",
        "print(\"train:\", len(flower_paths_and_classes_train) ,\"valid:\", len(flower_paths_and_classes_valid), \"test\", len(flower_paths_and_classes_test))\n",
        "\n",
        "#flower_paths_and_classes\n",
        "#print(\"flower_paths_and_classes\", len(flower_paths_and_classes))\n",
        "#print(\"flower_paths_and_classes_valid\", len(flower_paths_and_classes_valid))\n",
        "X_train, y_train = prepare_data(flower_paths_and_classes_train)\n",
        "X_test, y_test = prepare_data(flower_paths_and_classes_test)\n",
        "X_valid, y_valid = prepare_data(flower_paths_and_classes_valid)        \n",
        "#n_epochs = 10000\n",
        "#batch_size = 50       \n",
        "\n",
        "#best_loss_val = np.infty\n",
        "#check_interval = 100\n",
        "#checks_since_last_progress = 0\n",
        "#max_checks_without_progress = 20\n",
        "#best_model_params = None \n",
        "\n",
        "#step\n",
        "\n",
        "def shuffle_batch(X, batch_size):\n",
        "  #rnd_idx = [i for i in range(len(X)) if i>=batch_n*batch_size and i<(batch_n*batch_size+batch_size)]\n",
        "  #print(\"rnd_idx\", rnd_idx)\n",
        "  rnd_idx = []\n",
        "  for i in range(len(X)): rnd_idx.append(i)\n",
        "  for batch_idx in np.array_split(rnd_idx, batch_size):   # np.array_split делит масим на несколько массивов\n",
        "      #print(\"batch_idx\", batch_idx)  \n",
        "      X_batch = X[batch_idx]\n",
        "      yield X_batch      \n",
        "\n",
        "\n",
        "\n",
        "############################## ВЫБИРАЕМ ПЕРЕМЕННЫЕ ##########################\n",
        "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"MobilenetV2\") # regular expression\n",
        "restore_saver = tf.train.Saver(reuse_vars) # to restore layers 1-3\n",
        "\n",
        "h2_cache_2 = np.empty((0, layer_17.get_shape().as_list()[1]))\n",
        "h2_cache_valid_2 = np.empty((0, layer_17.get_shape().as_list()[1]))\n",
        "\n",
        "################################################################################\n",
        "with tf.Session() as sess:\n",
        "    init.run()\n",
        "    inception_saver.restore(sess, os.path.join(MODELS_PATH, MODEL_NAME))\n",
        "    \n",
        "    #for batch_n in range(len(X_train)//batch_size): \n",
        "    for X_batch in shuffle_batch(X_train, batch_size):\n",
        "            h2_cache_1 = sess.run(layer_17, feed_dict={X: X_batch, training: False})\n",
        "            h2_cache_2 = np.concatenate((h2_cache_2, h2_cache_1))\n",
        "    #for batch_n in range(len(X_train)//batch_size):\n",
        "    for X_val in shuffle_batch(X_valid, batch_size):\n",
        "            h2_cache_valid_1 = sess.run(layer_17, feed_dict={X: X_val, training: False})\n",
        "            h2_cache_valid_2 = np.concatenate((h2_cache_valid_2, h2_cache_valid_1))\n",
        "\n",
        "            \n",
        "############################## СОЗДАЕМ КЭШ ###################################\n",
        "   \n",
        "  # h2_cache = prelogits.eval(feed_dict={X: X_train})\n",
        "   # h2_cache_valid = sess.run(prelogits, feed_dict={X: X_valid})\n",
        "################################################################################  \n",
        "    \n",
        "  "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`use_explicit_padding` should only be used with  SAME padding.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-4ff74dee538e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"new_output_layer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m     \u001b[0mlayer_18\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpanded_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_17\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m                  \u001b[0;31m# num_outputs=320,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m                  \u001b[0;31m# expansion_size=conv_blocks.expand_input_by_factor(6),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-b0b208ae618b>\u001b[0m in \u001b[0;36mexpanded_conv\u001b[0;34m(input_tensor)\u001b[0m\n\u001b[1;32m     89\u001b[0m           \u001b[0mscope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'expand'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m           \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m           normalizer_fn=normalizer_fn)\n\u001b[0m\u001b[1;32m     92\u001b[0m       \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'expansion_output'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mendpoints\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-b0b208ae618b>\u001b[0m in \u001b[0;36msplit_conv\u001b[0;34m(input_tensor, num_outputs, num_ways, scope, divisible_by, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m   \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m   if num_ways == 1 or min(b / num_ways,\n\u001b[0m\u001b[1;32m    144\u001b[0m                           num_outputs / num_ways) < divisible_by:\n\u001b[1;32m    145\u001b[0m     \u001b[0;31m# Don't do any splitting if we end up with less than 8 filters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'int' and 'tuple'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "_tVEO2qanJx_",
        "colab_type": "code",
        "outputId": "54b26cd4-c462-465f-f0aa-6ee59cefc4a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "cell_type": "code",
      "source": [
        "b = input_tensor.get_shape().as_list()[3]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-cadd6de456e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'input_tensor' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "DY3VSf6af5G9",
        "colab_type": "code",
        "outputId": "e02f4abf-d363-44bb-8b16-750c5c094b3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import time\n",
        "n_epochs = 1000\n",
        "batch_size = 40\n",
        "from IPython.display import clear_output\n",
        "max_checks_without_progress = 1000\n",
        "checks_without_progress = 0\n",
        "best_loss = np.infty\n",
        "with tf.Session() as sess:\n",
        " \n",
        "\n",
        "\n",
        "    init.run()\n",
        "    inception_saver.restore(sess, os.path.join(MODELS_PATH, MODEL_NAME))\n",
        "    for var in flower_vars:\n",
        "        var.initializer.run()\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        rnd_idx = np.random.permutation(len(X_train))\n",
        "        for rnd_indices in np.array_split(rnd_idx, len(X_train) // batch_size):\n",
        "            h5_batch, y_batch = h2_cache_2[rnd_indices], y_train[rnd_indices]\n",
        "            sess.run(training_op, feed_dict={prelogits: h5_batch, y: y_batch})\n",
        "        loss_val, acc_val = sess.run([loss, accuracy], feed_dict={prelogits: h2_cache_valid_2, y: y_valid})\n",
        "        if loss_val < best_loss:\n",
        "            save_path = saver.save(sess, \"./my_mnist_model_5_to_9_five_frozen\")\n",
        "            best_loss = loss_val\n",
        "            checks_without_progress = 0\n",
        "        else:\n",
        "            checks_without_progress += 1\n",
        "            if checks_without_progress > max_checks_without_progress:\n",
        "                print(\"Early stopping!\")\n",
        "                break\n",
        "        if epoch % 10 == 0:\n",
        "          \n",
        "          \n",
        "          clear_output(wait=True)\n",
        "          print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n",
        "              epoch, loss_val, best_loss, acc_val * 100))\n",
        "          #print(printm())\n",
        "\n",
        "    t1 = time.time()\n",
        "    print(\"Total training time: {:.1f}s\".format(t1 - t0))\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    saver.restore(sess, \"./my_mnist_model_5_to_9_five_frozen\")\n",
        "    acc_test = accuracy.eval(feed_dict={X: X_test[:200], y: y_test[:200]})\n",
        "    print(\"Final test accuracy: {:.2f}%\".format(acc_test * 100))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "990\tValidation loss: 1.655289\tBest loss: 0.382186\tAccuracy: 85.56%\n",
            "Total training time: 123.3s\n",
            "INFO:tensorflow:Restoring parameters from ./my_mnist_model_5_to_9_five_frozen\n",
            "Final test accuracy: 84.00%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}