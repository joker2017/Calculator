{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "17122018.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joker2017/Calculator/blob/master/17122018.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "yl0_vUGm2U_x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "  \n",
        "print( os.listdir('/content') )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JeO02mGVr6VA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#Загрузите последнюю предварительную модель Inception v3: контрольная точка доступна по адресу https://github.com/tensorflow/models/tree/master/research/slim . \n",
        "#писок имен классов доступен по адресу https://goo.gl/brXRtZ , но вы должны сначала вставить «background» класс.\n",
        "import os\n",
        "import re \n",
        "import sys\n",
        "import tarfile\n",
        "from six.moves import urllib\n",
        "\n",
        "TF_MODELS_URL = \"http://download.tensorflow.org/models\"\n",
        "INCEPTION_V3_URL = TF_MODELS_URL + \"/inception_v3_2016_08_28.tar.gz\"\n",
        "INCEPTION_PATH = os.path.join(\"/content/datasets\", \"inception\")\n",
        "INCEPTION_V3_CHECKPOINT_PATH = os.path.join(INCEPTION_PATH, \"inception_v3.ckpt\")\n",
        "CLASS_NAME_REGEX = re.compile(r\"^n\\d+\\s+(.*)\\s*$\", re.M | re.U)\n",
        "FLOWERS_URL = \"http://download.tensorflow.org/example_images/flower_photos.tgz\"\n",
        "FLOWERS_PATH = os.path.join(\"/content/datasets\", \"flowers\")\n",
        "\n",
        "\n",
        "def download_progress(count, block_size, total_size):\n",
        "    percent = count * block_size * 100 // total_size\n",
        "    sys.stdout.write(\"\\rDownloading: {}%\".format(percent))\n",
        "    sys.stdout.flush()\n",
        "\n",
        "def fetch_pretrained_inception_v3(url=INCEPTION_V3_URL, path=INCEPTION_PATH):\n",
        "    if os.path.exists(INCEPTION_V3_CHECKPOINT_PATH):\n",
        "        return\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "    tgz_path = os.path.join(path, \"inception_v3.tgz\")\n",
        "    urllib.request.urlretrieve(url, tgz_path, reporthook=download_progress)\n",
        "    inception_tgz = tarfile.open(tgz_path)\n",
        "    inception_tgz.extractall(path=path)\n",
        "    inception_tgz.close()\n",
        "    os.remove(tgz_path)\n",
        "\n",
        "def fetch_flowers(url=FLOWERS_URL, path=FLOWERS_PATH):\n",
        "    if os.path.exists(FLOWERS_PATH):\n",
        "        return\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "    tgz_path = os.path.join(path, \"flower_photos.tgz\")\n",
        "    urllib.request.urlretrieve(url, tgz_path, reporthook=download_progress)\n",
        "    flowers_tgz = tarfile.open(tgz_path)\n",
        "    flowers_tgz.extractall(path=path)\n",
        "    flowers_tgz.close()\n",
        "    os.remove(tgz_path)\n",
        "\n",
        "def load_class_names():\n",
        "    with open(os.path.join(\"/content\", \"imagenet_class_names.txt\"), \"rb\") as f:\n",
        "        content = f.read().decode(\"utf-8\")\n",
        "        return CLASS_NAME_REGEX.findall(content)\n",
        "      \n",
        "    \n",
        "      \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bscuD7qN0Z8b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fetch_pretrained_inception_v3()   \n",
        "load_class_names()\n",
        "fetch_flowers()\n",
        "class_names = [\"background\"] + load_class_names() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d-5FW1Mo4DUs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class_names = [\"background\"] + load_class_names() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nSZTTwcEvLG9",
        "colab_type": "code",
        "outputId": "0a874e49-8d97-4c09-dcd3-272c82ffed35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "print( os.getcwd() )\n",
        "print( os.listdir('/content/datasets/flowers') )"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "['flower_photos']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WNOC6n9out14",
        "colab_type": "code",
        "outputId": "a9581b5e-1d81-4383-ad41-0be02b9d0cca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "cell_type": "code",
      "source": [
        "class_names[:5]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['background',\n",
              " 'tench, Tinca tinca\\r',\n",
              " 'goldfish, Carassius auratus\\r',\n",
              " 'great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias\\r',\n",
              " 'tiger shark, Galeocerdo cuvieri\\r']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "KKRuqAYnqyZc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Jwe1UXvnqIK2",
        "colab_type": "code",
        "outputId": "b50d5659-6cb9-4dcf-ba81-098cf451c46f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#готовим данные\n",
        "from tensorflow.contrib.slim.nets import inception\n",
        "import tensorflow.contrib.slim as slim\n",
        "import tensorflow as tf\n",
        "from scipy.misc import imresize\n",
        "from skimage.transform import rescale, resize, downscale_local_mean\n",
        "import numpy as np\n",
        "from random import sample\n",
        "import matplotlib.image as mpimg\n",
        "import os\n",
        "\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"/tmp/data/\")\n",
        "\n",
        "width = 299\n",
        "height = 299\n",
        "channels = 3\n",
        "tf.reset_default_graph()\n",
        "\n",
        "    \n",
        "n_epochs = 1000\n",
        "batch_size = 50\n",
        "\n",
        "best_loss_val = np.infty\n",
        "check_interval = 500\n",
        "checks_since_last_progress = 0\n",
        "max_checks_without_progress = 20\n",
        "best_model_params = None \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Каждый подкаталог flower_photos каталога содержит все изображения данного класса. Давайте получим список классов:\n",
        "flowers_root_path = os.path.join(FLOWERS_PATH, \"flower_photos\")\n",
        "flower_classes = sorted([dirname for dirname in os.listdir(flowers_root_path)\n",
        "                  if os.path.isdir(os.path.join(flowers_root_path, dirname))])\n",
        "#flower_classes\n",
        "\n",
        "\n",
        "#Давайте получим список всех путей файлов изображений для каждого класса:\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "image_paths = defaultdict(list)\n",
        "\n",
        "for flower_class in flower_classes:\n",
        "    image_dir = os.path.join(flowers_root_path, flower_class)\n",
        "    for filepath in os.listdir(image_dir):\n",
        "        if filepath.endswith(\".jpg\"):\n",
        "            image_paths[flower_class].append(os.path.join(image_dir, filepath))\n",
        "            \n",
        "            \n",
        "#Давайте сортируем пути изображения, чтобы заставить этот ноутбук вести себя последовательно на нескольких запусках:\n",
        "\n",
        "for paths in image_paths.values():\n",
        "    paths.sort() \n",
        "\n",
        "    \n",
        "#Для получения дополнительных функций манипуляции с изображениями, таких как вращения, проверьте документацию SciPy или эту приятную страницу .\n",
        "\n",
        "\n",
        "\n",
        "def prepare_image(image, target_width = 299, target_height = 299, max_zoom = 0.2):\n",
        "    \"\"\"Zooms and crops the image randomly for data augmentation.\"\"\"\n",
        "\n",
        "    # First, let's find the largest bounding box with the target size ratio that fits within the image\n",
        "    height = image.shape[0]\n",
        "    width = image.shape[1]\n",
        "    image_ratio = width / height\n",
        "    target_image_ratio = target_width / target_height\n",
        "    crop_vertically = image_ratio < target_image_ratio\n",
        "    crop_width = width if crop_vertically else int(height * target_image_ratio)\n",
        "    crop_height = int(width / target_image_ratio) if crop_vertically else height\n",
        "        \n",
        "    # Now let's shrink this bounding box by a random factor (dividing the dimensions by a random number\n",
        "    # between 1.0 and 1.0 + `max_zoom`.\n",
        "    resize_factor = np.random.rand() * max_zoom + 1.0\n",
        "    crop_width = int(crop_width / resize_factor)\n",
        "    crop_height = int(crop_height / resize_factor)\n",
        "    \n",
        "    # Next, we can select a random location on the image for this bounding box.\n",
        "    x0 = np.random.randint(0, width - crop_width)\n",
        "    y0 = np.random.randint(0, height - crop_height)\n",
        "    x1 = x0 + crop_width\n",
        "    y1 = y0 + crop_height\n",
        "    \n",
        "    # Let's crop the image using the random bounding box we built.\n",
        "    image = image[y0:y1, x0:x1]\n",
        "\n",
        "    # Let's also flip the image horizontally with 50% probability:\n",
        "    if np.random.rand() < 0.5:\n",
        "        image = np.fliplr(image)\n",
        "\n",
        "    # Now, let's resize the image to the target dimensions.\n",
        "    image = resize(image, (target_width, target_height), mode='reflect')\n",
        "    \n",
        "    # Finally, let's ensure that the colors are represented as\n",
        "    # 32-bit floats ranging from 0.0 to 1.0 (for now):\n",
        "    return image.astype(np.float32) / 255\n",
        "  \n",
        "\n",
        "def prepare_batch(flower_paths_and_classes, batch_size):\n",
        "    batch_paths_and_classes = sample(flower_paths_and_classes, batch_size)\n",
        "    images = [mpimg.imread(path)[:, :, :channels] for path, labels in batch_paths_and_classes]\n",
        "    prepared_images = [prepare_image(image) for image in images]\n",
        "    X_batch = 2 * np.stack(prepared_images) - 1 # Inception expects colors ranging from -1 to 1\n",
        "    y_batch = np.array([labels for path, labels in batch_paths_and_classes], dtype=np.int32)\n",
        "    return X_batch, y_batch \n",
        "  \n",
        "#раннняя остановка\n",
        "def get_model_params():\n",
        "    gvars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
        "    return {gvar.op.name: value for gvar, value in zip(gvars, tf.get_default_session().run(gvars))}\n",
        "\n",
        "def restore_model_params(model_params):\n",
        "    gvar_names = list(model_params.keys())\n",
        "    assign_ops = {gvar_name: tf.get_default_graph().get_operation_by_name(gvar_name + \"/Assign\")\n",
        "                  for gvar_name in gvar_names}\n",
        "    init_values = {gvar_name: assign_op.inputs[1] for gvar_name, assign_op in assign_ops.items()}\n",
        "    feed_dict = {init_values[gvar_name]: model_params[gvar_name] for gvar_name in gvar_names}\n",
        "    tf.get_default_session().run(assign_ops, feed_dict=feed_dict)  \n",
        "  \n",
        "  #снова получить начальный график v3. На этот раз воспользуемся trainingзаполнителем\n",
        "tf.reset_default_graph()  \n",
        "X = tf.placeholder(tf.float32, shape=[None, height, width, channels], name=\"X\")\n",
        "training = tf.placeholder_with_default(False, shape=[])\n",
        "with slim.arg_scope(inception.inception_v3_arg_scope()):\n",
        "    logits, end_points = inception.inception_v3(X, num_classes=1001, is_training=training)\n",
        "\n",
        "inception_saver = tf.train.Saver()\n",
        "\n",
        "\n",
        "#изучить входные данные выходного уровня:\n",
        "# logits.op.inputs[0].op.inputs[0]  или end_points\n",
        "end_points[\"PreLogits\"]\n",
        "\n",
        "#отказаться от 2-го и 3-го измерений с помощью tf.squeeze() функции\n",
        "prelogits = tf.squeeze(end_points[\"PreLogits\"], axis=[1, 2])\n",
        "\n",
        "#добавить окончательный полностью подключенный слой поверх этого слоя:\n",
        "  \n",
        "n_outputs = len(flower_classes)\n",
        "\n",
        "with tf.name_scope(\"new_output_layer\"):\n",
        "    flower_logits = tf.layers.dense(prelogits, n_outputs, name=\"flower_logits\")\n",
        "    Y_proba = tf.nn.softmax(flower_logits, name=\"Y_proba\")\n",
        "    \n",
        "#Добавляемстандвартные вещи\n",
        "\n",
        "y = tf.placeholder(tf.int32, shape=[None])\n",
        "\n",
        "with tf.name_scope(\"train\"):\n",
        "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=flower_logits, labels=y)\n",
        "    loss = tf.reduce_mean(xentropy)\n",
        "    optimizer = tf.train.AdamOptimizer()\n",
        "    flower_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"flower_logits\")\n",
        "    training_op = optimizer.minimize(loss, var_list=flower_vars)\n",
        "\n",
        "with tf.name_scope(\"eval\"):\n",
        "    correct = tf.nn.in_top_k(flower_logits, y, 1)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
        "\n",
        "with tf.name_scope(\"init_and_save\"):\n",
        "    init = tf.global_variables_initializer()\n",
        "    saver = tf.train.Saver() \n",
        "    \n",
        "    \n",
        "#Разделите свой набор данных на тренировочный набор и тестовый набор. Обучите модель тренировочному набору и оцените его на тестовом наборе.\n",
        "\n",
        "#Во-первых, мы хотим представить классы как int, а не строки:\n",
        "\n",
        "flower_class_ids = {flower_class: index for index, flower_class in enumerate(flower_classes)}\n",
        "#flower_class_ids\n",
        "\n",
        "#Будет проще перетасовать набор данных, если мы представим его как список пар filepath / class:\n",
        "\n",
        "flower_paths_and_classes = []\n",
        "for flower_class, paths in image_paths.items():\n",
        "    for path in paths:\n",
        "        flower_paths_and_classes.append((path, flower_class_ids[flower_class]))\n",
        "        \n",
        "#Затем давайте перетасовать набор данных и разделим его на обучающий набор и тестовый набор:\n",
        "\n",
        "test_ratio = 0.2\n",
        "valid_ratio = 0.4\n",
        "train_size = int(len(flower_paths_and_classes) * (1 - test_ratio - valid_ratio))\n",
        "test_size = int(len(flower_paths_and_classes) * test_ratio)\n",
        "valid_size = int(len(flower_paths_and_classes) * valid_ratio)                \n",
        "np.random.shuffle(flower_paths_and_classes)\n",
        "\n",
        "\n",
        "flower_paths_and_classes_test = flower_paths_and_classes[:test_size]\n",
        "flower_paths_and_classes_valid = flower_paths_and_classes[test_size:valid_size]\n",
        "flower_paths_and_classes_train = flower_paths_and_classes[valid_size:]\n",
        "# предварительной обработки набора тестов, а также для создания партий во время обучения. Для простоты мы будем использовать реализацию NumPy / SciPy:\n",
        "#X_batch, y_batch = prepare_batch(flower_paths_and_classes_train, batch_size=4)\n",
        "\n",
        "#Теперь давайте использовать эту функцию для подготовки тестового набора:\n",
        "\n",
        "#X_test, y_test = prepare_batch(flower_paths_and_classes_test, batch_size=len(flower_paths_and_classes_test))\n",
        "\n",
        "        \n",
        "#n_epochs = 10\n",
        "#batch_size = 100\n",
        "#n_iterations_per_epoch = len(flower_paths_and_classes_train) // batch_size\n",
        "\n",
        "#with tf.Session() as sess:\n",
        "#    init.run()\n",
        "#    inception_saver.restore(sess, INCEPTION_V3_CHECKPOINT_PATH)\n",
        "\n",
        "#    for epoch in range(n_epochs):\n",
        "#        print(\"Epoch\", epoch, end=\"\")\n",
        "#        for iteration in range(n_iterations_per_epoch):\n",
        "#            print(\".\", end=\"\")\n",
        "#            X_batch, y_batch = prepare_batch(flower_paths_and_classes_train, batch_size)\n",
        "#            sess.run(training_op, feed_dict={X: X_batch, y: y_batch, training: True})\n",
        "\n",
        "#        X_test, y_test = prepare_batch(flower_paths_and_classes_test, batch_size=len(flower_paths_and_classes_test)//5)\n",
        "#        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
        "#        acc_val = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
        "   \n",
        "        #print(\"  Train accuracy:\", acc_train*100, \"Validate accuracy:\", acc_val*100 )\n",
        "#        print(\"train accuracy: {:.4f}%, valid. accuracy: {:.4f}% \".format(acc_train * 100, acc_val * 100))\n",
        "\n",
        "#        save_path = saver.save(sess, \"./my_flowers_model\")\n",
        "        \n",
        "n_epochs = 1000\n",
        "batch_size = 50       \n",
        "\n",
        "best_loss_val = np.infty\n",
        "check_interval = 100\n",
        "checks_since_last_progress = 0\n",
        "max_checks_without_progress = 20\n",
        "best_model_params = None \n",
        "\n",
        "with tf.Session() as sess:\n",
        "    init.run()\n",
        "    for epoch in range(n_epochs):\n",
        "        print(\"Epoch\", epoch, end=\"\")\n",
        "        for iteration in range(n_iterations_per_epoch):\n",
        "            print(\".\", end=\"\")\n",
        "            X_batch, y_batch = prepare_batch(flower_paths_and_classes_train, batch_size)\n",
        "            X_test, y_test = prepare_batch(flower_paths_and_classes_test, batch_size=len(flower_paths_and_classes_test)//5)\n",
        "            X_valid, y_valid = prepare_batch(flower_paths_and_classes_valid, batch_size=len(flower_paths_and_classes_test)//5)\n",
        "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch, training: True})\n",
        "            if iteration % check_interval == 0:\n",
        "                \n",
        "                loss_val = loss.eval(feed_dict={X: X_valid, y: y_valid})\n",
        "                print(\"  Train loss_val:\", loss_val)\n",
        "                if loss_val < best_loss_val:\n",
        "                    best_loss_val = loss_val\n",
        "                    checks_since_last_progress = 0\n",
        "                    best_model_params = get_model_params()\n",
        "                else:\n",
        "                    checks_since_last_progress += 1\n",
        "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
        "        acc_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
        "        print(\"Epoch {}, train accuracy: {:.4f}%, valid. accuracy: {:.4f}%, valid. best loss: {:.6f}\".format(\n",
        "                  epoch, acc_train * 100, acc_val * 100, best_loss_val))\n",
        "        if checks_since_last_progress > max_checks_without_progress:\n",
        "            print(\"Early stopping!\")\n",
        "            break\n",
        "\n",
        "    if best_model_params:\n",
        "        restore_model_params(best_model_params)\n",
        "    acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
        "    print(\"Final accuracy on test set:\", acc_test)\n",
        "    save_path = saver.save(sess, \"./my_mnist_model\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
            "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
            "Epoch 0.  Train loss_val: 1.9641699\n",
            "............................Epoch 0, train accuracy: 14.0000%, valid. accuracy: 15.7534%, valid. best loss: 1.964170\n",
            "Epoch 1.  Train loss_val: 1.7237432\n",
            "."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uXuKWQRjqK3R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kb1z-kCBqLA8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KZx31Biwqb3Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "ранняя остановка"
      ]
    },
    {
      "metadata": {
        "id": "BiYr1gVzdlGL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_model_params():\n",
        "    gvars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
        "    return {gvar.op.name: value for gvar, value in zip(gvars, tf.get_default_session().run(gvars))}\n",
        "\n",
        "def restore_model_params(model_params):\n",
        "    gvar_names = list(model_params.keys())\n",
        "    assign_ops = {gvar_name: tf.get_default_graph().get_operation_by_name(gvar_name + \"/Assign\")\n",
        "                  for gvar_name in gvar_names}\n",
        "    init_values = {gvar_name: assign_op.inputs[1] for gvar_name, assign_op in assign_ops.items()}\n",
        "    feed_dict = {init_values[gvar_name]: model_params[gvar_name] for gvar_name in gvar_names}\n",
        "    tf.get_default_session().run(assign_ops, feed_dict=feed_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BV44UIhhdlGM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "n_epochs = 1000\n",
        "batch_size = 50\n",
        "\n",
        "best_loss_val = np.infty\n",
        "check_interval = 500\n",
        "checks_since_last_progress = 0\n",
        "max_checks_without_progress = 20\n",
        "best_model_params = None \n",
        "\n",
        "with tf.Session() as sess:\n",
        "    init.run()\n",
        "    for epoch in range(n_epochs):\n",
        "        for iteration in range(mnist.train.num_examples // batch_size):\n",
        "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
        "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch, training: True})\n",
        "            if iteration % check_interval == 0:\n",
        "                loss_val = loss.eval(feed_dict={X: mnist.validation.images,\n",
        "                                                y: mnist.validation.labels})\n",
        "                if loss_val < best_loss_val:\n",
        "                    best_loss_val = loss_val\n",
        "                    checks_since_last_progress = 0\n",
        "                    best_model_params = get_model_params()\n",
        "                else:\n",
        "                    checks_since_last_progress += 1\n",
        "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
        "        acc_val = accuracy.eval(feed_dict={X: mnist.validation.images,\n",
        "                                           y: mnist.validation.labels})\n",
        "        print(\"Epoch {}, train accuracy: {:.4f}%, valid. accuracy: {:.4f}%, valid. best loss: {:.6f}\".format(\n",
        "                  epoch, acc_train * 100, acc_val * 100, best_loss_val))\n",
        "        if checks_since_last_progress > max_checks_without_progress:\n",
        "            print(\"Early stopping!\")\n",
        "            break\n",
        "\n",
        "    if best_model_params:\n",
        "        restore_model_params(best_model_params)\n",
        "    acc_test = accuracy.eval(feed_dict={X: mnist.test.images,\n",
        "                                        y: mnist.test.labels})\n",
        "    print(\"Final accuracy on test set:\", acc_test)\n",
        "    save_path = saver.save(sess, \"./my_mnist_model\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}