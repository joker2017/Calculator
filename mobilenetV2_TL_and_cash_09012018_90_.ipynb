{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mobilenetV2 TL and cash 09012018_90%.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joker2017/Calculator/blob/master/mobilenetV2_TL_and_cash_09012018_90_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "6BPCjWiVMPh4",
        "colab_type": "code",
        "outputId": "f4d923cc-29a0-4705-9fe0-1190c276ae12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/tensorflow/models"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'models' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rzNx01j6MWK1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/models/research/slim')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yl0_vUGm2U_x",
        "colab_type": "code",
        "outputId": "f7fd3e78-ee9d-45fc-fe1e-418f4e94533a",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "  \n",
        "print( os.listdir('/content') )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4906c7be-5f37-41e1-9974-645125be4d9d\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-4906c7be-5f37-41e1-9974-645125be4d9d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving imagenet_class_names.txt to imagenet_class_names (2).txt\n",
            "User uploaded file \"imagenet_class_names.txt\" with length 31674 bytes\n",
            "['.config', 'imagenet_class_names (2).txt', 'imagenet_class_names.txt', 'my_mnist_model_5_to_9_five_frozen.index', 'models', 'my_mnist_model_5_to_9_five_frozen.meta', 'flower_photos', 'checkpoint', 'my_mnist_model_5_to_9_five_frozen.data-00000-of-00001', 'imagenet_class_names (1).txt', 'mobilenet_v2_1', 'sample_data']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JeO02mGVr6VA",
        "colab_type": "code",
        "outputId": "7c85d70c-b36f-4fd7-b983-833feab650a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#Загрузите последнюю предварительную модель Inception v3: контрольная точка доступна по адресу https://github.com/tensorflow/models/tree/master/research/slim . \n",
        "#писок имен классов доступен по адресу https://goo.gl/brXRtZ , но вы должны сначала вставить «background» класс.\n",
        "import os\n",
        "import re \n",
        "import sys\n",
        "import tarfile\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from random import sample\n",
        "from six.moves import urllib\n",
        "import matplotlib.image as mpimg\n",
        "import tensorflow.contrib.slim as slim\n",
        "#from tensorflow.contrib.slim.nets import inception\n",
        "from nets.mobilenet import mobilenet_v2\n",
        "from skimage.transform import rescale, resize, downscale_local_mean\n",
        "\n",
        "MODEL_NAME = \"mobilenet_v2_1.0_224.ckpt\"\n",
        "CLASS_NAME_REGEX = re.compile(r\"^n\\d+\\s+(.*)\\s*$\", re.M | re.U)\n",
        "DATASET_URL = \"http://download.tensorflow.org/example_images/flower_photos.tgz\"\n",
        "MODEL_URL = \"https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_1.0_224.tgz\"\n",
        "\n",
        "\n",
        "\n",
        "def download_progress(count, block_size, total_size):\n",
        "    percent = count * block_size * 100 // total_size\n",
        "    sys.stdout.write(\"\\rDownloading: {}%\".format(percent))\n",
        "    sys.stdout.flush()\n",
        "    \n",
        "\n",
        "def download_tgz(url):\n",
        "    file_name=os.path.basename(url)\n",
        "    name_parts = os.path.split(file_name)\n",
        "    _, name = name_parts\n",
        "    name_path = name.rsplit( \".\", 2 )[ 0 ]\n",
        "    path = os.path.join(\"/content\", name_path)\n",
        "    tgz_path = os.path.join(path, name)\n",
        "    if os.path.exists(path): #and os.path.isfile(tgz_path):\n",
        "      print(\"file and path exist \", tgz_path, \"Path is:\", os.listdir(path), \"!\")\n",
        "      return path\n",
        "    else:\n",
        "      print(os.path.exists(path), os.path.isfile(tgz_path), path, tgz_path)\n",
        "      os.makedirs(path, exist_ok=True)\n",
        "      urllib.request.urlretrieve(url, tgz_path, reporthook=download_progress)\n",
        "      tmp_tgz = tarfile.open(tgz_path)\n",
        "      tmp_tgz.extractall(path=path)\n",
        "      tmp_tgz.close()\n",
        "      os.remove(tgz_path)\n",
        "      print(\"path created:\", path)\n",
        "      return path\n",
        "    \n",
        "# Загружаем модель\n",
        "MODELS_PATH = download_tgz(MODEL_URL)\n",
        "# Загружаем датасет\n",
        "FLOWERS_PATH = download_tgz(DATASET_URL)\n",
        " \n",
        "\n",
        "def load_class_names():\n",
        "    with open(os.path.join(\"/content\", \"imagenet_class_names.txt\"), \"rb\") as f:\n",
        "        content = f.read().decode(\"utf-8\")\n",
        "        #sys.stdout.flush()\n",
        "        return CLASS_NAME_REGEX.findall(content)\n",
        "      \n",
        "load_class_names()    \n",
        "class_names = [\"background\"] + load_class_names()       \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "file and path exist  /content/mobilenet_v2_1/mobilenet_v2_1.0_224.tgz Path is: ['mobilenet_v2_1.0_224.ckpt.data-00000-of-00001', 'mobilenet_v2_1.0_224_eval.pbtxt', 'mobilenet_v2_1.0_224_frozen.pb', 'mobilenet_v2_1.0_224.ckpt.meta', 'mobilenet_v2_1.0_224.ckpt.index', 'mobilenet_v2_1.0_224.tflite', 'mobilenet_v2_1.0_224_info.txt'] !\n",
            "file and path exist  /content/flower_photos/flower_photos.tgz Path is: ['flower_photos'] !\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EaCnXGYLK0gX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"Convolution blocks for mobilenet.\"\"\"\n",
        "import contextlib\n",
        "import functools\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "slim = tf.contrib.slim\n",
        "\n",
        "# tf.contrib.slim.conv2d\n",
        "# tf.contrib.slim.separable_conv2d\n",
        "\n",
        "\n",
        "def _make_divisible(v, divisor, min_value=None):\n",
        "  if min_value is None:\n",
        "    min_value = divisor\n",
        "  new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
        "  # Make sure that round down does not go down by more than 10%.\n",
        "  if new_v < 0.9 * v:\n",
        "    new_v += divisor\n",
        "  return new_v\n",
        "\n",
        "\n",
        "def _split_divisible(num, num_ways, divisible_by=8):\n",
        "  \"\"\"Равномерно разбивает num, num_ways, поэтому каждый фрагмент кратен divisible_by.\"\"\"\n",
        "  assert num % divisible_by == 0\n",
        "  assert num / num_ways >= divisible_by\n",
        "  # Примечание: хотите округлить, мы корректируем каждый сплит в соответствии с итогом.\n",
        "  base = num // num_ways // divisible_by * divisible_by\n",
        "  result = []\n",
        "  accumulated = 0\n",
        "  for i in range(num_ways):\n",
        "    r = base\n",
        "    while accumulated + r < num * (i + 1) / num_ways:\n",
        "      r += divisible_by\n",
        "    result.append(r)\n",
        "    accumulated += r\n",
        "  assert accumulated == num\n",
        "  return result\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def expand_input_by_factor(n, divisible_by=8):\n",
        "  return lambda num_inputs, **_: _make_divisible(num_inputs * n, divisible_by)\n",
        "\n",
        "#################################################################################\n",
        "#[None, 7, 7, 160]\n",
        "#################################################################################\n",
        "#@slim.add_arg_scope\n",
        "def expanded_conv(input_tensor,\n",
        "                  num_outputs,\n",
        "                 # expansion_size=expand_input_by_factor(6),\n",
        "                 # stride=1,\n",
        "                 # rate=1,\n",
        "                 # kernel_size=(3, 3),\n",
        "                 # residual=True,\n",
        "                 # normalizer_fn=None,    # normalizer_fn\n",
        "                 # project_activation_fn=tf.identity,\n",
        "                 # split_projection=1,\n",
        "                 # split_expansion=1,\n",
        "                 # expansion_transform=None,\n",
        "                  depthwise_location,\n",
        "                 # depthwise_channel_multiplier=1,\n",
        "                 # endpoints=None,\n",
        "                 # use_explicit_padding=False,\n",
        "                 # padding='SAME',\n",
        "                 # scope=None\n",
        "    \n",
        "                         ):\n",
        "    #depthwise_location='expansion',\n",
        "    expansion_size=expand_input_by_factor(6)\n",
        " # with tf.variable_scope(scope, default_name='expanded_conv') as s, \\\n",
        " #      tf.name_scope(s.original_name_scope):\n",
        "    prev_depth = input_tensor.get_shape().as_list()[3]  ########################  prev_depth = 160\n",
        " #   if  depthwise_location not in [None, 'input', 'output', 'expansion']:\n",
        " #     raise TypeError('%r is unknown value for depthwise_location' %\n",
        " #                     depthwise_location)\n",
        " #   if use_explicit_padding:                            #######################  use_explicit_padding = False\n",
        " #     if padding != 'SAME':\n",
        " #       raise TypeError('`use_explicit_padding` should only be used with '\n",
        " #                       '\"SAME\" padding.')\n",
        " #     padding = 'VALID'\n",
        "    depthwise_func = functools.partial(\n",
        "        slim.separable_conv2d,\n",
        "        num_outputs=None,\n",
        "        kernel_size=(3, 3),\n",
        "        depth_multiplier=1, #depthwise_channel_multiplier\n",
        "        stride=1,\n",
        "        rate=1,\n",
        "        normalizer_fn=slim.batch_norm,\n",
        "        padding='SAME',\n",
        "        scope='depthwise')\n",
        "    # b1 -> b2 * r -> b2\n",
        "    #   i -> (o * r) (bottleneck) -> o\n",
        "    \n",
        "    input_tensor = tf.identity(input_tensor, 'input')\n",
        "    net = input_tensor\n",
        "\n",
        "    if depthwise_location == 'input':\n",
        "  #    if use_explicit_padding:                          #######################  use_explicit_padding = False\n",
        "  #      net = _fixed_padding(net, kernel_size, rate)\n",
        "      net = depthwise_func(net, activation_fn=None)\n",
        "      print(\"net = depthwise_func(net, activation_fn=None, )\", \"net =\", net  )\n",
        "\n",
        "    if callable(expansion_size):\n",
        "      inner_size = expansion_size(num_inputs=prev_depth)\n",
        "      print(\"callable(expansion_size)\", callable(expansion_size),\"inner_size\", inner_size)\n",
        "      ########################  prev_depth = 160 inner_size = 960\n",
        "    else:\n",
        "      inner_size = expansion_size\n",
        "      print(\"callable(expansion_size)\", callable(expansion_size),\"inner_size\", inner_size)\n",
        "    if inner_size > net.shape[3]:                        #######################   [None, 7, 7, 160]\n",
        "      print(\"if inner_size > net.shape[3]\", \"inner_size=\", inner_size, \"net.shape[3] =\", net.shape[3])\n",
        "      net = split_conv(\n",
        "          net,\n",
        "          inner_size,\n",
        "          num_ways= 1, #split_expansion,\n",
        "          scope='expand',\n",
        "          stride=1,\n",
        "          normalizer_fn=slim.batch_norm)\n",
        "#      print(\"net = slim.conv2d(net(\", net.get_shape().as_list(), \"),inner_size =\", inne_size, \"num_ways= 1, #split_expansion,scope='expand', stride=1,normalizer_fn=slim.batch_norm))\", \"inner_size\", inner_size, \"net=\", net)\n",
        "      net = tf.identity(net, 'expansion_output')\n",
        "#    if endpoints is not None:\n",
        "#      endpoints['expansion_output'] = net\n",
        "\n",
        "    if depthwise_location == 'expansion':\n",
        "#      if use_explicit_padding:                          #######################  use_explicit_padding = False\n",
        "#        net = _fixed_padding(net, kernel_size, rate)\n",
        "      net = depthwise_func(net)\n",
        "      print(\"net = slim.separable_conv2d(net\", net.get_shape().as_list(), \", num_outputs=None,kernel_size=(3, 3), depth_multiplier=1, stride=1, rate=1, normalizer_fn=slim.batch_norm, padding='SAME',scope='depthwise')\")\n",
        "     \n",
        "    net = tf.identity(net, name='depthwise_output')\n",
        "   # if endpoints is not None:\n",
        "   #   endpoints['depthwise_output'] = net\n",
        "   # if expansion_transform:                             #######################  expansion_transform = None\n",
        "   #   net = expansion_transform(expansion_tensor=net, input_tensor=input_tensor)\n",
        "    #Обратите внимание, что в отличие от расширения, у нас всегда есть проекция для получения желаемого выходного размера.\n",
        "    net = split_conv(\n",
        "        net,\n",
        "        num_outputs,\n",
        "        num_ways=1,\n",
        "        stride=1,\n",
        "        scope='project',\n",
        "        normalizer_fn=slim.batch_norm,\n",
        "        activation_fn=tf.identity)\n",
        "#    print(\"net = slim.conv2d(net(\", net.get_shape().as_list(), \"), num_outputs =\", num_outputs=\", \"num_ways= 1, #split_expansion,scope='project', stride=1, normalizer_fn=slim.batch_norm, activation_fn=tf.identity)\")\n",
        "         \n",
        "#    if endpoints is not None:\n",
        "#      endpoints['projection_output'] = net\n",
        "    if depthwise_location == 'output':\n",
        "#      if use_explicit_padding:                          #######################  use_explicit_padding = False\n",
        "#        net = _fixed_padding(net, kernel_size, rate)\n",
        "      net = depthwise_func(net, activation_fn=None)\n",
        "      print(\"net = slim.separable_conv2d(net(\", net.get_shape().as_list(), \"), num_outputs=None,kernel_size=(3, 3), depth_multiplier=1, depthwise_channel_multiplier, stride=1, rate=1, normalizer_fn=slim.batch_norm, padding='SAME',scope='depthwise', activation_fn=None)\")\n",
        " #   if callable(residual):  # custom residual/ таможенный остаток\n",
        "    #net = residual(input_tensor=input_tensor, output_tensor=net)\n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "    if (net.get_shape().as_list()[3] == input_tensor.get_shape().as_list()[3]):\n",
        "        net += input_tensor\n",
        "        print(\"net += input_tensor\")\n",
        "      \n",
        "      \n",
        "    if (net.get_shape().as_list()[3] == input_tensor.get_shape().as_list()[3]):\n",
        "#          print(\"if (net.get_shape().as_list()[3] == input_tensor.get_shape().as_list()[3]):\", net.get_shape().as_list()[3], \"==\", input_tensor.get_shape().as_list()[3])\n",
        "          net += input_tensor\n",
        "          print(\"net += input_tensor\")\n",
        "    return tf.identity(net, name='output')\n",
        "\n",
        "\n",
        "def split_conv(input_tensor,\n",
        "               num_outputs,\n",
        "               num_ways,\n",
        "               scope,\n",
        "               divisible_by=8,\n",
        "               **kwargs):\n",
        "\n",
        "  b = input_tensor.get_shape().as_list()[3]\n",
        "\n",
        "  if num_ways == 1 or min(b // num_ways,\n",
        "                          num_outputs // num_ways) < divisible_by:\n",
        "    # Don't do any splitting if we end up with less than 8 filters\n",
        "    # on either side.\n",
        "    print(\"net = slim.conv2d(net\", input_tensor.get_shape().as_list(), \", num_outputs=\", num_outputs, \", [1, 1], scope=\", scope, \"**kwargs)\")\n",
        "    return slim.conv2d(input_tensor, num_outputs, [1, 1], scope=scope, **kwargs)\n",
        "    \n",
        "\n",
        "  outs = []\n",
        "  print(\"outs = []\")\n",
        "  input_splits = _split_divisible(b, num_ways, divisible_by=divisible_by)\n",
        "  print(\"input_splits =\", input_splits)\n",
        "  output_splits = _split_divisible(\n",
        "      num_outputs, num_ways, divisible_by=divisible_by)\n",
        "  print(\"output_splits =\", output_splits)\n",
        "  inputs = tf.split(input_tensor, input_splits, axis=3, name='split_' + scope)\n",
        "  print(\"inputs = tf.split(input_tensor, input_splits, axis=3, name='split_' + scope)\")\n",
        "  base = scope\n",
        "  for i, (input_tensor, out_size) in enumerate(zip(inputs, output_splits)):\n",
        "    scope = base + '_part_%d' % (i,)\n",
        "    n = slim.conv2d(input_tensor, out_size, [1, 1], scope=scope, **kwargs)\n",
        "    n = tf.identity(n, scope + '_output')\n",
        "    outs.append(n)\n",
        "  print(\"net = tf.concat(outs=\", outs, \", 3, name=\", scope, \"+ '_concat')\")  \n",
        "  return tf.concat(outs, 3, name=scope + '_concat')\n",
        "\n",
        "\n",
        "\n",
        "#net = slim.conv2d(net [None, 7, 7, 160] , num_outputs= 960 , [1, 1], scope= 'expand', stride=1, normalizer_fn=slim.batch_norm)\n",
        "\n",
        "#net = slim.separable_conv2d(net [None, 7, 7, 960] , num_outputs=None, kernel_size=(3, 3), depth_multiplier=1, stride=1, rate=1, normalizer_fn=slim.batch_norm, padding='SAME',scope='depthwise')\n",
        "\n",
        "#net = slim.conv2d(net [None, 7, 7, 960] , num_outputs= 320 , [1, 1], scope= 'project', normalizer_fn=slim.batch_norm, activation_fn=tf.identity)\n",
        "#net = slim.conv2d(net,  stride=1, kernel_size=[1, 1], num_outputs=1280)\n",
        "#net = tf.nn.avg_pool( net [None, 7, 7, 160] ksize= [1, 7, 7, 1] strides=[1, 1, 1, 1], padding='VALID')\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i2T09-MCLOyx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def global_pool(input_tensor):\n",
        "\n",
        "  pool_op=tf.nn.avg_pool\n",
        "  shape = input_tensor.get_shape().as_list()\n",
        "  if shape[1] is None or shape[2] is None:\n",
        "    print(\"if shape[1] is None or shape[2] is None:\", \"shape[1]=\". shape[1], \"shape[2]=\", shape[2])\n",
        "    kernel_size = tf.convert_to_tensor(\n",
        "        [1, tf.shape(input_tensor)[1],\n",
        "         tf.shape(input_tensor)[2], 1])\n",
        "  else:\n",
        "    \n",
        "    kernel_size = [1, shape[1], shape[2], 1]\n",
        "    \n",
        "  output = pool_op(\n",
        "      input_tensor, ksize=kernel_size, strides=[1, 1, 1, 1], padding='VALID')\n",
        "  print(\"net = tf.nn.avg_pool( net\", input_tensor.get_shape().as_list(), \"ksize=\", kernel_size, \"strides=[1, 1, 1, 1], padding='VALID')\")\n",
        "  # Recover output shape, for unknown shape.\n",
        "  output.set_shape([None, 1, 1, None])\n",
        "  #print(\"net = output.set_shape([None, 1, 1, None])\", output.get_shape().as_list())\n",
        "  return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Jwe1UXvnqIK2",
        "colab_type": "code",
        "outputId": "17d7c1b0-8f04-4059-e8be-c282a7f4ada2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "width = 224 #299\n",
        "height = 224 #299\n",
        "channels = 3\n",
        "import tensorflow as tf\n",
        "tf.reset_default_graph()\n",
        "n_epochs = 1000\n",
        "batch_size = 100\n",
        "best_loss_val = np.infty\n",
        "check_interval = 500\n",
        "checks_since_last_progress = 0\n",
        "max_checks_without_progress = 20\n",
        "best_model_params = None \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Каждый подкаталог flower_photos каталога содержит все изображения данного класса. Давайте получим список классов:\n",
        "flowers_root_path = os.path.join(FLOWERS_PATH, \"flower_photos\")\n",
        "flower_classes = sorted([dirname for dirname in os.listdir(flowers_root_path)\n",
        "                  if os.path.isdir(os.path.join(flowers_root_path, dirname))])\n",
        "\n",
        "#Давайте получим список всех путей файлов изображений для каждого класса:\n",
        "\n",
        "from collections import defaultdict\n",
        "image_paths = defaultdict(list)\n",
        "for flower_class in flower_classes:\n",
        "    image_dir = os.path.join(flowers_root_path, flower_class)\n",
        "    for filepath in os.listdir(image_dir):\n",
        "        if filepath.endswith(\".jpg\"):\n",
        "            image_paths[flower_class].append(os.path.join(image_dir, filepath))\n",
        "            \n",
        "            \n",
        "#Давайте сортируем пути изображения, чтобы заставить этот ноутбук вести себя последовательно на нескольких запусках:\n",
        "for paths in image_paths.values():\n",
        "    paths.sort() \n",
        "\n",
        "    \n",
        "#Для получения дополнительных функций манипуляции с изображениями, таких как вращения, проверьте документацию SciPy или эту приятную страницу .\n",
        "from scipy.misc import imresize\n",
        "def prepare_image(image, target_width = width, target_height = height, max_zoom = 0.2):\n",
        "    \"\"\"Zooms and crops the image randomly for data augmentation.\"\"\"\n",
        "\n",
        "    # First, let's find the largest bounding box with the target size ratio that fits within the image\n",
        "    height = image.shape[0]\n",
        "    width = image.shape[1]\n",
        "    image_ratio = width / height\n",
        "    target_image_ratio = target_width / target_height\n",
        "    crop_vertically = image_ratio < target_image_ratio\n",
        "    crop_width = width if crop_vertically else int(height * target_image_ratio)\n",
        "    crop_height = int(width / target_image_ratio) if crop_vertically else height\n",
        "        \n",
        "    # Now let's shrink this bounding box by a random factor (dividing the dimensions by a random number\n",
        "    # between 1.0 and 1.0 + `max_zoom`.\n",
        "    resize_factor = np.random.rand() * max_zoom + 1.0\n",
        "    crop_width = int(crop_width / resize_factor)\n",
        "    crop_height = int(crop_height / resize_factor)\n",
        "    \n",
        "    # Next, we can select a random location on the image for this bounding box.\n",
        "    x0 = np.random.randint(0, width - crop_width)\n",
        "    y0 = np.random.randint(0, height - crop_height)\n",
        "    x1 = x0 + crop_width\n",
        "    y1 = y0 + crop_height\n",
        "    \n",
        "    # Let's crop the image using the random bounding box we built.\n",
        "    image = image[y0:y1, x0:x1]\n",
        "\n",
        "    # Let's also flip the image horizontally with 50% probability:\n",
        "    if np.random.rand() < 0.5:\n",
        "        image = np.fliplr(image)\n",
        "\n",
        "    # Now, let's resize the image to the target dimensions.\n",
        "    image = imresize(image, (target_width, target_height))\n",
        "    \n",
        "    # Finally, let's ensure that the colors are represented as\n",
        "    # 32-bit floats ranging from 0.0 to 1.0 (for now):\n",
        "    return image.astype(np.float32) / 255\n",
        "\n",
        "\n",
        "\n",
        "def prepare_batch(flower_paths_and_classes, batch_size):\n",
        "    batch_paths_and_classes = sample(flower_paths_and_classes, batch_size)\n",
        "    images = [mpimg.imread(path)[:, :, :channels] for path, labels in batch_paths_and_classes]\n",
        "    prepared_images = [prepare_image(image) for image in images]\n",
        "    X_batch = 2 * np.stack(prepared_images) - 1 # Inception expects colors ranging from -1 to 1\n",
        "    y_batch = np.array([labels for path, labels in batch_paths_and_classes], dtype=np.int32)\n",
        "    return X_batch, y_batch \n",
        "  \n",
        "  \n",
        "def prepare_data(flower_paths_and_classes):\n",
        "    batch_paths_and_classes = flower_paths_and_classes\n",
        "    images = [mpimg.imread(path)[:, :, :channels] for path, labels in batch_paths_and_classes]\n",
        "    prepared_images = [prepare_image(image) for image in images]\n",
        "    X_data = 2 * np.stack(prepared_images) - 1 # Inception expects colors ranging from -1 to 1\n",
        "    y_data = np.array([labels for path, labels in batch_paths_and_classes], dtype=np.int32)\n",
        "    return X_data, y_data  \n",
        "  \n",
        "  \n",
        "#раннняя остановка\n",
        "def get_model_params():\n",
        "    gvars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
        "    return {gvar.op.name: value for gvar, value in zip(gvars, tf.get_default_session().run(gvars))}\n",
        "\n",
        "def restore_model_params(model_params):\n",
        "    gvar_names = list(model_params.keys())\n",
        "    assign_ops = {gvar_name: tf.get_default_graph().get_operation_by_name(gvar_name + \"/Assign\")\n",
        "                  for gvar_name in gvar_names}\n",
        "    init_values = {gvar_name: assign_op.inputs[1] for gvar_name, assign_op in assign_ops.items()}\n",
        "    feed_dict = {init_values[gvar_name]: model_params[gvar_name] for gvar_name in gvar_names}\n",
        "    tf.get_default_session().run(assign_ops, feed_dict=feed_dict)  \n",
        "  \n",
        "  #снова получить начальный график v3. На этот раз воспользуемся trainingзаполнителем\n",
        "\n",
        "flower_class_ids = {flower_class: index for index, flower_class in enumerate(flower_classes)}\n",
        "\n",
        "\n",
        "#Будет проще перетасовать набор данных, если мы представим его как список пар filepath / class:\n",
        "flower_paths_and_classes = []\n",
        "for flower_class, paths in image_paths.items():\n",
        "    for path in paths:\n",
        "        flower_paths_and_classes.append((path, flower_class_ids[flower_class]))\n",
        "        \n",
        "#Затем давайте перетасовать набор данных и разделим его на обучающий набор и тестовый набор:\n",
        "\n",
        "\n",
        "np.random.shuffle(flower_paths_and_classes)\n",
        "a = flower_paths_and_classes\n",
        "flower_paths_and_classes_train = a[:int(len(a)*0.6)]\n",
        "flower_paths_and_classes_valid = a[int(len(a)*0.6):int(len(a)*0.8)]\n",
        "flower_paths_and_classes_test = a[int(len(a)*0.8):]\n",
        "\n",
        "n_iterations_per_epoch = len(flower_paths_and_classes_train) // batch_size\n",
        "\n",
        "print(\"train:\", len(flower_paths_and_classes_train) ,\"valid:\", len(flower_paths_and_classes_valid), \"test\", len(flower_paths_and_classes_test))\n",
        "\n",
        "#flower_paths_and_classes\n",
        "#print(\"flower_paths_and_classes\", len(flower_paths_and_classes))\n",
        "#print(\"flower_paths_and_classes_valid\", len(flower_paths_and_classes_valid))\n",
        "X_train, y_train = prepare_data(flower_paths_and_classes_train)\n",
        "X_test, y_test = prepare_data(flower_paths_and_classes_test)\n",
        "X_valid, y_valid = prepare_data(flower_paths_and_classes_valid)        \n",
        "#n_epochs = 10000\n",
        "#batch_size = 50       \n",
        "\n",
        "#best_loss_val = np.infty\n",
        "#check_interval = 100\n",
        "#checks_since_last_progress = 0\n",
        "#max_checks_without_progress = 20\n",
        "#best_model_params = None \n",
        "\n",
        "#step\n",
        "\n",
        "def shuffle_batch(X, batch_size):\n",
        "  #rnd_idx = [i for i in range(len(X)) if i>=batch_n*batch_size and i<(batch_n*batch_size+batch_size)]\n",
        "  #print(\"rnd_idx\", rnd_idx)\n",
        "  \n",
        "  print(\"X.shape\", X.shape)\n",
        "  rnd_idx = []\n",
        "  for i in range(len(X)): \n",
        "    rnd_idx.append(i)\n",
        "  print(\"rnd_idx.append\", rnd_idx)\n",
        "  for batch_idx in np.array_split(rnd_idx, batch_size):   # np.array_split делит масим на несколько массивов\n",
        "      #print(\"batch_idx\", batch_idx)  \n",
        "      X_batch = X[batch_idx]\n",
        "      \n",
        "      yield X_batch      \n",
        "\n",
        "\n",
        "def get_var_list(target_tensor=None):\n",
        "  \n",
        "    if target_tensor==None:\n",
        "        target_tensor = \"MobilenetV2/expanded_conv_15/output:0\"\n",
        "    target = target_tensor.split(\"/\")[1]\n",
        "    all_list = []\n",
        "    all_var = []\n",
        "\n",
        "    for var in tf.global_variables():\n",
        "        if var != []:\n",
        "            all_list.append(var.name)\n",
        "            all_var.append(var)\n",
        "    try:\n",
        "        all_list = list(map(lambda x:x.split(\"/\")[1],all_list))\n",
        "\n",
        "        ind = all_list[::-1].index(target)\n",
        "        ind = len(all_list) -  ind - 1\n",
        "        print(ind)\n",
        "        del all_list\n",
        "        return all_var[:ind+1]\n",
        "    except:\n",
        "        print(\"target_tensor is not exist!\")\n",
        "\n",
        "\n",
        "        \n",
        "def cash(height, width, channels, flower_classes):\n",
        "  X = tf.placeholder(tf.float32, shape=[None, height, width, channels], name=\"X\")\n",
        "  training = tf.placeholder_with_default(False, shape=[])\n",
        "\n",
        "  with slim.arg_scope(mobilenet_v2.training_scope(is_training=False)): #is_training=False\n",
        "    logits, end_points = mobilenet_v2.mobilenet(X)\n",
        "\n",
        "  #with slim.arg_scope(inception.inception_v3_arg_scope()):\n",
        "      #logits, end_points = inception.inception_v3(X, num_classes=1001, is_training=training)\n",
        "\n",
        "  inception_saver = tf.train.Saver() \n",
        "  n_outputs = len(flower_classes)\n",
        "  mobilenet_tensor = end_points[\"layer_17/output\"]\n",
        "  y = tf.placeholder(tf.int32, shape=[None])        \n",
        "  #target_tensor = \"MobilenetV2/expanded_conv_15/output:0\"\n",
        "  #var_list = get_var_list(target_tensor)\n",
        "  #saver = tf.train.Saver(var_list=var_list)      \n",
        "  init = tf.global_variables_initializer()      \n",
        "      \n",
        "      \n",
        "      \n",
        "\n",
        "  ############################## ВЫБИРАЕМ ПЕРЕМЕННЫЕ ##########################\n",
        "\n",
        "  reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"MobilenetV2\") # regular expression\n",
        "  restore_saver = tf.train.Saver(reuse_vars) # to restore layers 1-3\n",
        "\n",
        "  h2_cache_2 = np.empty([0, 7, 7, 160]) \n",
        "  print(h2_cache_2)\n",
        "  h2_cache_valid_2 = np.empty([0, 7, 7, 160])                                                              \n",
        "  init = tf.global_variables_initializer()\n",
        "  ################################################################################\n",
        "  with tf.Session() as sess:\n",
        "    \n",
        "      #sess.run(tf.variables_initializer(var_list=train_var))\n",
        "      inception_saver.restore(sess, os.path.join(MODELS_PATH, MODEL_NAME))\n",
        "      init.run()\n",
        "    \n",
        "     \n",
        "      for X_batch in shuffle_batch(X_train, batch_size):\n",
        "           \n",
        "              h2_cache_1 = sess.run(mobilenet_tensor, feed_dict={X: X_batch, training: False})\n",
        "          \n",
        "              h2_cache_2 = np.concatenate((h2_cache_2, h2_cache_1))\n",
        "           \n",
        "      for X_val in shuffle_batch(X_valid, batch_size):\n",
        "              h2_cache_valid_1 = sess.run(mobilenet_tensor, feed_dict={X: X_val, training: False})\n",
        "            \n",
        "              h2_cache_valid_2 = np.concatenate((h2_cache_valid_2, h2_cache_valid_1))\n",
        "\n",
        "  return h2_cache_2, h2_cache_valid_2       \n",
        " \n",
        "    \n",
        "#h2_cache_2, h2_cache_valid_2 = cash(height, width, channels, flower_classes)\n",
        "#print(h2_cache_2.shape, h2_cache_valid_2.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train: 2202 valid: 734 test 734\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:73: DeprecationWarning: `imresize` is deprecated!\n",
            "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``skimage.transform.resize`` instead.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "DY3VSf6af5G9",
        "colab_type": "code",
        "outputId": "b48c3d0c-0042-42fd-e799-cdcf65d01625",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "batch_size = 50\n",
        "filters=256\n",
        "kernel_size=3\n",
        "#inception_saver = tf.train.Saver() \n",
        "n_outputs = len(flower_classes)\n",
        "\n",
        "#y_label = tf.placeholder(tf.int32, (None,10))\n",
        "ys = tf.placeholder(tf.int32, shape=[None])  \n",
        "images = tf.placeholder(tf.float32,(None,224,224,3))\n",
        "with slim.arg_scope(mobilenet_v2.training_scope(is_training=False)): #is_training=False\n",
        "    logits, end_points = mobilenet_v2.mobilenet(images)\n",
        "    \n",
        "with tf.variable_scope(\"finetune_layers\"): \n",
        "    \n",
        "    mobilenet_tensor = tf.get_default_graph().get_tensor_by_name(\"MobilenetV2/expanded_conv_15/output:0\")\n",
        "    x = slim.conv2d(mobilenet_tensor, num_outputs=960, kernel_size=[1, 1], stride=1)\n",
        "    x = slim.separable_conv2d(x, num_outputs=None, kernel_size=(3, 3), depth_multiplier=1, stride=1, rate=1,  padding='SAME') #normalizer_fn=slim.batch_norm,\n",
        "    x = slim.conv2d(x, num_outputs=320, kernel_size=[1, 1], activation_fn=tf.identity)\n",
        "    x = slim.conv2d(x, stride=1, kernel_size=[1, 1], num_outputs=1280)\n",
        "    x = tf.nn.avg_pool(x, ksize=[1, 7, 7, 1], strides=[1, 1, 1, 1], padding='VALID')\n",
        "    dropout = tf.nn.dropout(x, 0.5)\n",
        "    logits = slim.conv2d(dropout, 10, [1, 1], activation_fn=None, normalizer_fn=None, biases_initializer=tf.zeros_initializer())\n",
        "    logits = tf.squeeze(logits, [1, 2])\n",
        "    predictions = slim.softmax(logits)\n",
        "    \n",
        " \n",
        "'''\n",
        "####################################layers from mobilnet########################     \n",
        "    \n",
        "    x = tf.layers.Conv2D(filters=256,kernel_size=3,name=\"conv2d_1\")(mobilenet_tensor)\n",
        "    x = tf.nn.relu(x,name=\"relu_1\")\n",
        "    x = tf.layers.Conv2D(filters=256,kernel_size=3,name=\"conv2d_2\")(x)\n",
        "    x = tf.layers.Conv2D(10,3,name=\"conv2d_3\")(x)\n",
        "    logits = tf.reshape(x, (-1,10))\n",
        "################################################################################\n",
        "''' \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "with tf.name_scope(\"train1\"):     \n",
        "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=ys)\n",
        "    loss = tf.reduce_mean(xentropy)\n",
        "    train_var = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"finetune_layers\")\n",
        "    training_op = optimizer.minimize(loss, var_list=train_var)\n",
        "    training_op = tf.train.AdamOptimizer().minimize(loss, var_list=train_var)\n",
        "    #training_op = tf.train.GradientDescentOptimizer(0.0001).minimize(loss, var_list=train_var)   \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "with tf.name_scope(\"eval1\"):\n",
        "    correct = tf.nn.in_top_k(logits, ys, 1)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
        "\n",
        "#init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver() \n",
        "\n",
        "\n",
        "'''\n",
        "def get_var_list(target_tensor=None):\n",
        "   \n",
        "    if target_tensor==None:\n",
        "        target_tensor = \"MobilenetV2/expanded_conv_15/output:0\"\n",
        "    target = target_tensor.split(\"/\")[1]\n",
        "    all_list = []\n",
        "    all_var = []\n",
        "    # 遍历所有变量，node.name得到变量名称\n",
        "    # 不使用tf.trainable_variables()，因为batchnorm的moving_mean/variance不属于可训练变量\n",
        "    for var in tf.global_variables():\n",
        "        if var != []:\n",
        "            all_list.append(var.name)\n",
        "            all_var.append(var)\n",
        "    try:\n",
        "        all_list = list(map(lambda x:x.split(\"/\")[1],all_list))\n",
        "        # 查找对应变量作用域的索引\n",
        "        ind = all_list[::-1].index(target)\n",
        "        ind = len(all_list) -  ind - 1\n",
        "        print(ind)\n",
        "        del all_list\n",
        "        return all_var[:ind+1]\n",
        "    except:\n",
        "        print(\"target_tensor is not exist!\")\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "#target_tensor = \"MobilenetV2/expanded_conv_15/output:0\"\n",
        "#var_list = get_var_list(target_tensor)\n",
        "\n",
        "var_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"MobilenetV2\")\n",
        "#saver = tf.train.Saver()\n",
        "#init = tf.global_variables_initializer()\n",
        "import time\n",
        "n_epochs = 100000\n",
        "\n",
        "from IPython.display import clear_output\n",
        "max_checks_without_progress = 1000\n",
        "checks_without_progress = 0\n",
        "best_loss = np.infty\n",
        "init = tf.global_variables_initializer() # for Adam\n",
        "saver = tf.train.Saver(var_list=var_list)  #var_list=var_list)\n",
        "\n",
        "with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n",
        "    \n",
        "    init.run() # for Adam\n",
        "    sess.run(tf.variables_initializer(var_list=train_var))\n",
        "\n",
        "    #saver.restore(sess,tf.train.latest_checkpoint(MODELS_PATH))\n",
        "    saver.restore(sess, os.path.join(MODELS_PATH, MODEL_NAME))\n",
        "    #(os.path.join(MODELS_PATH, MODEL_NAME)))\n",
        "    #for var in train_var:\n",
        "     #   var.initializer.run()\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        rnd_idx = np.random.permutation(2200)\n",
        "        for rnd_indices in np.array_split(rnd_idx, 2200 // batch_size):\n",
        "            X_batch, y_batch = X_train[rnd_indices], y_train[rnd_indices]\n",
        "            sess.run(training_op, feed_dict={images: X_batch, ys: y_batch})\n",
        "            #print(\"h5_batch\", h5_batch.shape, \"y_batch\", y_batch.shape )\n",
        "        loss_val, acc_val = sess.run([loss, accuracy], feed_dict={images: X_valid[:50], ys: y_valid[:50]})\n",
        "        #loss_val = loss.eval(feed_dict={X: h2_cache_valid_2[:10], ys: y_valid[:10]})\n",
        "        if loss_val < best_loss:\n",
        "            save_path = saver.save(sess, \"./my_mnist_model_5_to_9_five_frozen\")\n",
        "            best_loss = loss_val\n",
        "            checks_without_progress = 0\n",
        "        else:\n",
        "            checks_without_progress += 1\n",
        "            if checks_without_progress > max_checks_without_progress:\n",
        "                print(\"Early stopping!\")\n",
        "                break\n",
        "        if epoch % 1 == 0:\n",
        "          \n",
        "          \n",
        "          #clear_output(wait=True)\n",
        "          print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n",
        "              epoch, loss_val, best_loss, acc_val * 100))\n",
        "          \n",
        "          #print(tf.train.get_or_create_global_step())\n",
        "          #print(printm())\n",
        "\n",
        "    t1 = time.time()\n",
        "    print(\"Total training time: {:.1f}s\".format(t1 - t0))\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    saver.restore(sess, \"./my_mnist_model_5_to_9_five_frozen\")\n",
        "    acc_test = accuracy.eval(feed_dict={images: X_test[:50], ys: y_test[:50]})\n",
        "    print(\"Final test accuracy: {:.2f}%\".format(acc_test * 100))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/mobilenet_v2_1/mobilenet_v2_1.0_224.ckpt\n",
            "0\tValidation loss: 0.448671\tBest loss: 0.448671\tAccuracy: 80.00%\n",
            "1\tValidation loss: 0.440637\tBest loss: 0.440637\tAccuracy: 86.00%\n",
            "2\tValidation loss: 0.421484\tBest loss: 0.421484\tAccuracy: 86.00%\n",
            "3\tValidation loss: 0.446935\tBest loss: 0.421484\tAccuracy: 84.00%\n",
            "4\tValidation loss: 0.479475\tBest loss: 0.421484\tAccuracy: 88.00%\n",
            "5\tValidation loss: 0.457684\tBest loss: 0.421484\tAccuracy: 88.00%\n",
            "6\tValidation loss: 0.456539\tBest loss: 0.421484\tAccuracy: 88.00%\n",
            "7\tValidation loss: 0.608155\tBest loss: 0.421484\tAccuracy: 84.00%\n",
            "8\tValidation loss: 0.726347\tBest loss: 0.421484\tAccuracy: 88.00%\n",
            "9\tValidation loss: 0.840778\tBest loss: 0.421484\tAccuracy: 88.00%\n",
            "10\tValidation loss: 0.779637\tBest loss: 0.421484\tAccuracy: 88.00%\n",
            "11\tValidation loss: 0.689861\tBest loss: 0.421484\tAccuracy: 84.00%\n",
            "12\tValidation loss: 0.702121\tBest loss: 0.421484\tAccuracy: 86.00%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iF0F6_8cDC0O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "var_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"MobilenetV2\")\n",
        "var_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FOuNWx7FLVvy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if target_tensor==None:\n",
        "        target_tensor = \"MobilenetV2/expanded_conv_15/output:0\"\n",
        "\n",
        "target = target_tensor.split(\"/\")[1]\n",
        "all_list = []\n",
        "all_var = []\n",
        "var_plus = []\n",
        "for var in tf.global_variables():\n",
        "        if var != []:\n",
        "            all_list.append(var.name)\n",
        "            all_var.append(var)\n",
        "try:\n",
        "    all_list = list(map(lambda x:x.split(\"/\")[1],all_list))\n",
        "\n",
        "    #ind = all_list.index(target)\n",
        "    for i, j in enumerate(all_list):\n",
        "      if j == target:\n",
        "          ind = i\n",
        "          var_plus.append(all_var[i])\n",
        "          print(var_plus)\n",
        "    return var_plus[:ind+1]\n",
        "except:    \n",
        "    print(\"target_tensor is not exist!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ocUBKDBASAgF",
        "colab_type": "code",
        "outputId": "43ac6a8c-a328-4196-be67-fe68d93c1713",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "train_var"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Variable 'finetune_layers/conv2d_1/kernel:0' shape=(3, 3, 160, 256) dtype=float32_ref>,\n",
              " <tf.Variable 'finetune_layers/conv2d_1/bias:0' shape=(256,) dtype=float32_ref>,\n",
              " <tf.Variable 'finetune_layers/conv2d_2/kernel:0' shape=(3, 3, 256, 256) dtype=float32_ref>,\n",
              " <tf.Variable 'finetune_layers/conv2d_2/bias:0' shape=(256,) dtype=float32_ref>,\n",
              " <tf.Variable 'finetune_layers/conv2d_3/kernel:0' shape=(3, 3, 256, 10) dtype=float32_ref>,\n",
              " <tf.Variable 'finetune_layers/conv2d_3/bias:0' shape=(10,) dtype=float32_ref>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "metadata": {
        "id": "YxioygNOsNWo",
        "colab_type": "code",
        "outputId": "96801524-dbbc-422d-8de1-330d57f64951",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "reset_opt_vars = adam_variables_initializer(optimizer, train_var)\n",
        "reset_opt_vars"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Operation 'init_1' type=NoOp>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "metadata": {
        "id": "-qv0xngWNy7n",
        "colab_type": "code",
        "outputId": "25fef747-d67c-48a9-b6eb-dec08aacab3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "opt = tf.train.AdamOptimizer(name=\"Adam\")\n",
        "training_op = opt.minimize(loss, var_list=train_var)\n",
        "print(opt.get_slot_names())\n",
        "# prints ['momentum']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['m', 'v']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IcHhTycTN5gy",
        "colab_type": "code",
        "outputId": "584bc513-8b70-4e06-d806-292d6370ea2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "cell_type": "code",
      "source": [
        "opt.get_slot(some_var, 'Adam')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-84-c6d52f4ff78e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_slot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msome_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Adam'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'some_var' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "fOlF2ckjtqGC",
        "colab_type": "code",
        "outputId": "23cdb857-875b-48d7-a113-1b86bfd92404",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "cell_type": "code",
      "source": [
        "momentum_initializers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Operation 'finetune_layers/conv2d_1/kernel/Adam/Assign' type=Assign>,\n",
              " <tf.Operation 'finetune_layers/conv2d_1/kernel/Adam_1/Assign' type=Assign>,\n",
              " <tf.Operation 'finetune_layers/conv2d_1/bias/Adam/Assign' type=Assign>,\n",
              " <tf.Operation 'finetune_layers/conv2d_1/bias/Adam_1/Assign' type=Assign>,\n",
              " <tf.Operation 'finetune_layers/conv2d_2/kernel/Adam/Assign' type=Assign>,\n",
              " <tf.Operation 'finetune_layers/conv2d_2/kernel/Adam_1/Assign' type=Assign>,\n",
              " <tf.Operation 'finetune_layers/conv2d_2/bias/Adam/Assign' type=Assign>,\n",
              " <tf.Operation 'finetune_layers/conv2d_2/bias/Adam_1/Assign' type=Assign>,\n",
              " <tf.Operation 'finetune_layers/conv2d_3/kernel/Adam/Assign' type=Assign>,\n",
              " <tf.Operation 'finetune_layers/conv2d_3/kernel/Adam_1/Assign' type=Assign>,\n",
              " <tf.Operation 'finetune_layers/conv2d_3/bias/Adam/Assign' type=Assign>,\n",
              " <tf.Operation 'finetune_layers/conv2d_3/bias/Adam_1/Assign' type=Assign>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "metadata": {
        "id": "eF9epR3FuFdN",
        "colab_type": "code",
        "outputId": "7e46a2e2-7757-49ae-883e-bfde095b1c6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "cell_type": "code",
      "source": [
        "saver.restore(sess,tf.train.latest_checkpoint(MODELS_PATH))  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-5dd5663aa99d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODELS_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1532\u001b[0m       \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1533\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msave_path\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1534\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can't load save_path when it is None.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1536\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheckpoint_management\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Can't load save_path when it is None."
          ]
        }
      ]
    }
  ]
}