{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "17122018.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joker2017/Calculator/blob/master/29122018.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "yl0_vUGm2U_x",
        "colab_type": "code",
        "outputId": "9b940ec9-434b-4393-9451-67322290ec93",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7Ci8vIE1heCBhbW91bnQgb2YgdGltZSB0byBibG9jayB3YWl0aW5nIGZvciB0aGUgdXNlci4KY29uc3QgRklMRV9DSEFOR0VfVElNRU9VVF9NUyA9IDMwICogMTAwMDsKCmZ1bmN0aW9uIF91cGxvYWRGaWxlcyhpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IHN0ZXBzID0gdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKTsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIC8vIENhY2hlIHN0ZXBzIG9uIHRoZSBvdXRwdXRFbGVtZW50IHRvIG1ha2UgaXQgYXZhaWxhYmxlIGZvciB0aGUgbmV4dCBjYWxsCiAgLy8gdG8gdXBsb2FkRmlsZXNDb250aW51ZSBmcm9tIFB5dGhvbi4KICBvdXRwdXRFbGVtZW50LnN0ZXBzID0gc3RlcHM7CgogIHJldHVybiBfdXBsb2FkRmlsZXNDb250aW51ZShvdXRwdXRJZCk7Cn0KCi8vIFRoaXMgaXMgcm91Z2hseSBhbiBhc3luYyBnZW5lcmF0b3IgKG5vdCBzdXBwb3J0ZWQgaW4gdGhlIGJyb3dzZXIgeWV0KSwKLy8gd2hlcmUgdGhlcmUgYXJlIG11bHRpcGxlIGFzeW5jaHJvbm91cyBzdGVwcyBhbmQgdGhlIFB5dGhvbiBzaWRlIGlzIGdvaW5nCi8vIHRvIHBvbGwgZm9yIGNvbXBsZXRpb24gb2YgZWFjaCBzdGVwLgovLyBUaGlzIHVzZXMgYSBQcm9taXNlIHRvIGJsb2NrIHRoZSBweXRob24gc2lkZSBvbiBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcCwKLy8gdGhlbiBwYXNzZXMgdGhlIHJlc3VsdCBvZiB0aGUgcHJldmlvdXMgc3RlcCBhcyB0aGUgaW5wdXQgdG8gdGhlIG5leHQgc3RlcC4KZnVuY3Rpb24gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpIHsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIGNvbnN0IHN0ZXBzID0gb3V0cHV0RWxlbWVudC5zdGVwczsKCiAgY29uc3QgbmV4dCA9IHN0ZXBzLm5leHQob3V0cHV0RWxlbWVudC5sYXN0UHJvbWlzZVZhbHVlKTsKICByZXR1cm4gUHJvbWlzZS5yZXNvbHZlKG5leHQudmFsdWUucHJvbWlzZSkudGhlbigodmFsdWUpID0+IHsKICAgIC8vIENhY2hlIHRoZSBsYXN0IHByb21pc2UgdmFsdWUgdG8gbWFrZSBpdCBhdmFpbGFibGUgdG8gdGhlIG5leHQKICAgIC8vIHN0ZXAgb2YgdGhlIGdlbmVyYXRvci4KICAgIG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSA9IHZhbHVlOwogICAgcmV0dXJuIG5leHQudmFsdWUucmVzcG9uc2U7CiAgfSk7Cn0KCi8qKgogKiBHZW5lcmF0b3IgZnVuY3Rpb24gd2hpY2ggaXMgY2FsbGVkIGJldHdlZW4gZWFjaCBhc3luYyBzdGVwIG9mIHRoZSB1cGxvYWQKICogcHJvY2Vzcy4KICogQHBhcmFtIHtzdHJpbmd9IGlucHV0SWQgRWxlbWVudCBJRCBvZiB0aGUgaW5wdXQgZmlsZSBwaWNrZXIgZWxlbWVudC4KICogQHBhcmFtIHtzdHJpbmd9IG91dHB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIG91dHB1dCBkaXNwbGF5LgogKiBAcmV0dXJuIHshSXRlcmFibGU8IU9iamVjdD59IEl0ZXJhYmxlIG9mIG5leHQgc3RlcHMuCiAqLwpmdW5jdGlvbiogdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKSB7CiAgY29uc3QgaW5wdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQoaW5wdXRJZCk7CiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gZmFsc2U7CgogIGNvbnN0IG91dHB1dEVsZW1lbnQgPSBkb2N1bWVudC5nZXRFbGVtZW50QnlJZChvdXRwdXRJZCk7CiAgb3V0cHV0RWxlbWVudC5pbm5lckhUTUwgPSAnJzsKCiAgY29uc3QgcGlja2VkUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBpbnB1dEVsZW1lbnQuYWRkRXZlbnRMaXN0ZW5lcignY2hhbmdlJywgKGUpID0+IHsKICAgICAgcmVzb2x2ZShlLnRhcmdldC5maWxlcyk7CiAgICB9KTsKICB9KTsKCiAgY29uc3QgY2FuY2VsID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnYnV0dG9uJyk7CiAgaW5wdXRFbGVtZW50LnBhcmVudEVsZW1lbnQuYXBwZW5kQ2hpbGQoY2FuY2VsKTsKICBjYW5jZWwudGV4dENvbnRlbnQgPSAnQ2FuY2VsIHVwbG9hZCc7CiAgY29uc3QgY2FuY2VsUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBjYW5jZWwub25jbGljayA9ICgpID0+IHsKICAgICAgcmVzb2x2ZShudWxsKTsKICAgIH07CiAgfSk7CgogIC8vIENhbmNlbCB1cGxvYWQgaWYgdXNlciBoYXNuJ3QgcGlja2VkIGFueXRoaW5nIGluIHRpbWVvdXQuCiAgY29uc3QgdGltZW91dFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgc2V0VGltZW91dCgoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9LCBGSUxFX0NIQU5HRV9USU1FT1VUX01TKTsKICB9KTsKCiAgLy8gV2FpdCBmb3IgdGhlIHVzZXIgdG8gcGljayB0aGUgZmlsZXMuCiAgY29uc3QgZmlsZXMgPSB5aWVsZCB7CiAgICBwcm9taXNlOiBQcm9taXNlLnJhY2UoW3BpY2tlZFByb21pc2UsIHRpbWVvdXRQcm9taXNlLCBjYW5jZWxQcm9taXNlXSksCiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdzdGFydGluZycsCiAgICB9CiAgfTsKCiAgaWYgKCFmaWxlcykgewogICAgcmV0dXJuIHsKICAgICAgcmVzcG9uc2U6IHsKICAgICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICAgIH0KICAgIH07CiAgfQoKICBjYW5jZWwucmVtb3ZlKCk7CgogIC8vIERpc2FibGUgdGhlIGlucHV0IGVsZW1lbnQgc2luY2UgZnVydGhlciBwaWNrcyBhcmUgbm90IGFsbG93ZWQuCiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gdHJ1ZTsKCiAgZm9yIChjb25zdCBmaWxlIG9mIGZpbGVzKSB7CiAgICBjb25zdCBsaSA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2xpJyk7CiAgICBsaS5hcHBlbmQoc3BhbihmaWxlLm5hbWUsIHtmb250V2VpZ2h0OiAnYm9sZCd9KSk7CiAgICBsaS5hcHBlbmQoc3BhbigKICAgICAgICBgKCR7ZmlsZS50eXBlIHx8ICduL2EnfSkgLSAke2ZpbGUuc2l6ZX0gYnl0ZXMsIGAgKwogICAgICAgIGBsYXN0IG1vZGlmaWVkOiAkewogICAgICAgICAgICBmaWxlLmxhc3RNb2RpZmllZERhdGUgPyBmaWxlLmxhc3RNb2RpZmllZERhdGUudG9Mb2NhbGVEYXRlU3RyaW5nKCkgOgogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAnbi9hJ30gLSBgKSk7CiAgICBjb25zdCBwZXJjZW50ID0gc3BhbignMCUgZG9uZScpOwogICAgbGkuYXBwZW5kQ2hpbGQocGVyY2VudCk7CgogICAgb3V0cHV0RWxlbWVudC5hcHBlbmRDaGlsZChsaSk7CgogICAgY29uc3QgZmlsZURhdGFQcm9taXNlID0gbmV3IFByb21pc2UoKHJlc29sdmUpID0+IHsKICAgICAgY29uc3QgcmVhZGVyID0gbmV3IEZpbGVSZWFkZXIoKTsKICAgICAgcmVhZGVyLm9ubG9hZCA9IChlKSA9PiB7CiAgICAgICAgcmVzb2x2ZShlLnRhcmdldC5yZXN1bHQpOwogICAgICB9OwogICAgICByZWFkZXIucmVhZEFzQXJyYXlCdWZmZXIoZmlsZSk7CiAgICB9KTsKICAgIC8vIFdhaXQgZm9yIHRoZSBkYXRhIHRvIGJlIHJlYWR5LgogICAgbGV0IGZpbGVEYXRhID0geWllbGQgewogICAgICBwcm9taXNlOiBmaWxlRGF0YVByb21pc2UsCiAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgYWN0aW9uOiAnY29udGludWUnLAogICAgICB9CiAgICB9OwoKICAgIC8vIFVzZSBhIGNodW5rZWQgc2VuZGluZyB0byBhdm9pZCBtZXNzYWdlIHNpemUgbGltaXRzLiBTZWUgYi82MjExNTY2MC4KICAgIGxldCBwb3NpdGlvbiA9IDA7CiAgICB3aGlsZSAocG9zaXRpb24gPCBmaWxlRGF0YS5ieXRlTGVuZ3RoKSB7CiAgICAgIGNvbnN0IGxlbmd0aCA9IE1hdGgubWluKGZpbGVEYXRhLmJ5dGVMZW5ndGggLSBwb3NpdGlvbiwgTUFYX1BBWUxPQURfU0laRSk7CiAgICAgIGNvbnN0IGNodW5rID0gbmV3IFVpbnQ4QXJyYXkoZmlsZURhdGEsIHBvc2l0aW9uLCBsZW5ndGgpOwogICAgICBwb3NpdGlvbiArPSBsZW5ndGg7CgogICAgICBjb25zdCBiYXNlNjQgPSBidG9hKFN0cmluZy5mcm9tQ2hhckNvZGUuYXBwbHkobnVsbCwgY2h1bmspKTsKICAgICAgeWllbGQgewogICAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgICBhY3Rpb246ICdhcHBlbmQnLAogICAgICAgICAgZmlsZTogZmlsZS5uYW1lLAogICAgICAgICAgZGF0YTogYmFzZTY0LAogICAgICAgIH0sCiAgICAgIH07CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPQogICAgICAgICAgYCR7TWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCl9JSBkb25lYDsKICAgIH0KICB9CgogIC8vIEFsbCBkb25lLgogIHlpZWxkIHsKICAgIHJlc3BvbnNlOiB7CiAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgIH0KICB9Owp9CgpzY29wZS5nb29nbGUgPSBzY29wZS5nb29nbGUgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYiA9IHNjb3BlLmdvb2dsZS5jb2xhYiB8fCB7fTsKc2NvcGUuZ29vZ2xlLmNvbGFiLl9maWxlcyA9IHsKICBfdXBsb2FkRmlsZXMsCiAgX3VwbG9hZEZpbGVzQ29udGludWUsCn07Cn0pKHNlbGYpOwo=",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "  \n",
        "print( os.listdir('/content') )"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-688a4c1e-bb50-4ff1-947e-62427cf7d49a\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-688a4c1e-bb50-4ff1-947e-62427cf7d49a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving imagenet_class_names.txt to imagenet_class_names (2).txt\n",
            "User uploaded file \"imagenet_class_names.txt\" with length 32673 bytes\n",
            "['.config', 'imagenet_class_names (1).txt', 'imagenet_class_names.txt', 'inception_v3_2016_08_28', 'flower_photos', 'imagenet_class_names (2).txt', 'sample_data']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JeO02mGVr6VA",
        "colab_type": "code",
        "outputId": "a9c39c36-e527-4a49-d3e1-fec8cfae7bbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#Загрузите последнюю предварительную модель Inception v3: контрольная точка доступна по адресу https://github.com/tensorflow/models/tree/master/research/slim . \n",
        "#писок имен классов доступен по адресу https://goo.gl/brXRtZ , но вы должны сначала вставить «background» класс.\n",
        "import os\n",
        "import re \n",
        "import sys\n",
        "import tarfile\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from random import sample\n",
        "from six.moves import urllib\n",
        "import matplotlib.image as mpimg\n",
        "import tensorflow.contrib.slim as slim\n",
        "from tensorflow.contrib.slim.nets import inception\n",
        "from skimage.transform import rescale, resize, downscale_local_mean\n",
        "\n",
        "MODEL_NAME = \"inception_v3.ckpt\"\n",
        "CLASS_NAME_REGEX = re.compile(r\"^n\\d+\\s+(.*)\\s*$\", re.M | re.U)\n",
        "DATASET_URL = \"http://download.tensorflow.org/example_images/flower_photos.tgz\"\n",
        "MODEL_URL = \"http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz\"\n",
        "\n",
        "\n",
        "\n",
        "def download_progress(count, block_size, total_size):\n",
        "    percent = count * block_size * 100 // total_size\n",
        "    sys.stdout.write(\"\\rDownloading: {}%\".format(percent))\n",
        "    sys.stdout.flush()\n",
        "    \n",
        "\n",
        "def download_tgz(url):\n",
        "    file_name=os.path.basename(url)\n",
        "    name_parts = os.path.split(file_name)\n",
        "    _, name = name_parts\n",
        "    name_path = name.rsplit( \".\", 2 )[ 0 ]\n",
        "    path = os.path.join(\"/content\", name_path)\n",
        "    tgz_path = os.path.join(path, name)\n",
        "    if os.path.exists(path): #and os.path.isfile(tgz_path):\n",
        "      print(\"file and path exist \", tgz_path, \"Path is:\", os.listdir(path), \"!\")\n",
        "      return path\n",
        "    else:\n",
        "      print(os.path.exists(path), os.path.isfile(tgz_path), path, tgz_path)\n",
        "      os.makedirs(path, exist_ok=True)\n",
        "      urllib.request.urlretrieve(url, tgz_path, reporthook=download_progress)\n",
        "      tmp_tgz = tarfile.open(tgz_path)\n",
        "      tmp_tgz.extractall(path=path)\n",
        "      tmp_tgz.close()\n",
        "      os.remove(tgz_path)\n",
        "      print(\"path created:\", path)\n",
        "      return path\n",
        "    \n",
        "# Загружаем модель\n",
        "MODELS_PATH = download_tgz(MODEL_URL)\n",
        "# Загружаем датасет\n",
        "FLOWERS_PATH = download_tgz(DATASET_URL)\n",
        " \n",
        "\n",
        "def load_class_names():\n",
        "    with open(os.path.join(\"/content\", \"imagenet_class_names.txt\"), \"rb\") as f:\n",
        "        content = f.read().decode(\"utf-8\")\n",
        "        #sys.stdout.flush()\n",
        "        return CLASS_NAME_REGEX.findall(content)\n",
        "      \n",
        "load_class_names()    \n",
        "class_names = [\"background\"] + load_class_names()       \n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "file and path exist  /content/inception_v3_2016_08_28/inception_v3_2016_08_28.tar.gz Path is: ['inception_v3.ckpt'] !\n",
            "file and path exist  /content/flower_photos/flower_photos.tgz Path is: ['flower_photos'] !\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_mcfvHNWTo9Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.contrib.slim.nets import inception\n",
        "import tensorflow.contrib.slim as slim\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=[None, height, width, channels], name=\"X\")\n",
        "training = tf.placeholder_with_default(False, shape=[])\n",
        "with slim.arg_scope(inception.inception_v3_arg_scope()):\n",
        "    logits, end_points = inception.inception_v3(X, num_classes=1001, is_training=training)\n",
        "\n",
        "inception_saver = tf.train.Saver()\n",
        "\n",
        "end_points"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IxKDcAXkUJzF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "flower_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"InceptionV3\")\n",
        "flower_vars"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SjLFWKnGo4FR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#MODEL_URL\n",
        "import os\n",
        "name_parts = os.path.splitext(MODEL_URL)  \n",
        "file, name = name_parts\n",
        "print(file, name)\n",
        "name_parts = os.path.split(MODEL_URL)  \n",
        "file, name = name_parts\n",
        "print(file, name)\n",
        "print(name.rsplit( \".\", 2 )[ 0 ]) \n",
        "print(os.path.dirname(MODEL_URL))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nSZTTwcEvLG9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "print( os.getcwd() )\n",
        "print( os.listdir('/content/flower_photos/flower_photos') )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WNOC6n9out14",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class_names[:5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Jwe1UXvnqIK2",
        "colab_type": "code",
        "outputId": "c532c27e-a2dc-4373-f52b-814d3cbc4b10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "width = 299\n",
        "height = 299\n",
        "channels = 3\n",
        "import tensorflow as tf\n",
        "tf.reset_default_graph()\n",
        "n_epochs = 1000\n",
        "batch_size = 50\n",
        "best_loss_val = np.infty\n",
        "check_interval = 500\n",
        "checks_since_last_progress = 0\n",
        "max_checks_without_progress = 20\n",
        "best_model_params = None \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Каждый подкаталог flower_photos каталога содержит все изображения данного класса. Давайте получим список классов:\n",
        "flowers_root_path = os.path.join(FLOWERS_PATH, \"flower_photos\")\n",
        "flower_classes = sorted([dirname for dirname in os.listdir(flowers_root_path)\n",
        "                  if os.path.isdir(os.path.join(flowers_root_path, dirname))])\n",
        "\n",
        "#Давайте получим список всех путей файлов изображений для каждого класса:\n",
        "\n",
        "from collections import defaultdict\n",
        "image_paths = defaultdict(list)\n",
        "for flower_class in flower_classes:\n",
        "    image_dir = os.path.join(flowers_root_path, flower_class)\n",
        "    for filepath in os.listdir(image_dir):\n",
        "        if filepath.endswith(\".jpg\"):\n",
        "            image_paths[flower_class].append(os.path.join(image_dir, filepath))\n",
        "            \n",
        "            \n",
        "#Давайте сортируем пути изображения, чтобы заставить этот ноутбук вести себя последовательно на нескольких запусках:\n",
        "for paths in image_paths.values():\n",
        "    paths.sort() \n",
        "\n",
        "    \n",
        "#Для получения дополнительных функций манипуляции с изображениями, таких как вращения, проверьте документацию SciPy или эту приятную страницу .\n",
        "def prepare_image(image, target_width = 299, target_height = 299, max_zoom = 0.2):\n",
        "    \"\"\"Zooms and crops the image randomly for data augmentation.\"\"\"\n",
        "\n",
        "    \n",
        "    # Во-первых, давайте найдем самый большой ограничивающий прямоугольник с целевым соотношением размеров, которое вписывается в изображение\n",
        "    height = image.shape[0]\n",
        "    width = image.shape[1]\n",
        "    image_ratio = width / height\n",
        "    target_image_ratio = target_width / target_height\n",
        "    crop_vertically = image_ratio < target_image_ratio\n",
        "    crop_width = width if crop_vertically else int(height * target_image_ratio)\n",
        "    crop_height = int(width / target_image_ratio) if crop_vertically else height\n",
        "        \n",
        "   #Теперь давайте уменьшим этот ограничивающий прямоугольник случайным фактором (разделив размеры на случайное число\n",
        "   # между 1.0 и 1.0 + `max_zoom`\n",
        "    resize_factor = np.random.rand() * max_zoom + 1.0\n",
        "    crop_width = int(crop_width / resize_factor)\n",
        "    crop_height = int(crop_height / resize_factor)\n",
        "    \n",
        "    # Далее мы можем выбрать случайное место на изображении этого прямоугольника.\n",
        "    x0 = np.random.randint(0, width - crop_width)\n",
        "    y0 = np.random.randint(0, height - crop_height)\n",
        "    x1 = x0 + crop_width\n",
        "    y1 = y0 + crop_height\n",
        "    \n",
        "    # Давайте обрезать изображение, используя случайную ограничивающую рамку, которую мы построили.\n",
        "    image = image[y0:y1, x0:x1]\n",
        "\n",
        "    # Давайте также перевернем изображение по горизонтали с вероятностью 50% :\n",
        "    if np.random.rand() < 0.5:\n",
        "        image = np.fliplr(image)\n",
        "\n",
        "    # Теперь давайте изменим размер изображения до целевых размеров.\n",
        "    image = resize(image, (target_width, target_height), mode='reflect')\n",
        "    \n",
        "    # Наконец, давайте убедимся, что цвета представлены как\n",
        "    # 32-бит плавает в диапазоне от 0.0 до 1.0 (на данный момент):\n",
        "    return image.astype(np.float32) / 255\n",
        "  \n",
        "\n",
        "def prepare_batch(flower_paths_and_classes, batch_size):\n",
        "    batch_paths_and_classes = sample(flower_paths_and_classes, batch_size)\n",
        "    images = [mpimg.imread(path)[:, :, :channels] for path, labels in batch_paths_and_classes]\n",
        "    prepared_images = [prepare_image(image) for image in images]\n",
        "    X_batch = 2 * np.stack(prepared_images) - 1 # Inception expects colors ranging from -1 to 1\n",
        "    y_batch = np.array([labels for path, labels in batch_paths_and_classes], dtype=np.int32)\n",
        "    return X_batch, y_batch \n",
        "  \n",
        "  \n",
        "def prepare_data(flower_paths_and_classes):\n",
        "    batch_paths_and_classes = flower_paths_and_classes\n",
        "    images = [mpimg.imread(path)[:, :, :channels] for path, labels in batch_paths_and_classes]\n",
        "    prepared_images = [prepare_image(image) for image in images]\n",
        "    X_data = 2 * np.stack(prepared_images) - 1 # Inception expects colors ranging from -1 to 1\n",
        "    y_data = np.array([labels for path, labels in batch_paths_and_classes], dtype=np.int32)\n",
        "    return X_data, y_data  \n",
        "  \n",
        "  \n",
        "#раннняя остановка\n",
        "def get_model_params():\n",
        "    gvars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
        "    return {gvar.op.name: value for gvar, value in zip(gvars, tf.get_default_session().run(gvars))}\n",
        "\n",
        "def restore_model_params(model_params):\n",
        "    gvar_names = list(model_params.keys())\n",
        "    assign_ops = {gvar_name: tf.get_default_graph().get_operation_by_name(gvar_name + \"/Assign\")\n",
        "                  for gvar_name in gvar_names}\n",
        "    init_values = {gvar_name: assign_op.inputs[1] for gvar_name, assign_op in assign_ops.items()}\n",
        "    feed_dict = {init_values[gvar_name]: model_params[gvar_name] for gvar_name in gvar_names}\n",
        "    tf.get_default_session().run(assign_ops, feed_dict=feed_dict)  \n",
        "  \n",
        "  #снова получить начальный график v3. На этот раз воспользуемся trainingзаполнителем\n",
        "  \n",
        "X = tf.placeholder(tf.float32, shape=[None, height, width, channels], name=\"X\")\n",
        "training = tf.placeholder_with_default(False, shape=[])\n",
        "with slim.arg_scope(inception.inception_v3_arg_scope()):\n",
        "    logits, end_points = inception.inception_v3(X, num_classes=1001, is_training=training)\n",
        "\n",
        "inception_saver = tf.train.Saver()\n",
        "\n",
        "\n",
        "#изучить входные данные выходного уровня:\n",
        "# logits.op.inputs[0].op.inputs[0]  или end_points\n",
        "#end_points[\"PreLogits\"]\n",
        "\n",
        "#отказаться от 2-го и 3-го измерений с помощью tf.squeeze() функции\n",
        "prelogits = tf.squeeze(end_points[\"PreLogits\"], axis=[1, 2])\n",
        "\n",
        "#добавить окончательный полностью подключенный слой поверх этого слоя:  \n",
        "n_outputs = len(flower_classes)\n",
        "\n",
        "with tf.name_scope(\"new_output_layer\"):\n",
        "    flower_logits = tf.layers.dense(prelogits, n_outputs, name=\"flower_logits\")\n",
        "    Y_proba = tf.nn.softmax(flower_logits, name=\"Y_proba\")\n",
        "    \n",
        "\n",
        "y = tf.placeholder(tf.int32, shape=[None])\n",
        "\n",
        "with tf.name_scope(\"train\"):\n",
        "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=flower_logits, labels=y)\n",
        "    loss = tf.reduce_mean(xentropy)\n",
        "    optimizer = tf.train.AdamOptimizer()\n",
        "    flower_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"flower_logits\") #flower_logits\n",
        "    training_op = optimizer.minimize(loss, var_list=flower_vars)\n",
        "    print(flower_vars)\n",
        "\n",
        "with tf.name_scope(\"eval\"):\n",
        "    correct = tf.nn.in_top_k(flower_logits, y, 1)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
        "\n",
        "with tf.name_scope(\"init_and_save\"):\n",
        "    init = tf.global_variables_initializer()\n",
        "    saver = tf.train.Saver() \n",
        "    \n",
        "\n",
        "    \n",
        "#Разделите свой набор данных на тренировочный набор и тестовый набор. Обучите модель тренировочному набору и оцените его на тестовом наборе.\n",
        "#Во-первых, мы хотим представить классы как int, а не строки:\n",
        "flower_class_ids = {flower_class: index for index, flower_class in enumerate(flower_classes)}\n",
        "\n",
        "\n",
        "#Будет проще перетасовать набор данных, если мы представим его как список пар filepath / class:\n",
        "flower_paths_and_classes = []\n",
        "for flower_class, paths in image_paths.items():\n",
        "    for path in paths:\n",
        "        flower_paths_and_classes.append((path, flower_class_ids[flower_class]))\n",
        "        \n",
        "#Затем давайте перетасовать набор данных и разделим его на обучающий набор и тестовый набор:\n",
        "test_ratio = 0.2\n",
        "valid_ratio = 0.4\n",
        "train_size = int(len(flower_paths_and_classes) * (1 - test_ratio - valid_ratio))\n",
        "test_size = int(len(flower_paths_and_classes) * test_ratio)\n",
        "valid_size = int(len(flower_paths_and_classes) * valid_ratio)                \n",
        "np.random.shuffle(flower_paths_and_classes)\n",
        "flower_paths_and_classes_test = flower_paths_and_classes[:test_size]\n",
        "flower_paths_and_classes_valid = flower_paths_and_classes[test_size:valid_size]\n",
        "flower_paths_and_classes_train = flower_paths_and_classes[valid_size:]\n",
        "n_iterations_per_epoch = len(flower_paths_and_classes_train) // batch_size\n",
        "\n",
        "X_train, y_train = prepare_data(flower_paths_and_classes_train)\n",
        "X_test, y_test = prepare_data(flower_paths_and_classes_test)\n",
        "X_valid, y_valid = prepare_data(flower_paths_and_classes_valid)        \n",
        "n_epochs = 1000\n",
        "#batch_size = 50       \n",
        "\n",
        "best_loss_val = np.infty\n",
        "check_interval = 100\n",
        "checks_since_last_progress = 0\n",
        "max_checks_without_progress = 20\n",
        "best_model_params = None \n",
        "\n",
        "#step\n",
        "\n",
        "def shuffle_batch(X, batch_size, batch_n):\n",
        "#rnd_idx = np.random.permutation(len(X)) # np.random.permutation Случайно переставляйте последовательность или возвращайте заданный диапазон / len возвращает длину\n",
        "  rnd_idx = [i for i in range(len(X)) if i>batch_n*batch_size and i<(batch_n*batch_size+batch_size+1)]\n",
        " \n",
        " # n_batches = len(X) // batch_size #//\n",
        "\n",
        "  for batch_idx in np.array_split(rnd_idx, batch_size):  # np.array_split делит масим на несколько массивов\n",
        "        X_batch = X[batch_idx]\n",
        "\n",
        "  yield X_batch      \n",
        "\n",
        "  \n",
        "def shuffle_batch2 (X, y, batch_size):\n",
        "  rnd_id = np.random.permutation(len(X))\n",
        "  batch_rnd = len(X)//batch_size\n",
        "  for batch_n in np.array_split(rnd_id, batch_rnd):\n",
        "    X_batch, y_batch = X[batch_n], y[batch_n]\n",
        "    #print(X_batch, y_batch)\n",
        "  yield X_batch, y_batch\n",
        "\n",
        " \n",
        "############################## ВЫБИРАЕМ ПЕРЕМЕННЫЕ ##########################\n",
        "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"InceptionV3\") # regular expression\n",
        "restore_saver = tf.train.Saver(reuse_vars) # to restore layers 1-3\n",
        "#n_batches = len(X_train) // batch_size\n",
        "#batch_n = 1101\n",
        "#batch_n1 = 734\n",
        "batch_size = 3\n",
        "\n",
        "h2_cache_2 = []\n",
        "h2_cache = []\n",
        "h2_cache_1 = []\n",
        "#h2_cache_valid_1 = []\n",
        "h2_cache_valid = []\n",
        "################################################################################\n",
        "with tf.Session() as sess:\n",
        "    init.run()\n",
        "    inception_saver.restore(sess, os.path.join(MODELS_PATH, MODEL_NAME))\n",
        "    #h2_cache = prelogits.eval(feed_dict={X: X_train})\n",
        "    for batch_n in range(len(X_train)//batch_size): \n",
        "      for X_batch in shuffle_batch(X_train, batch_size, batch_n):\n",
        "            h2_cache_1 = sess.run(prelogits, feed_dict={X: X_batch})\n",
        "            if batch_n % 100 == 0:\n",
        "              print(h2_cache_1)\n",
        "              print(h2_cache_1.shape) \n",
        "            h2_cache_2.append(h2_cache_1)\n",
        "    h2_cache_without_resize = np.array(h2_cache_2)   #, dtype=np.object)\n",
        "    print(\"h2_cache_without_resize\", h2_cache_without_resize.shape)\n",
        "    h2_cache = np.resize(h2_cache_without_resize,(122,2048))\n",
        "    #print(\"h2_cache\", h2_cache.shape)\n",
        "           # np.extend\n",
        "    #for batch_n in range(len(X_valid)//batch_size): \n",
        "      #for X_batch in shuffle_batch(X_valid, batch_size, batch_n):\n",
        "           # h2_cache_valid_1 = sess.run(prelogits, feed_dict={X: X_batch})\n",
        "            #h2_cache_valid.extend(h2_cache_valid_1)\n",
        "            #h2_cache_valid = np.array(h2_cache_valid)\n",
        "   \n",
        "    \n",
        "############################## СОЗДАЕМ КЭШ ###################################\n",
        "   \n",
        "  # h2_cache = prelogits.eval(feed_dict={X: X_train})\n",
        "   # h2_cache_valid = sess.run(prelogits, feed_dict={X: X_valid})\n",
        "################################################################################  \n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<tf.Variable 'flower_logits/kernel:0' shape=(2048, 5) dtype=float32_ref>, <tf.Variable 'flower_logits/bias:0' shape=(5,) dtype=float32_ref>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "G4rJLsqN8gMY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a56e418c-21ae-45b6-b507-865a4f0b6b58"
      },
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(367, 299, 299, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "id": "2jZxYFi_l1_e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "e9734b69-4d2f-4bef-9bf0-0599e174521f"
      },
      "cell_type": "code",
      "source": [
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gputil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Building wheels for collected packages: gputil\n",
            "  Running setup.py bdist_wheel for gputil ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Gen RAM Free: 7.0 GB  | Proc size: 12.0 GB\n",
            "GPU RAM Free: 7933MB | Used: 3508MB | Util  31% | Total 11441MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "b_b3cs8HDi68",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "64df6b4d-745e-4d6a-82dc-2c2f857577b1"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#h2_cache = np.array(h2_cache)\n",
        "print(h2_cache[:3])\n",
        "h2_cache\n",
        "print(h2_cache.shape)\n",
        "h2_cache[:3]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.15117478 0.0227656  0.266469   ... 0.21273202 0.20294254 0.        ]\n",
            " [0.11786628 0.03779475 0.22354326 ... 0.27310672 0.16936435 0.        ]\n",
            " [0.17138407 0.04705916 0.13491091 ... 0.30150726 0.05455118 0.        ]]\n",
            "(122, 2048)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.15117478, 0.0227656 , 0.266469  , ..., 0.21273202, 0.20294254,\n",
              "        0.        ],\n",
              "       [0.11786628, 0.03779475, 0.22354326, ..., 0.27310672, 0.16936435,\n",
              "        0.        ],\n",
              "       [0.17138407, 0.04705916, 0.13491091, ..., 0.30150726, 0.05455118,\n",
              "        0.        ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "D6JzNEsBoyJe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "c517ba42-475f-4282-a590-6ca608323e81"
      },
      "cell_type": "code",
      "source": [
        "np.set_printoptions(threshold=10)\n",
        "h2_cache[2]\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.0046885 , 0.11755972, 0.04769261, ..., 0.15647194, 0.08709227,\n",
              "       0.00934501], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "metadata": {
        "id": "4zavCUbfYBvW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "97c5e8f7-8cd6-45e7-ed50-4215cbdccc33"
      },
      "cell_type": "code",
      "source": [
        "# НЕ ТРОГАТЬ\n",
        "print(h2_cache[:3])\n",
        "h2_cache\n",
        "print(h2_cache.shape)\n",
        "h2_cache[:3]"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.0523254  0.18964252 0.21357931 ... 0.33312514 0.12824863 0.        ]\n",
            " [0.08656099 0.05714273 0.11072858 ... 0.1827167  0.04561879 0.        ]\n",
            " [0.10064159 0.01881678 0.16078235 ... 0.22866349 0.10744427 0.        ]]\n",
            "(367, 2048)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.0523254 , 0.18964252, 0.21357931, ..., 0.33312514, 0.12824863,\n",
              "        0.        ],\n",
              "       [0.08656099, 0.05714273, 0.11072858, ..., 0.1827167 , 0.04561879,\n",
              "        0.        ],\n",
              "       [0.10064159, 0.01881678, 0.16078235, ..., 0.22866349, 0.10744427,\n",
              "        0.        ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "id": "ihMFdDv7mR6L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#shuffled_idx = np.random.permutation(len(X_train))\n",
        "#np.array_split(h2_cache[6,8,1], 2)\n",
        "#print(shuffled_idx)\n",
        "#h2_cache_1 \n",
        "#print(X_batch)\n",
        "h2_cache.extend(h2_cache)\n",
        "h2_cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DY3VSf6af5G9",
        "colab_type": "code",
        "outputId": "937284a9-fff8-4469-88bc-f3a7cc814575",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        }
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "    init.run()\n",
        "    h2_cache = np.array(h2_cache)\n",
        "    n_batches = len(X_train) // batch_size\n",
        "    # y_valid_batches = np.array_split(y_valid, n_valid_batches) \n",
        "    inception_saver.restore(sess, os.path.join(MODELS_PATH, MODEL_NAME))\n",
        "    #n_batches = 100\n",
        "    n_valid_batches = 10\n",
        "    n_test_batches = 15\n",
        "    X_valid_batches, y_valid_batches = shuffle_batch2(X_valid, y_valid, n_valid_batches)              #np.array_split\n",
        "    X_test_batches, y_test_batches = shuffle_batch2(X_test, y_test, n_valid_batches) \n",
        "  \n",
        "   # X_test_batches = np.array_split(X_test, n_test_batches)\n",
        "   # y_test_batches = np.array_split(y_test, n_test_batches)\n",
        "    #n_valid_batches = 10\n",
        "    #n_test_batches = 10\n",
        "    for epoch in range(n_epochs):\n",
        "        print(\"Epoch\", epoch, end=\"\")\n",
        "        for iteration in range(n_iterations_per_epoch):\n",
        "            print(\".\", end=\"\")\n",
        "            \n",
        "            shuffled_idx = np.random.permutation(122)\n",
        "            #print(shuffled_idx)\n",
        "            hidden2_batches = np.array_split(h2_cache[shuffled_idx], n_batches)\n",
        "            y_batches = np.array_split(y_train[shuffled_idx], n_batches)\n",
        "            for hidden2_batch, y_batch in zip(hidden2_batches, y_batches):\n",
        "               sess.run(training_op, feed_dict={prelogits:hidden2_batch, y:y_batch})\n",
        "\n",
        "            #X_valid_1, y_valid_1 = prepare_batch(flower_paths_and_classes_valid, batch_size=len(flower_paths_and_classes_test)//5)\n",
        "            #sess.run(training_op, feed_dict={X: X_batch_1, y: y_batch_1, training: True})\n",
        "            if iteration % check_interval == 0:\n",
        "                X_valid_batches, y_valid_batches = shuffle_batch2(X_valid, y_valid, n_valid_batches)\n",
        "                loss_val = loss.eval(feed_dict={X: X_valid_batches, y: y_valid_batches})\n",
        "                print(\"  Train loss_val:\", loss_val)\n",
        "                if loss_val < best_loss_val:\n",
        "                    best_loss_val = loss_val\n",
        "                    checks_since_last_progress = 0\n",
        "                    best_model_params = get_model_params()\n",
        "                else:\n",
        "                    checks_since_last_progress += 1\n",
        "        X_valid_batches, y_valid_batches = shuffle_batch2(X_valid, y_valid, n_valid_batches)              #np.array_split\n",
        "        X_test_batches, y_test_batches = shuffle_batch2(X_test, y_test, n_valid_batches) \n",
        "        acc_train = accuracy.eval(feed_dict={X: X_test_batches, y: y_test_batches})\n",
        "        acc_val = accuracy.eval(feed_dict={X: X_valid_batches, y: y_valid_batches})\n",
        "        print(\"Epoch {}, train accuracy: {:.4f}%, valid. accuracy: {:.4f}%, valid. best loss: {:.6f}\".format(\n",
        "                  epoch, acc_train * 100, acc_val * 100, best_loss_val))\n",
        "        if checks_since_last_progress > max_checks_without_progress:\n",
        "            print(\"Early stopping!\")\n",
        "            break\n",
        "\n",
        "    if best_model_params:\n",
        "        restore_model_params(best_model_params)\n",
        "    X_test_batches, y_test_batches = shuffle_batch2(X_test, y_test, n_valid_batches) \n",
        "    acc_test = accuracy.eval(feed_dict={X: X_test_batches, y: y_test_batches})\n",
        "    print(\"Final accuracy on test set:\", acc_test)\n",
        "    save_path = saver.save(sess, \"./my_mnist_model\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/inception_v3_2016_08_28/inception_v3.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-b9d983a543a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mn_valid_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mn_test_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mX_valid_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshuffle_batch2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_valid_batches\u001b[0m\u001b[0;34m)\u001b[0m              \u001b[0;31m#np.array_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mX_test_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshuffle_batch2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_valid_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "jhQ1ciMmzaDj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4717b7ea-d94d-4a88-dfa7-85a7d7ea8434"
      },
      "cell_type": "code",
      "source": [
        "a = np.array(X_valid)\n",
        "a.shape"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2202, 299, 299, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "uXuKWQRjqK3R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#X_valid_batches \n",
        "X_valid\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kb1z-kCBqLA8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}