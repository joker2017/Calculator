{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "17122018.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joker2017/Calculator/blob/master/29122018_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "yl0_vUGm2U_x",
        "colab_type": "code",
        "outputId": "9acd39ab-35b3-4e58-d933-bd7d86df1815",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "  \n",
        "print( os.listdir('/content') )"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-67628865-82d0-41e5-b0ac-533d6e49261c\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-67628865-82d0-41e5-b0ac-533d6e49261c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving imagenet_class_names.txt to imagenet_class_names (3).txt\n",
            "User uploaded file \"imagenet_class_names.txt\" with length 31674 bytes\n",
            "['.config', 'imagenet_class_names (1).txt', 'imagenet_class_names.txt', 'inception_v3_2016_08_28', 'imagenet_class_names (3).txt', 'flower_photos', 'imagenet_class_names (2).txt', 'sample_data']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JeO02mGVr6VA",
        "colab_type": "code",
        "outputId": "c74f1a99-0a00-40e6-ece9-d5006e90e46a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#Загрузите последнюю предварительную модель Inception v3: контрольная точка доступна по адресу https://github.com/tensorflow/models/tree/master/research/slim . \n",
        "#писок имен классов доступен по адресу https://goo.gl/brXRtZ , но вы должны сначала вставить «background» класс.\n",
        "import os\n",
        "import re \n",
        "import sys\n",
        "import tarfile\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from random import sample\n",
        "from six.moves import urllib\n",
        "import matplotlib.image as mpimg\n",
        "import tensorflow.contrib.slim as slim\n",
        "from tensorflow.contrib.slim.nets import inception\n",
        "from skimage.transform import rescale, resize, downscale_local_mean\n",
        "\n",
        "MODEL_NAME = \"inception_v3.ckpt\"\n",
        "CLASS_NAME_REGEX = re.compile(r\"^n\\d+\\s+(.*)\\s*$\", re.M | re.U)\n",
        "DATASET_URL = \"http://download.tensorflow.org/example_images/flower_photos.tgz\"\n",
        "MODEL_URL = \"http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz\"\n",
        "\n",
        "\n",
        "\n",
        "def download_progress(count, block_size, total_size):\n",
        "    percent = count * block_size * 100 // total_size\n",
        "    sys.stdout.write(\"\\rDownloading: {}%\".format(percent))\n",
        "    sys.stdout.flush()\n",
        "    \n",
        "\n",
        "def download_tgz(url):\n",
        "    file_name=os.path.basename(url)\n",
        "    name_parts = os.path.split(file_name)\n",
        "    _, name = name_parts\n",
        "    name_path = name.rsplit( \".\", 2 )[ 0 ]\n",
        "    path = os.path.join(\"/content\", name_path)\n",
        "    tgz_path = os.path.join(path, name)\n",
        "    if os.path.exists(path): #and os.path.isfile(tgz_path):\n",
        "      print(\"file and path exist \", tgz_path, \"Path is:\", os.listdir(path), \"!\")\n",
        "      return path\n",
        "    else:\n",
        "      print(os.path.exists(path), os.path.isfile(tgz_path), path, tgz_path)\n",
        "      os.makedirs(path, exist_ok=True)\n",
        "      urllib.request.urlretrieve(url, tgz_path, reporthook=download_progress)\n",
        "      tmp_tgz = tarfile.open(tgz_path)\n",
        "      tmp_tgz.extractall(path=path)\n",
        "      tmp_tgz.close()\n",
        "      os.remove(tgz_path)\n",
        "      print(\"path created:\", path)\n",
        "      return path\n",
        "    \n",
        "# Загружаем модель\n",
        "MODELS_PATH = download_tgz(MODEL_URL)\n",
        "# Загружаем датасет\n",
        "FLOWERS_PATH = download_tgz(DATASET_URL)\n",
        " \n",
        "\n",
        "def load_class_names():\n",
        "    with open(os.path.join(\"/content\", \"imagenet_class_names.txt\"), \"rb\") as f:\n",
        "        content = f.read().decode(\"utf-8\")\n",
        "        #sys.stdout.flush()\n",
        "        return CLASS_NAME_REGEX.findall(content)\n",
        "      \n",
        "load_class_names()    \n",
        "class_names = [\"background\"] + load_class_names()       \n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "file and path exist  /content/inception_v3_2016_08_28/inception_v3_2016_08_28.tar.gz Path is: ['inception_v3.ckpt'] !\n",
            "file and path exist  /content/flower_photos/flower_photos.tgz Path is: ['flower_photos'] !\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_mcfvHNWTo9Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.contrib.slim.nets import inception\n",
        "import tensorflow.contrib.slim as slim\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=[None, height, width, channels], name=\"X\")\n",
        "training = tf.placeholder_with_default(False, shape=[])\n",
        "with slim.arg_scope(inception.inception_v3_arg_scope()):\n",
        "    logits, end_points = inception.inception_v3(X, num_classes=1001, is_training=training)\n",
        "\n",
        "inception_saver = tf.train.Saver()\n",
        "\n",
        "end_points"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IxKDcAXkUJzF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "flower_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"InceptionV3\")\n",
        "flower_vars"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SjLFWKnGo4FR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#MODEL_URL\n",
        "import os\n",
        "name_parts = os.path.splitext(MODEL_URL)  \n",
        "file, name = name_parts\n",
        "print(file, name)\n",
        "name_parts = os.path.split(MODEL_URL)  \n",
        "file, name = name_parts\n",
        "print(file, name)\n",
        "print(name.rsplit( \".\", 2 )[ 0 ]) \n",
        "print(os.path.dirname(MODEL_URL))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nSZTTwcEvLG9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "print( os.getcwd() )\n",
        "print( os.listdir('/content/flower_photos/flower_photos') )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WNOC6n9out14",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class_names[:5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Jwe1UXvnqIK2",
        "colab_type": "code",
        "outputId": "8a6e202d-606e-4d63-e154-ce8bbfeba4f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "width = 299\n",
        "height = 299\n",
        "channels = 3\n",
        "import tensorflow as tf\n",
        "tf.reset_default_graph()\n",
        "n_epochs = 1000\n",
        "batch_size = 100\n",
        "best_loss_val = np.infty\n",
        "check_interval = 500\n",
        "checks_since_last_progress = 0\n",
        "max_checks_without_progress = 20\n",
        "best_model_params = None \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Каждый подкаталог flower_photos каталога содержит все изображения данного класса. Давайте получим список классов:\n",
        "flowers_root_path = os.path.join(FLOWERS_PATH, \"flower_photos\")\n",
        "flower_classes = sorted([dirname for dirname in os.listdir(flowers_root_path)\n",
        "                  if os.path.isdir(os.path.join(flowers_root_path, dirname))])\n",
        "\n",
        "#Давайте получим список всех путей файлов изображений для каждого класса:\n",
        "\n",
        "from collections import defaultdict\n",
        "image_paths = defaultdict(list)\n",
        "for flower_class in flower_classes:\n",
        "    image_dir = os.path.join(flowers_root_path, flower_class)\n",
        "    for filepath in os.listdir(image_dir):\n",
        "        if filepath.endswith(\".jpg\"):\n",
        "            image_paths[flower_class].append(os.path.join(image_dir, filepath))\n",
        "            \n",
        "            \n",
        "#Давайте сортируем пути изображения, чтобы заставить этот ноутбук вести себя последовательно на нескольких запусках:\n",
        "for paths in image_paths.values():\n",
        "    paths.sort() \n",
        "\n",
        "    \n",
        "#Для получения дополнительных функций манипуляции с изображениями, таких как вращения, проверьте документацию SciPy или эту приятную страницу .\n",
        "def prepare_image(image, target_width = 299, target_height = 299, max_zoom = 0.2):\n",
        "    \"\"\"Zooms and crops the image randomly for data augmentation.\"\"\"\n",
        "\n",
        "    \n",
        "    # Во-первых, давайте найдем самый большой ограничивающий прямоугольник с целевым соотношением размеров, которое вписывается в изображение\n",
        "    height = image.shape[0]\n",
        "    width = image.shape[1]\n",
        "    image_ratio = width / height\n",
        "    target_image_ratio = target_width / target_height\n",
        "    crop_vertically = image_ratio < target_image_ratio\n",
        "    crop_width = width if crop_vertically else int(height * target_image_ratio)\n",
        "    crop_height = int(width / target_image_ratio) if crop_vertically else height\n",
        "        \n",
        "   #Теперь давайте уменьшим этот ограничивающий прямоугольник случайным фактором (разделив размеры на случайное число\n",
        "   # между 1.0 и 1.0 + `max_zoom`\n",
        "    resize_factor = np.random.rand() * max_zoom + 1.0\n",
        "    crop_width = int(crop_width / resize_factor)\n",
        "    crop_height = int(crop_height / resize_factor)\n",
        "    \n",
        "    # Далее мы можем выбрать случайное место на изображении этого прямоугольника.\n",
        "    x0 = np.random.randint(0, width - crop_width)\n",
        "    y0 = np.random.randint(0, height - crop_height)\n",
        "    x1 = x0 + crop_width\n",
        "    y1 = y0 + crop_height\n",
        "    \n",
        "    # Давайте обрезать изображение, используя случайную ограничивающую рамку, которую мы построили.\n",
        "    image = image[y0:y1, x0:x1]\n",
        "\n",
        "    # Давайте также перевернем изображение по горизонтали с вероятностью 50% :\n",
        "    if np.random.rand() < 0.5:\n",
        "        image = np.fliplr(image)\n",
        "\n",
        "    # Теперь давайте изменим размер изображения до целевых размеров.\n",
        "    image = resize(image, (target_width, target_height), mode='reflect')\n",
        "    \n",
        "    # Наконец, давайте убедимся, что цвета представлены как\n",
        "    # 32-бит плавает в диапазоне от 0.0 до 1.0 (на данный момент):\n",
        "    return image.astype(np.float32) / 255\n",
        "  \n",
        "\n",
        "def prepare_batch(flower_paths_and_classes, batch_size):\n",
        "    batch_paths_and_classes = sample(flower_paths_and_classes, batch_size)\n",
        "    images = [mpimg.imread(path)[:, :, :channels] for path, labels in batch_paths_and_classes]\n",
        "    prepared_images = [prepare_image(image) for image in images]\n",
        "    X_batch = 2 * np.stack(prepared_images) - 1 # Inception expects colors ranging from -1 to 1\n",
        "    y_batch = np.array([labels for path, labels in batch_paths_and_classes], dtype=np.int32)\n",
        "    return X_batch, y_batch \n",
        "  \n",
        "  \n",
        "def prepare_data(flower_paths_and_classes):\n",
        "    batch_paths_and_classes = flower_paths_and_classes\n",
        "    images = [mpimg.imread(path)[:, :, :channels] for path, labels in batch_paths_and_classes]\n",
        "    prepared_images = [prepare_image(image) for image in images]\n",
        "    X_data = 2 * np.stack(prepared_images) - 1 # Inception expects colors ranging from -1 to 1\n",
        "    y_data = np.array([labels for path, labels in batch_paths_and_classes], dtype=np.int32)\n",
        "    return X_data, y_data  \n",
        "  \n",
        "  \n",
        "#раннняя остановка\n",
        "def get_model_params():\n",
        "    gvars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
        "    return {gvar.op.name: value for gvar, value in zip(gvars, tf.get_default_session().run(gvars))}\n",
        "\n",
        "def restore_model_params(model_params):\n",
        "    gvar_names = list(model_params.keys())\n",
        "    assign_ops = {gvar_name: tf.get_default_graph().get_operation_by_name(gvar_name + \"/Assign\")\n",
        "                  for gvar_name in gvar_names}\n",
        "    init_values = {gvar_name: assign_op.inputs[1] for gvar_name, assign_op in assign_ops.items()}\n",
        "    feed_dict = {init_values[gvar_name]: model_params[gvar_name] for gvar_name in gvar_names}\n",
        "    tf.get_default_session().run(assign_ops, feed_dict=feed_dict)  \n",
        "  \n",
        "  #снова получить начальный график v3. На этот раз воспользуемся trainingзаполнителем\n",
        "  \n",
        "X = tf.placeholder(tf.float32, shape=[None, height, width, channels], name=\"X\")\n",
        "training = tf.placeholder_with_default(False, shape=[])\n",
        "with slim.arg_scope(inception.inception_v3_arg_scope()):\n",
        "    logits, end_points = inception.inception_v3(X, num_classes=1001, is_training=training)\n",
        "\n",
        "inception_saver = tf.train.Saver()\n",
        "\n",
        "\n",
        "#изучить входные данные выходного уровня:\n",
        "# logits.op.inputs[0].op.inputs[0]  или end_points\n",
        "#end_points[\"PreLogits\"]\n",
        "\n",
        "#отказаться от 2-го и 3-го измерений с помощью tf.squeeze() функции\n",
        "prelogits = tf.squeeze(end_points[\"PreLogits\"], axis=[1, 2])\n",
        "\n",
        "#добавить окончательный полностью подключенный слой поверх этого слоя:  \n",
        "n_outputs = len(flower_classes)\n",
        "\n",
        "with tf.name_scope(\"new_output_layer\"):\n",
        "    flower_logits = tf.layers.dense(prelogits, n_outputs, name=\"flower_logits\")\n",
        "    Y_proba = tf.nn.softmax(flower_logits, name=\"Y_proba\")\n",
        "    \n",
        "\n",
        "y = tf.placeholder(tf.int32, shape=[None])\n",
        "\n",
        "with tf.name_scope(\"train\"):\n",
        "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=flower_logits, labels=y)\n",
        "    loss = tf.reduce_mean(xentropy)\n",
        "    optimizer = tf.train.AdamOptimizer()\n",
        "    flower_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"flower_logits\") #flower_logits\n",
        "    training_op = optimizer.minimize(loss, var_list=flower_vars)\n",
        "    \n",
        "\n",
        "with tf.name_scope(\"eval\"):\n",
        "    correct = tf.nn.in_top_k(flower_logits, y, 1)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
        "\n",
        "with tf.name_scope(\"init_and_save\"):\n",
        "    init = tf.global_variables_initializer()\n",
        "    saver = tf.train.Saver() \n",
        "    \n",
        "\n",
        "    \n",
        "#Разделите свой набор данных на тренировочный набор и тестовый набор. Обучите модель тренировочному набору и оцените его на тестовом наборе.\n",
        "#Во-первых, мы хотим представить классы как int, а не строки:\n",
        "flower_class_ids = {flower_class: index for index, flower_class in enumerate(flower_classes)}\n",
        "\n",
        "\n",
        "#Будет проще перетасовать набор данных, если мы представим его как список пар filepath / class:\n",
        "flower_paths_and_classes = []\n",
        "for flower_class, paths in image_paths.items():\n",
        "    for path in paths:\n",
        "        flower_paths_and_classes.append((path, flower_class_ids[flower_class]))\n",
        "        \n",
        "#Затем давайте перетасовать набор данных и разделим его на обучающий набор и тестовый набор:\n",
        "\n",
        "\n",
        "np.random.shuffle(flower_paths_and_classes)\n",
        "a = flower_paths_and_classes\n",
        "flower_paths_and_classes_train = a[:int(len(a)*0.6)]\n",
        "flower_paths_and_classes_valid = a[int(len(a)*0.6):int(len(a)*0.8)]\n",
        "flower_paths_and_classes_test = a[int(len(a)*0.8):]\n",
        "\n",
        "n_iterations_per_epoch = len(flower_paths_and_classes_train) // batch_size\n",
        "\n",
        "print(\"train:\", len(flower_paths_and_classes_train) ,\"valid:\", len(flower_paths_and_classes_valid), \"test\", len(flower_paths_and_classes_test))\n",
        "\n",
        "#flower_paths_and_classes\n",
        "#print(\"flower_paths_and_classes\", len(flower_paths_and_classes))\n",
        "#print(\"flower_paths_and_classes_valid\", len(flower_paths_and_classes_valid))\n",
        "X_train, y_train = prepare_data(flower_paths_and_classes_train)\n",
        "X_test, y_test = prepare_data(flower_paths_and_classes_test)\n",
        "X_valid, y_valid = prepare_data(flower_paths_and_classes_valid)        \n",
        "n_epochs = 10000\n",
        "#batch_size = 50       \n",
        "\n",
        "best_loss_val = np.infty\n",
        "check_interval = 100\n",
        "checks_since_last_progress = 0\n",
        "max_checks_without_progress = 20\n",
        "best_model_params = None \n",
        "\n",
        "#step\n",
        "\n",
        "def shuffle_batch(X, batch_size):\n",
        "  #rnd_idx = [i for i in range(len(X)) if i>=batch_n*batch_size and i<(batch_n*batch_size+batch_size)]\n",
        "  #print(\"rnd_idx\", rnd_idx)\n",
        "  rnd_idx = []\n",
        "  for i in range(len(X)): rnd_idx.append(i)\n",
        "  for batch_idx in np.array_split(rnd_idx, batch_size):   # np.array_split делит масим на несколько массивов\n",
        "      #print(\"batch_idx\", batch_idx)  \n",
        "      X_batch = X[batch_idx]\n",
        "      yield X_batch      \n",
        "\n",
        "\n",
        " \n",
        "############################## ВЫБИРАЕМ ПЕРЕМЕННЫЕ ##########################\n",
        "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"InceptionV3\") # regular expression\n",
        "restore_saver = tf.train.Saver(reuse_vars) # to restore layers 1-3\n",
        "#n_batches = len(X_train) // batch_size\n",
        "#batch_n = 1101\n",
        "#batch_n1 = 734\n",
        "#batch_size = 5\n",
        "\n",
        "h2_cache_2 = np.empty((0, 2048))\n",
        "h2_cache_valid_2 = np.empty((0, 2048))\n",
        "#h2_cache = []\n",
        "#h2_cache_1 = []\n",
        "#h2_cache_valid_1 = []\n",
        "#h2_cache_valid = []\n",
        "################################################################################\n",
        "with tf.Session() as sess:\n",
        "    init.run()\n",
        "    inception_saver.restore(sess, os.path.join(MODELS_PATH, MODEL_NAME))\n",
        "    \n",
        "    #for batch_n in range(len(X_train)//batch_size): \n",
        "    for X_batch in shuffle_batch(X_train, batch_size):\n",
        "            h2_cache_1 = sess.run(prelogits, feed_dict={X: X_batch, training: False})\n",
        "            h2_cache_2 = np.concatenate((h2_cache_2, h2_cache_1))\n",
        "    #for batch_n in range(len(X_train)//batch_size):\n",
        "    for X_val in shuffle_batch(X_valid, batch_size):\n",
        "            h2_cache_valid_1 = sess.run(prelogits, feed_dict={X: X_val, training: False})\n",
        "            h2_cache_valid_2 = np.concatenate((h2_cache_valid_2, h2_cache_valid_1))\n",
        "\n",
        "            \n",
        "############################## СОЗДАЕМ КЭШ ###################################\n",
        "   \n",
        "  # h2_cache = prelogits.eval(feed_dict={X: X_train})\n",
        "   # h2_cache_valid = sess.run(prelogits, feed_dict={X: X_valid})\n",
        "################################################################################  \n",
        "    \n",
        "    "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train: 2202 valid: 734 test 734\n",
            "INFO:tensorflow:Restoring parameters from /content/inception_v3_2016_08_28/inception_v3.ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "qnMAlf_TL5Pu",
        "outputId": "8363eb7c-47ee-43b7-db36-e770a0c02b3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "h2_cache_valid_2.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(734, 2048)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "G4rJLsqN8gMY",
        "colab_type": "code",
        "outputId": "43d2dc2e-7d7c-4c8b-9c73-a87aaabb7b45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "h2_cache_without_resize"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([], shape=(0, 2048), dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "metadata": {
        "id": "2jZxYFi_l1_e",
        "colab_type": "code",
        "outputId": "e023edd6-c5cf-4f28-fe9d-25200786174c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "cell_type": "code",
      "source": [
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gputil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Building wheels for collected packages: gputil\n",
            "  Running setup.py bdist_wheel for gputil ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Gen RAM Free: 12.9 GB  | Proc size: 140.7 MB\n",
            "GPU RAM Free: 11441MB | Used: 0MB | Util   0% | Total 11441MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "b_b3cs8HDi68",
        "colab_type": "code",
        "outputId": "64df6b4d-745e-4d6a-82dc-2c2f857577b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#h2_cache = np.array(h2_cache)\n",
        "print(h2_cache[:3])\n",
        "h2_cache\n",
        "print(h2_cache.shape)\n",
        "h2_cache[:3]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.15117478 0.0227656  0.266469   ... 0.21273202 0.20294254 0.        ]\n",
            " [0.11786628 0.03779475 0.22354326 ... 0.27310672 0.16936435 0.        ]\n",
            " [0.17138407 0.04705916 0.13491091 ... 0.30150726 0.05455118 0.        ]]\n",
            "(122, 2048)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.15117478, 0.0227656 , 0.266469  , ..., 0.21273202, 0.20294254,\n",
              "        0.        ],\n",
              "       [0.11786628, 0.03779475, 0.22354326, ..., 0.27310672, 0.16936435,\n",
              "        0.        ],\n",
              "       [0.17138407, 0.04705916, 0.13491091, ..., 0.30150726, 0.05455118,\n",
              "        0.        ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "D6JzNEsBoyJe",
        "colab_type": "code",
        "outputId": "c517ba42-475f-4282-a590-6ca608323e81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "np.set_printoptions(threshold=10)\n",
        "h2_cache[2]\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.0046885 , 0.11755972, 0.04769261, ..., 0.15647194, 0.08709227,\n",
              "       0.00934501], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "metadata": {
        "id": "4zavCUbfYBvW",
        "colab_type": "code",
        "outputId": "c84dcc65-196c-4d3e-ea67-5b6f8e93f989",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        }
      },
      "cell_type": "code",
      "source": [
        "# НЕ ТРОГАТЬ\n",
        "print(h2_cache_2[:3])\n",
        "h2_cache_2\n",
        "print(h2_cache_2.shape)\n",
        "\n",
        "h2_cache_2[:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.14001292 0.03021909 0.17526796 ... 0.15923414 0.01626447 0.        ]\n",
            " [0.12993246 0.02821892 0.17930259 ... 0.23912324 0.15550253 0.        ]\n",
            " [0.01801635 0.07236545 0.05991421 ... 0.00897975 0.00563239 0.00939953]]\n",
            "(2202, 2048)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.14001292, 0.03021909, 0.17526796, ..., 0.15923414, 0.01626447,\n",
              "        0.        ],\n",
              "       [0.12993246, 0.02821892, 0.17930259, ..., 0.23912324, 0.15550253,\n",
              "        0.        ],\n",
              "       [0.01801635, 0.07236545, 0.05991421, ..., 0.00897975, 0.00563239,\n",
              "        0.00939953],\n",
              "       [0.06372327, 0.08178102, 0.11291628, ..., 0.0740202 , 0.0315853 ,\n",
              "        0.        ],\n",
              "       [0.19616571, 0.0306903 , 0.20578927, ..., 0.23803714, 0.08466715,\n",
              "        0.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "ihMFdDv7mR6L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#shuffled_idx = np.random.permutation(len(X_train))\n",
        "#np.array_split(h2_cache[6,8,1], 2)\n",
        "#print(shuffled_idx)\n",
        "#h2_cache_1 \n",
        "#print(X_batch)\n",
        "h2_cache.extend(h2_cache)\n",
        "h2_cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JbCOc0UAkclW",
        "colab_type": "code",
        "outputId": "01b7d017-10b3-4292-c694-650bc282b988",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "prelogits.shape\n",
        "#y_train.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([Dimension(None), Dimension(2048)])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "metadata": {
        "id": "DY3VSf6af5G9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1734
        },
        "outputId": "524e57e1-6403-48d3-fcfa-82658e85c7fb"
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph\n",
        "def shuffle_batch_2(X, y, batch_size):\n",
        "    rnd_idx = np.random.permutation(len(X))\n",
        "    n_batches = len(X) // batch_size\n",
        "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
        "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
        "        yield X_batch, y_batch\n",
        "  \n",
        "batch_size = 200\n",
        "n_iterations_per_epoch = len(flower_paths_and_classes_train) // 1000\n",
        "best_loss_val = np.infty\n",
        "check_interval = 500\n",
        "checks_since_last_progress = 0\n",
        "max_checks_without_progress = 100\n",
        "\n",
        "#X_test_batches, y_test_batches = shuffle_batch(X_test, y_test, n_valid_batches)\n",
        "#X_valid_batches, y_valid_batches = shuffle_batch(X_valid, y_valid, n_valid_batches)  \n",
        "\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  \n",
        "  \n",
        "    n_batches = len(X_train) // batch_size\n",
        "\n",
        "    inception_saver.restore(sess, os.path.join(MODELS_PATH, MODEL_NAME))\n",
        "    init.run()\n",
        "    for epoch in range(n_epochs):\n",
        "        shuffled_idx = np.random.permutation(len(X_train))\n",
        "        hidden2_batches = np.array_split(h2_cache_2[shuffled_idx], n_batches)\n",
        "        y_batches = np.array_split(y_train[shuffled_idx], n_batches)\n",
        "        for hidden2_batch, y_batch in zip(hidden2_batches, y_batches):\n",
        "            sess.run(training_op, feed_dict={prelogits:hidden2_batch, y:y_batch, training: True})\n",
        "\n",
        "        accuracy_val = accuracy.eval(feed_dict={prelogits: h2_cache_valid_2, # not shown\n",
        "                                                y: y_valid})   \n",
        "        if epoch % 100 == 0:\n",
        "           print(epoch, \"Validation accuracy:\", accuracy_val)\n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "    ''' init.run()\n",
        "    X_test_batches, y_test_batches = shuffle_batch(X_test, y_test, n_valid_batches)\n",
        "    print(X_test_batches.shape)\n",
        "    X_valid_batches, y_valid_batches = shuffle_batch(X_valid, y_valid, n_valid_batches)\n",
        "    inception_saver.restore(sess, os.path.join(MODELS_PATH, MODEL_NAME))\n",
        "    n_valid_batches = 50\n",
        "    n_test_batches = 50\n",
        "    n_epochs = 1000\n",
        "    for epoch in range(n_epochs):\n",
        "        X_test_batches, y_test_batches = shuffle_batch(X_test, y_test, n_valid_batches)\n",
        "        X_valid_batches, y_valid_batches = shuffle_batch(X_valid, y_valid, n_valid_batches)\n",
        "        huffled_idx = np.random.permutation(len(X_train))\n",
        "        for iteration in range(len(X_train) // batch_size):\n",
        "          hidden2_batches = np.array_split(h2_cache_2[shuffled_idx], n_batches)\n",
        "          y_batches = np.array_split(y_train[shuffled_idx], n_batches)\n",
        "          sess.run(training_op, feed_dict={prelogits:hidden2_batch, y:y_batch, training: True})\n",
        "          if iteration % check_interval == 0:\n",
        "                loss_val = loss.eval(feed_dict={X: X_valid_batches,\n",
        "                                                y: y_valid_batches})\n",
        "                if loss_val < best_loss_val:\n",
        "                    best_loss_val = loss_val\n",
        "                    checks_since_last_progress = 0\n",
        "                    best_model_params = get_model_params()\n",
        "                else:\n",
        "                    checks_since_last_progress += 1               \n",
        "        acc_train = accuracy.eval(feed_dict={X: X_test_batches, y: y_test_batches})\n",
        "        acc_val = accuracy.eval(feed_dict={X: X_valid_batches, y: y_valid_batches})\n",
        "\n",
        "        print(\"Epoch {}, train accuracy: {:.4f}%, valid. accuracy: {:.4f}%, valid. best loss: {:.6f}\".format(\n",
        "                  epoch, acc_train * 100, acc_val * 100, best_loss_val))\n",
        "        if checks_since_last_progress > max_checks_without_progress:\n",
        "            print(\"Early stopping!\")\n",
        "            break\n",
        "\n",
        "    if best_model_params:\n",
        "        restore_model_params(best_model_params)\n",
        "    X_valid_batches, y_valid_batches = shuffle_batch(X_valid, y_valid, n_valid_batches)\n",
        "    acc_test = accuracy.eval(feed_dict={X: X_valid_batches,\n",
        "                                        y: y_valid_batches})\n",
        "    print(\"Final accuracy on test set:\", acc_test)\n",
        "    save_path = saver.save(sess, \"./my_mnist_model\") \n",
        "            \n",
        "    '''\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/inception_v3_2016_08_28/inception_v3.ckpt\n",
            "0 Validation accuracy: 0.24795641\n",
            "100 Validation accuracy: 0.5217984\n",
            "200 Validation accuracy: 0.5449591\n",
            "300 Validation accuracy: 0.5640327\n",
            "400 Validation accuracy: 0.5708447\n",
            "500 Validation accuracy: 0.5735695\n",
            "600 Validation accuracy: 0.56948227\n",
            "700 Validation accuracy: 0.5776567\n",
            "800 Validation accuracy: 0.58446866\n",
            "900 Validation accuracy: 0.5885559\n",
            "1000 Validation accuracy: 0.59536785\n",
            "1100 Validation accuracy: 0.58038145\n",
            "1200 Validation accuracy: 0.5776567\n",
            "1300 Validation accuracy: 0.5817439\n",
            "1400 Validation accuracy: 0.58583105\n",
            "1500 Validation accuracy: 0.57901907\n",
            "1600 Validation accuracy: 0.58446866\n",
            "1700 Validation accuracy: 0.58446866\n",
            "1800 Validation accuracy: 0.5831063\n",
            "1900 Validation accuracy: 0.57493186\n",
            "2000 Validation accuracy: 0.58583105\n",
            "2100 Validation accuracy: 0.5776567\n",
            "2200 Validation accuracy: 0.58446866\n",
            "2300 Validation accuracy: 0.5776567\n",
            "2400 Validation accuracy: 0.5708447\n",
            "2500 Validation accuracy: 0.57901907\n",
            "2600 Validation accuracy: 0.57901907\n",
            "2700 Validation accuracy: 0.5735695\n",
            "2800 Validation accuracy: 0.57901907\n",
            "2900 Validation accuracy: 0.5667575\n",
            "3000 Validation accuracy: 0.5722071\n",
            "3100 Validation accuracy: 0.57493186\n",
            "3200 Validation accuracy: 0.5722071\n",
            "3300 Validation accuracy: 0.5640327\n",
            "3400 Validation accuracy: 0.5640327\n",
            "3500 Validation accuracy: 0.5762943\n",
            "3600 Validation accuracy: 0.57901907\n",
            "3700 Validation accuracy: 0.5667575\n",
            "3800 Validation accuracy: 0.5722071\n",
            "3900 Validation accuracy: 0.5653951\n",
            "4000 Validation accuracy: 0.5722071\n",
            "4100 Validation accuracy: 0.56948227\n",
            "4200 Validation accuracy: 0.5762943\n",
            "4300 Validation accuracy: 0.5708447\n",
            "4400 Validation accuracy: 0.5762943\n",
            "4500 Validation accuracy: 0.5653951\n",
            "4600 Validation accuracy: 0.5735695\n",
            "4700 Validation accuracy: 0.5708447\n",
            "4800 Validation accuracy: 0.5735695\n",
            "4900 Validation accuracy: 0.5626703\n",
            "5000 Validation accuracy: 0.5640327\n",
            "5100 Validation accuracy: 0.5531335\n",
            "5200 Validation accuracy: 0.5667575\n",
            "5300 Validation accuracy: 0.5599455\n",
            "5400 Validation accuracy: 0.5667575\n",
            "5500 Validation accuracy: 0.5653951\n",
            "5600 Validation accuracy: 0.5653951\n",
            "5700 Validation accuracy: 0.5626703\n",
            "5800 Validation accuracy: 0.5572207\n",
            "5900 Validation accuracy: 0.5640327\n",
            "6000 Validation accuracy: 0.5613079\n",
            "6100 Validation accuracy: 0.5667575\n",
            "6200 Validation accuracy: 0.5572207\n",
            "6300 Validation accuracy: 0.5585831\n",
            "6400 Validation accuracy: 0.5599455\n",
            "6500 Validation accuracy: 0.5572207\n",
            "6600 Validation accuracy: 0.55449593\n",
            "6700 Validation accuracy: 0.5599455\n",
            "6800 Validation accuracy: 0.5585831\n",
            "6900 Validation accuracy: 0.54904634\n",
            "7000 Validation accuracy: 0.5585831\n",
            "7100 Validation accuracy: 0.5585831\n",
            "7200 Validation accuracy: 0.5517711\n",
            "7300 Validation accuracy: 0.5585831\n",
            "7400 Validation accuracy: 0.5572207\n",
            "7500 Validation accuracy: 0.5585831\n",
            "7600 Validation accuracy: 0.5504087\n",
            "7700 Validation accuracy: 0.55449593\n",
            "7800 Validation accuracy: 0.5572207\n",
            "7900 Validation accuracy: 0.55449593\n",
            "8000 Validation accuracy: 0.5572207\n",
            "8100 Validation accuracy: 0.5613079\n",
            "8200 Validation accuracy: 0.5517711\n",
            "8300 Validation accuracy: 0.5599455\n",
            "8400 Validation accuracy: 0.5599455\n",
            "8500 Validation accuracy: 0.5585831\n",
            "8600 Validation accuracy: 0.55449593\n",
            "8700 Validation accuracy: 0.5558583\n",
            "8800 Validation accuracy: 0.5572207\n",
            "8900 Validation accuracy: 0.5558583\n",
            "9000 Validation accuracy: 0.5613079\n",
            "9100 Validation accuracy: 0.5613079\n",
            "9200 Validation accuracy: 0.5653951\n",
            "9300 Validation accuracy: 0.5531335\n",
            "9400 Validation accuracy: 0.5599455\n",
            "9500 Validation accuracy: 0.5626703\n",
            "9600 Validation accuracy: 0.5585831\n",
            "9700 Validation accuracy: 0.5613079\n",
            "9800 Validation accuracy: 0.5599455\n",
            "9900 Validation accuracy: 0.5572207\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jhQ1ciMmzaDj",
        "colab_type": "code",
        "outputId": "5af17c4b-dd0f-4990-83c8-bec00ece3f84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "hidden2_batches"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object shuffle_batch at 0x7fd0fb1c4308>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "metadata": {
        "id": "uXuKWQRjqK3R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "len(X_valid)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kb1z-kCBqLA8",
        "colab_type": "code",
        "outputId": "0fa06bd7-0baa-43f2-ec08-7d6a61f38b4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "len(X_valid)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "734"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    }
  ]
}