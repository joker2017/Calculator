{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mobilenetV2 TL and cash 08012018 24.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joker2017/Calculator/blob/master/mobilenetV2_TL_and_cash_08012018_24.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "6BPCjWiVMPh4",
        "colab_type": "code",
        "outputId": "55f90d41-48dd-4bed-fd2c-0f64323880af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/tensorflow/models"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'models'...\n",
            "remote: Enumerating objects: 47, done.\u001b[K\n",
            "remote: Counting objects: 100% (47/47), done.\u001b[K\n",
            "remote: Compressing objects: 100% (35/35), done.\u001b[K\n",
            "remote: Total 24042 (delta 29), reused 28 (delta 12), pack-reused 23995\u001b[K\n",
            "Receiving objects: 100% (24042/24042), 563.17 MiB | 31.36 MiB/s, done.\n",
            "Resolving deltas: 100% (14191/14191), done.\n",
            "Checking out files: 100% (2791/2791), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rzNx01j6MWK1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/models/research/slim')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yl0_vUGm2U_x",
        "colab_type": "code",
        "outputId": "02b42ca8-b3ce-4d24-fd71-3caa88401527",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "  \n",
        "print( os.listdir('/content') )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8fa04a17-9dcc-4a09-8c3b-e10d36d2e60a\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-8fa04a17-9dcc-4a09-8c3b-e10d36d2e60a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving imagenet_class_names.txt to imagenet_class_names.txt\n",
            "User uploaded file \"imagenet_class_names.txt\" with length 31674 bytes\n",
            "['.config', 'models', 'imagenet_class_names.txt', 'my_mnist_model.index', 'my_mnist_model.meta', 'checkpoint', 'my_mnist_model.data-00000-of-00001', 'sample_data']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JeO02mGVr6VA",
        "colab_type": "code",
        "outputId": "defbc070-abfc-4e1c-abce-ec499c14a1a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#Загрузите последнюю предварительную модель Inception v3: контрольная точка доступна по адресу https://github.com/tensorflow/models/tree/master/research/slim . \n",
        "#писок имен классов доступен по адресу https://goo.gl/brXRtZ , но вы должны сначала вставить «background» класс.\n",
        "import os\n",
        "import re \n",
        "import sys\n",
        "import tarfile\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from random import sample\n",
        "from six.moves import urllib\n",
        "import matplotlib.image as mpimg\n",
        "import tensorflow.contrib.slim as slim\n",
        "#from tensorflow.contrib.slim.nets import inception\n",
        "from nets.mobilenet import mobilenet_v2\n",
        "from skimage.transform import rescale, resize, downscale_local_mean\n",
        "\n",
        "MODEL_NAME = \"mobilenet_v2_1.0_224.ckpt\"\n",
        "CLASS_NAME_REGEX = re.compile(r\"^n\\d+\\s+(.*)\\s*$\", re.M | re.U)\n",
        "DATASET_URL = \"http://download.tensorflow.org/example_images/flower_photos.tgz\"\n",
        "MODEL_URL = \"https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_1.0_224.tgz\"\n",
        "\n",
        "\n",
        "\n",
        "def download_progress(count, block_size, total_size):\n",
        "    percent = count * block_size * 100 // total_size\n",
        "    sys.stdout.write(\"\\rDownloading: {}%\".format(percent))\n",
        "    sys.stdout.flush()\n",
        "    \n",
        "\n",
        "def download_tgz(url):\n",
        "    file_name=os.path.basename(url)\n",
        "    name_parts = os.path.split(file_name)\n",
        "    _, name = name_parts\n",
        "    name_path = name.rsplit( \".\", 2 )[ 0 ]\n",
        "    path = os.path.join(\"/content\", name_path)\n",
        "    tgz_path = os.path.join(path, name)\n",
        "    if os.path.exists(path): #and os.path.isfile(tgz_path):\n",
        "      print(\"file and path exist \", tgz_path, \"Path is:\", os.listdir(path), \"!\")\n",
        "      return path\n",
        "    else:\n",
        "      print(os.path.exists(path), os.path.isfile(tgz_path), path, tgz_path)\n",
        "      os.makedirs(path, exist_ok=True)\n",
        "      urllib.request.urlretrieve(url, tgz_path, reporthook=download_progress)\n",
        "      tmp_tgz = tarfile.open(tgz_path)\n",
        "      tmp_tgz.extractall(path=path)\n",
        "      tmp_tgz.close()\n",
        "      os.remove(tgz_path)\n",
        "      print(\"path created:\", path)\n",
        "      return path\n",
        "    \n",
        "# Загружаем модель\n",
        "MODELS_PATH = download_tgz(MODEL_URL)\n",
        "# Загружаем датасет\n",
        "FLOWERS_PATH = download_tgz(DATASET_URL)\n",
        " \n",
        "\n",
        "def load_class_names():\n",
        "    with open(os.path.join(\"/content\", \"imagenet_class_names.txt\"), \"rb\") as f:\n",
        "        content = f.read().decode(\"utf-8\")\n",
        "        #sys.stdout.flush()\n",
        "        return CLASS_NAME_REGEX.findall(content)\n",
        "      \n",
        "load_class_names()    \n",
        "class_names = [\"background\"] + load_class_names()       \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False False /content/mobilenet_v2_1 /content/mobilenet_v2_1/mobilenet_v2_1.0_224.tgz\n",
            "Downloading: 100%path created: /content/mobilenet_v2_1\n",
            "False False /content/flower_photos /content/flower_photos/flower_photos.tgz\n",
            "Downloading: 100%path created: /content/flower_photos\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EaCnXGYLK0gX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"Convolution blocks for mobilenet.\"\"\"\n",
        "import contextlib\n",
        "import functools\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "slim = tf.contrib.slim\n",
        "\n",
        "# tf.contrib.slim.conv2d\n",
        "# tf.contrib.slim.separable_conv2d\n",
        "\n",
        "\n",
        "def _make_divisible(v, divisor, min_value=None):\n",
        "  if min_value is None:\n",
        "    min_value = divisor\n",
        "  new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
        "  # Make sure that round down does not go down by more than 10%.\n",
        "  if new_v < 0.9 * v:\n",
        "    new_v += divisor\n",
        "  return new_v\n",
        "\n",
        "\n",
        "def _split_divisible(num, num_ways, divisible_by=8):\n",
        "  \"\"\"Равномерно разбивает num, num_ways, поэтому каждый фрагмент кратен divisible_by.\"\"\"\n",
        "  assert num % divisible_by == 0\n",
        "  assert num / num_ways >= divisible_by\n",
        "  # Примечание: хотите округлить, мы корректируем каждый сплит в соответствии с итогом.\n",
        "  base = num // num_ways // divisible_by * divisible_by\n",
        "  result = []\n",
        "  accumulated = 0\n",
        "  for i in range(num_ways):\n",
        "    r = base\n",
        "    while accumulated + r < num * (i + 1) / num_ways:\n",
        "      r += divisible_by\n",
        "    result.append(r)\n",
        "    accumulated += r\n",
        "  assert accumulated == num\n",
        "  return result\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def expand_input_by_factor(n, divisible_by=8):\n",
        "  return lambda num_inputs, **_: _make_divisible(num_inputs * n, divisible_by)\n",
        "\n",
        "#################################################################################\n",
        "#[None, 7, 7, 160]\n",
        "#################################################################################\n",
        "#@slim.add_arg_scope\n",
        "def expanded_conv(input_tensor,\n",
        "                  num_outputs,\n",
        "                 # expansion_size=expand_input_by_factor(6),\n",
        "                 # stride=1,\n",
        "                 # rate=1,\n",
        "                 # kernel_size=(3, 3),\n",
        "                 # residual=True,\n",
        "                 # normalizer_fn=None,    # normalizer_fn\n",
        "                 # project_activation_fn=tf.identity,\n",
        "                 # split_projection=1,\n",
        "                 # split_expansion=1,\n",
        "                 # expansion_transform=None,\n",
        "                  depthwise_location,\n",
        "                 # depthwise_channel_multiplier=1,\n",
        "                 # endpoints=None,\n",
        "                 # use_explicit_padding=False,\n",
        "                 # padding='SAME',\n",
        "                 # scope=None\n",
        "    \n",
        "                         ):\n",
        "    #depthwise_location='expansion',\n",
        "    expansion_size=expand_input_by_factor(6)\n",
        " # with tf.variable_scope(scope, default_name='expanded_conv') as s, \\\n",
        " #      tf.name_scope(s.original_name_scope):\n",
        "    prev_depth = input_tensor.get_shape().as_list()[3]  ########################  prev_depth = 160\n",
        " #   if  depthwise_location not in [None, 'input', 'output', 'expansion']:\n",
        " #     raise TypeError('%r is unknown value for depthwise_location' %\n",
        " #                     depthwise_location)\n",
        " #   if use_explicit_padding:                            #######################  use_explicit_padding = False\n",
        " #     if padding != 'SAME':\n",
        " #       raise TypeError('`use_explicit_padding` should only be used with '\n",
        " #                       '\"SAME\" padding.')\n",
        " #     padding = 'VALID'\n",
        "    depthwise_func = functools.partial(\n",
        "        slim.separable_conv2d,\n",
        "        num_outputs=None,\n",
        "        kernel_size=(3, 3),\n",
        "        depth_multiplier=1, #depthwise_channel_multiplier\n",
        "        stride=1,\n",
        "        rate=1,\n",
        "        normalizer_fn=slim.batch_norm,\n",
        "        padding='SAME',\n",
        "        scope='depthwise')\n",
        "    # b1 -> b2 * r -> b2\n",
        "    #   i -> (o * r) (bottleneck) -> o\n",
        "    \n",
        "    input_tensor = tf.identity(input_tensor, 'input')\n",
        "    net = input_tensor\n",
        "\n",
        "    if depthwise_location == 'input':\n",
        "  #    if use_explicit_padding:                          #######################  use_explicit_padding = False\n",
        "  #      net = _fixed_padding(net, kernel_size, rate)\n",
        "      net = depthwise_func(net, activation_fn=None)\n",
        "      print(\"net = depthwise_func(net, activation_fn=None, )\", \"net =\", net  )\n",
        "\n",
        "    if callable(expansion_size):\n",
        "      inner_size = expansion_size(num_inputs=prev_depth)\n",
        "      print(\"callable(expansion_size)\", callable(expansion_size),\"inner_size\", inner_size)\n",
        "      ########################  prev_depth = 160 inner_size = 960\n",
        "    else:\n",
        "      inner_size = expansion_size\n",
        "      print(\"callable(expansion_size)\", callable(expansion_size),\"inner_size\", inner_size)\n",
        "    if inner_size > net.shape[3]:                        #######################   [None, 7, 7, 160]\n",
        "      print(\"if inner_size > net.shape[3]\", \"inner_size=\", inner_size, \"net.shape[3] =\", net.shape[3])\n",
        "      net = split_conv(\n",
        "          net,\n",
        "          inner_size,\n",
        "          num_ways= 1, #split_expansion,\n",
        "          scope='expand',\n",
        "          stride=1,\n",
        "          normalizer_fn=slim.batch_norm)\n",
        "#      print(\"net = slim.conv2d(net(\", net.get_shape().as_list(), \"),inner_size =\", inne_size, \"num_ways= 1, #split_expansion,scope='expand', stride=1,normalizer_fn=slim.batch_norm))\", \"inner_size\", inner_size, \"net=\", net)\n",
        "      net = tf.identity(net, 'expansion_output')\n",
        "#    if endpoints is not None:\n",
        "#      endpoints['expansion_output'] = net\n",
        "\n",
        "    if depthwise_location == 'expansion':\n",
        "#      if use_explicit_padding:                          #######################  use_explicit_padding = False\n",
        "#        net = _fixed_padding(net, kernel_size, rate)\n",
        "      net = depthwise_func(net)\n",
        "      print(\"net = slim.separable_conv2d(net\", net.get_shape().as_list(), \", num_outputs=None,kernel_size=(3, 3), depth_multiplier=1, stride=1, rate=1, normalizer_fn=slim.batch_norm, padding='SAME',scope='depthwise')\")\n",
        "     \n",
        "    net = tf.identity(net, name='depthwise_output')\n",
        "   # if endpoints is not None:\n",
        "   #   endpoints['depthwise_output'] = net\n",
        "   # if expansion_transform:                             #######################  expansion_transform = None\n",
        "   #   net = expansion_transform(expansion_tensor=net, input_tensor=input_tensor)\n",
        "    #Обратите внимание, что в отличие от расширения, у нас всегда есть проекция для получения желаемого выходного размера.\n",
        "    net = split_conv(\n",
        "        net,\n",
        "        num_outputs,\n",
        "        num_ways=1,\n",
        "        stride=1,\n",
        "        scope='project',\n",
        "        normalizer_fn=slim.batch_norm,\n",
        "        activation_fn=tf.identity)\n",
        "#    print(\"net = slim.conv2d(net(\", net.get_shape().as_list(), \"), num_outputs =\", num_outputs=\", \"num_ways= 1, #split_expansion,scope='project', stride=1, normalizer_fn=slim.batch_norm, activation_fn=tf.identity)\")\n",
        "         \n",
        "#    if endpoints is not None:\n",
        "#      endpoints['projection_output'] = net\n",
        "    if depthwise_location == 'output':\n",
        "#      if use_explicit_padding:                          #######################  use_explicit_padding = False\n",
        "#        net = _fixed_padding(net, kernel_size, rate)\n",
        "      net = depthwise_func(net, activation_fn=None)\n",
        "      print(\"net = slim.separable_conv2d(net(\", net.get_shape().as_list(), \"), num_outputs=None,kernel_size=(3, 3), depth_multiplier=1, depthwise_channel_multiplier, stride=1, rate=1, normalizer_fn=slim.batch_norm, padding='SAME',scope='depthwise', activation_fn=None)\")\n",
        " #   if callable(residual):  # custom residual/ таможенный остаток\n",
        "    #net = residual(input_tensor=input_tensor, output_tensor=net)\n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "    if (net.get_shape().as_list()[3] == input_tensor.get_shape().as_list()[3]):\n",
        "        net += input_tensor\n",
        "        print(\"net += input_tensor\")\n",
        "      \n",
        "      \n",
        "    if (net.get_shape().as_list()[3] == input_tensor.get_shape().as_list()[3]):\n",
        "#          print(\"if (net.get_shape().as_list()[3] == input_tensor.get_shape().as_list()[3]):\", net.get_shape().as_list()[3], \"==\", input_tensor.get_shape().as_list()[3])\n",
        "          net += input_tensor\n",
        "          print(\"net += input_tensor\")\n",
        "    return tf.identity(net, name='output')\n",
        "\n",
        "\n",
        "def split_conv(input_tensor,\n",
        "               num_outputs,\n",
        "               num_ways,\n",
        "               scope,\n",
        "               divisible_by=8,\n",
        "               **kwargs):\n",
        "\n",
        "  b = input_tensor.get_shape().as_list()[3]\n",
        "\n",
        "  if num_ways == 1 or min(b // num_ways,\n",
        "                          num_outputs // num_ways) < divisible_by:\n",
        "    # Don't do any splitting if we end up with less than 8 filters\n",
        "    # on either side.\n",
        "    print(\"net = slim.conv2d(net\", input_tensor.get_shape().as_list(), \", num_outputs=\", num_outputs, \", [1, 1], scope=\", scope, \"**kwargs)\")\n",
        "    return slim.conv2d(input_tensor, num_outputs, [1, 1], scope=scope, **kwargs)\n",
        "    \n",
        "\n",
        "  outs = []\n",
        "  print(\"outs = []\")\n",
        "  input_splits = _split_divisible(b, num_ways, divisible_by=divisible_by)\n",
        "  print(\"input_splits =\", input_splits)\n",
        "  output_splits = _split_divisible(\n",
        "      num_outputs, num_ways, divisible_by=divisible_by)\n",
        "  print(\"output_splits =\", output_splits)\n",
        "  inputs = tf.split(input_tensor, input_splits, axis=3, name='split_' + scope)\n",
        "  print(\"inputs = tf.split(input_tensor, input_splits, axis=3, name='split_' + scope)\")\n",
        "  base = scope\n",
        "  for i, (input_tensor, out_size) in enumerate(zip(inputs, output_splits)):\n",
        "    scope = base + '_part_%d' % (i,)\n",
        "    n = slim.conv2d(input_tensor, out_size, [1, 1], scope=scope, **kwargs)\n",
        "    n = tf.identity(n, scope + '_output')\n",
        "    outs.append(n)\n",
        "  print(\"net = tf.concat(outs=\", outs, \", 3, name=\", scope, \"+ '_concat')\")  \n",
        "  return tf.concat(outs, 3, name=scope + '_concat')\n",
        "\n",
        "\n",
        "\n",
        "#net = slim.conv2d(net [None, 7, 7, 160] , num_outputs= 960 , [1, 1], scope= 'expand', stride=1, normalizer_fn=slim.batch_norm)\n",
        "\n",
        "#net = slim.separable_conv2d(net [None, 7, 7, 960] , num_outputs=None, kernel_size=(3, 3), depth_multiplier=1, stride=1, rate=1, normalizer_fn=slim.batch_norm, padding='SAME',scope='depthwise')\n",
        "\n",
        "#net = slim.conv2d(net [None, 7, 7, 960] , num_outputs= 320 , [1, 1], scope= 'project', normalizer_fn=slim.batch_norm, activation_fn=tf.identity)\n",
        "#net = slim.conv2d(net,  stride=1, kernel_size=[1, 1], num_outputs=1280)\n",
        "#net = tf.nn.avg_pool( net [None, 7, 7, 160] ksize= [1, 7, 7, 1] strides=[1, 1, 1, 1], padding='VALID')\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i2T09-MCLOyx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def global_pool(input_tensor):\n",
        "\n",
        "  pool_op=tf.nn.avg_pool\n",
        "  shape = input_tensor.get_shape().as_list()\n",
        "  if shape[1] is None or shape[2] is None:\n",
        "    print(\"if shape[1] is None or shape[2] is None:\", \"shape[1]=\". shape[1], \"shape[2]=\", shape[2])\n",
        "    kernel_size = tf.convert_to_tensor(\n",
        "        [1, tf.shape(input_tensor)[1],\n",
        "         tf.shape(input_tensor)[2], 1])\n",
        "  else:\n",
        "    \n",
        "    kernel_size = [1, shape[1], shape[2], 1]\n",
        "    \n",
        "  output = pool_op(\n",
        "      input_tensor, ksize=kernel_size, strides=[1, 1, 1, 1], padding='VALID')\n",
        "  print(\"net = tf.nn.avg_pool( net\", input_tensor.get_shape().as_list(), \"ksize=\", kernel_size, \"strides=[1, 1, 1, 1], padding='VALID')\")\n",
        "  # Recover output shape, for unknown shape.\n",
        "  output.set_shape([None, 1, 1, None])\n",
        "  #print(\"net = output.set_shape([None, 1, 1, None])\", output.get_shape().as_list())\n",
        "  return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Jwe1UXvnqIK2",
        "colab_type": "code",
        "outputId": "99019370-dd2a-4c83-fe05-abd053c03a7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "cell_type": "code",
      "source": [
        "width = 224 #299\n",
        "height = 224 #299\n",
        "channels = 3\n",
        "import tensorflow as tf\n",
        "tf.reset_default_graph()\n",
        "n_epochs = 1000\n",
        "batch_size = 100\n",
        "best_loss_val = np.infty\n",
        "check_interval = 500\n",
        "checks_since_last_progress = 0\n",
        "max_checks_without_progress = 20\n",
        "best_model_params = None \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Каждый подкаталог flower_photos каталога содержит все изображения данного класса. Давайте получим список классов:\n",
        "flowers_root_path = os.path.join(FLOWERS_PATH, \"flower_photos\")\n",
        "flower_classes = sorted([dirname for dirname in os.listdir(flowers_root_path)\n",
        "                  if os.path.isdir(os.path.join(flowers_root_path, dirname))])\n",
        "\n",
        "#Давайте получим список всех путей файлов изображений для каждого класса:\n",
        "\n",
        "from collections import defaultdict\n",
        "image_paths = defaultdict(list)\n",
        "for flower_class in flower_classes:\n",
        "    image_dir = os.path.join(flowers_root_path, flower_class)\n",
        "    for filepath in os.listdir(image_dir):\n",
        "        if filepath.endswith(\".jpg\"):\n",
        "            image_paths[flower_class].append(os.path.join(image_dir, filepath))\n",
        "            \n",
        "            \n",
        "#Давайте сортируем пути изображения, чтобы заставить этот ноутбук вести себя последовательно на нескольких запусках:\n",
        "for paths in image_paths.values():\n",
        "    paths.sort() \n",
        "\n",
        "    \n",
        "#Для получения дополнительных функций манипуляции с изображениями, таких как вращения, проверьте документацию SciPy или эту приятную страницу .\n",
        "from scipy.misc import imresize\n",
        "def prepare_image(image, target_width = width, target_height = height, max_zoom = 0.2):\n",
        "    \"\"\"Zooms and crops the image randomly for data augmentation.\"\"\"\n",
        "\n",
        "    # First, let's find the largest bounding box with the target size ratio that fits within the image\n",
        "    height = image.shape[0]\n",
        "    width = image.shape[1]\n",
        "    image_ratio = width / height\n",
        "    target_image_ratio = target_width / target_height\n",
        "    crop_vertically = image_ratio < target_image_ratio\n",
        "    crop_width = width if crop_vertically else int(height * target_image_ratio)\n",
        "    crop_height = int(width / target_image_ratio) if crop_vertically else height\n",
        "        \n",
        "    # Now let's shrink this bounding box by a random factor (dividing the dimensions by a random number\n",
        "    # between 1.0 and 1.0 + `max_zoom`.\n",
        "    resize_factor = np.random.rand() * max_zoom + 1.0\n",
        "    crop_width = int(crop_width / resize_factor)\n",
        "    crop_height = int(crop_height / resize_factor)\n",
        "    \n",
        "    # Next, we can select a random location on the image for this bounding box.\n",
        "    x0 = np.random.randint(0, width - crop_width)\n",
        "    y0 = np.random.randint(0, height - crop_height)\n",
        "    x1 = x0 + crop_width\n",
        "    y1 = y0 + crop_height\n",
        "    \n",
        "    # Let's crop the image using the random bounding box we built.\n",
        "    image = image[y0:y1, x0:x1]\n",
        "\n",
        "    # Let's also flip the image horizontally with 50% probability:\n",
        "    if np.random.rand() < 0.5:\n",
        "        image = np.fliplr(image)\n",
        "\n",
        "    # Now, let's resize the image to the target dimensions.\n",
        "    image = imresize(image, (target_width, target_height))\n",
        "    \n",
        "    # Finally, let's ensure that the colors are represented as\n",
        "    # 32-bit floats ranging from 0.0 to 1.0 (for now):\n",
        "    return image.astype(np.float32) / 255\n",
        "\n",
        "\n",
        "\n",
        "def prepare_batch(flower_paths_and_classes, batch_size):\n",
        "    batch_paths_and_classes = sample(flower_paths_and_classes, batch_size)\n",
        "    images = [mpimg.imread(path)[:, :, :channels] for path, labels in batch_paths_and_classes]\n",
        "    prepared_images = [prepare_image(image) for image in images]\n",
        "    X_batch = 2 * np.stack(prepared_images) - 1 # Inception expects colors ranging from -1 to 1\n",
        "    y_batch = np.array([labels for path, labels in batch_paths_and_classes], dtype=np.int32)\n",
        "    return X_batch, y_batch \n",
        "  \n",
        "  \n",
        "def prepare_data(flower_paths_and_classes):\n",
        "    batch_paths_and_classes = flower_paths_and_classes\n",
        "    images = [mpimg.imread(path)[:, :, :channels] for path, labels in batch_paths_and_classes]\n",
        "    prepared_images = [prepare_image(image) for image in images]\n",
        "    X_data = 2 * np.stack(prepared_images) - 1 # Inception expects colors ranging from -1 to 1\n",
        "    y_data = np.array([labels for path, labels in batch_paths_and_classes], dtype=np.int32)\n",
        "    return X_data, y_data  \n",
        "  \n",
        "  \n",
        "#раннняя остановка\n",
        "def get_model_params():\n",
        "    gvars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
        "    return {gvar.op.name: value for gvar, value in zip(gvars, tf.get_default_session().run(gvars))}\n",
        "\n",
        "def restore_model_params(model_params):\n",
        "    gvar_names = list(model_params.keys())\n",
        "    assign_ops = {gvar_name: tf.get_default_graph().get_operation_by_name(gvar_name + \"/Assign\")\n",
        "                  for gvar_name in gvar_names}\n",
        "    init_values = {gvar_name: assign_op.inputs[1] for gvar_name, assign_op in assign_ops.items()}\n",
        "    feed_dict = {init_values[gvar_name]: model_params[gvar_name] for gvar_name in gvar_names}\n",
        "    tf.get_default_session().run(assign_ops, feed_dict=feed_dict)  \n",
        "  \n",
        "  #снова получить начальный график v3. На этот раз воспользуемся trainingзаполнителем\n",
        "\n",
        "flower_class_ids = {flower_class: index for index, flower_class in enumerate(flower_classes)}\n",
        "\n",
        "\n",
        "#Будет проще перетасовать набор данных, если мы представим его как список пар filepath / class:\n",
        "flower_paths_and_classes = []\n",
        "for flower_class, paths in image_paths.items():\n",
        "    for path in paths:\n",
        "        flower_paths_and_classes.append((path, flower_class_ids[flower_class]))\n",
        "        \n",
        "#Затем давайте перетасовать набор данных и разделим его на обучающий набор и тестовый набор:\n",
        "\n",
        "\n",
        "np.random.shuffle(flower_paths_and_classes)\n",
        "a = flower_paths_and_classes\n",
        "flower_paths_and_classes_train = a[:int(len(a)*0.6)]\n",
        "flower_paths_and_classes_valid = a[int(len(a)*0.6):int(len(a)*0.8)]\n",
        "flower_paths_and_classes_test = a[int(len(a)*0.8):]\n",
        "\n",
        "n_iterations_per_epoch = len(flower_paths_and_classes_train) // batch_size\n",
        "\n",
        "print(\"train:\", len(flower_paths_and_classes_train) ,\"valid:\", len(flower_paths_and_classes_valid), \"test\", len(flower_paths_and_classes_test))\n",
        "\n",
        "#flower_paths_and_classes\n",
        "#print(\"flower_paths_and_classes\", len(flower_paths_and_classes))\n",
        "#print(\"flower_paths_and_classes_valid\", len(flower_paths_and_classes_valid))\n",
        "X_train, y_train = prepare_data(flower_paths_and_classes_train)\n",
        "X_test, y_test = prepare_data(flower_paths_and_classes_test)\n",
        "X_valid, y_valid = prepare_data(flower_paths_and_classes_valid)        \n",
        "#n_epochs = 10000\n",
        "#batch_size = 50       \n",
        "\n",
        "#best_loss_val = np.infty\n",
        "#check_interval = 100\n",
        "#checks_since_last_progress = 0\n",
        "#max_checks_without_progress = 20\n",
        "#best_model_params = None \n",
        "\n",
        "#step\n",
        "\n",
        "def shuffle_batch(X, batch_size):\n",
        "  #rnd_idx = [i for i in range(len(X)) if i>=batch_n*batch_size and i<(batch_n*batch_size+batch_size)]\n",
        "  #print(\"rnd_idx\", rnd_idx)\n",
        "  \n",
        "  print(\"X.shape\", X.shape)\n",
        "  rnd_idx = []\n",
        "  for i in range(len(X)): \n",
        "    rnd_idx.append(i)\n",
        "  print(\"rnd_idx.append\", rnd_idx)\n",
        "  for batch_idx in np.array_split(rnd_idx, batch_size):   # np.array_split делит масим на несколько массивов\n",
        "      #print(\"batch_idx\", batch_idx)  \n",
        "      X_batch = X[batch_idx]\n",
        "      \n",
        "      yield X_batch      \n",
        "\n",
        "\n",
        "def get_var_list(target_tensor=None):\n",
        "  \n",
        "    if target_tensor==None:\n",
        "        target_tensor = \"MobilenetV2/expanded_conv_15/output:0\"\n",
        "    target = target_tensor.split(\"/\")[1]\n",
        "    all_list = []\n",
        "    all_var = []\n",
        "\n",
        "    for var in tf.global_variables():\n",
        "        if var != []:\n",
        "            all_list.append(var.name)\n",
        "            all_var.append(var)\n",
        "    try:\n",
        "        all_list = list(map(lambda x:x.split(\"/\")[1],all_list))\n",
        "\n",
        "        ind = all_list[::-1].index(target)\n",
        "        ind = len(all_list) -  ind - 1\n",
        "        print(ind)\n",
        "        del all_list\n",
        "        return all_var[:ind+1]\n",
        "    except:\n",
        "        print(\"target_tensor is not exist!\")\n",
        "\n",
        "\n",
        "        \n",
        "def cash(height, width, channels, flower_classes):\n",
        "  X = tf.placeholder(tf.float32, shape=[None, height, width, channels], name=\"X\")\n",
        "  training = tf.placeholder_with_default(False, shape=[])\n",
        "\n",
        "  with slim.arg_scope(mobilenet_v2.training_scope(is_training=False)): #is_training=False\n",
        "    logits, end_points = mobilenet_v2.mobilenet(X)\n",
        "\n",
        "  #with slim.arg_scope(inception.inception_v3_arg_scope()):\n",
        "      #logits, end_points = inception.inception_v3(X, num_classes=1001, is_training=training)\n",
        "\n",
        "  inception_saver = tf.train.Saver() \n",
        "  n_outputs = len(flower_classes)\n",
        "  mobilenet_tensor = end_points[\"layer_17/output\"]\n",
        "  y = tf.placeholder(tf.int32, shape=[None])        \n",
        "  #target_tensor = \"MobilenetV2/expanded_conv_15/output:0\"\n",
        "  #var_list = get_var_list(target_tensor)\n",
        "  #saver = tf.train.Saver(var_list=var_list)      \n",
        "  init = tf.global_variables_initializer()      \n",
        "      \n",
        "      \n",
        "      \n",
        "\n",
        "  ############################## ВЫБИРАЕМ ПЕРЕМЕННЫЕ ##########################\n",
        "\n",
        "  reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"MobilenetV2\") # regular expression\n",
        "  restore_saver = tf.train.Saver(reuse_vars) # to restore layers 1-3\n",
        "\n",
        "  h2_cache_2 = np.empty([0, 7, 7, 160]) \n",
        "  print(h2_cache_2)\n",
        "  h2_cache_valid_2 = np.empty([0, 7, 7, 160])                                                              \n",
        "  init = tf.global_variables_initializer()\n",
        "  ################################################################################\n",
        "  with tf.Session() as sess:\n",
        "    \n",
        "      #sess.run(tf.variables_initializer(var_list=train_var))\n",
        "      inception_saver.restore(sess, os.path.join(MODELS_PATH, MODEL_NAME))\n",
        "      init.run()\n",
        "    \n",
        "     \n",
        "      for X_batch in shuffle_batch(X_train, batch_size):\n",
        "           \n",
        "              h2_cache_1 = sess.run(mobilenet_tensor, feed_dict={X: X_batch, training: False})\n",
        "          \n",
        "              h2_cache_2 = np.concatenate((h2_cache_2, h2_cache_1))\n",
        "           \n",
        "      for X_val in shuffle_batch(X_valid, batch_size):\n",
        "              h2_cache_valid_1 = sess.run(mobilenet_tensor, feed_dict={X: X_val, training: False})\n",
        "            \n",
        "              h2_cache_valid_2 = np.concatenate((h2_cache_valid_2, h2_cache_valid_1))\n",
        "\n",
        "  return h2_cache_2, h2_cache_valid_2       \n",
        " \n",
        "    \n",
        "h2_cache_2, h2_cache_valid_2 = cash(height, width, channels, flower_classes)\n",
        "print(h2_cache_2.shape, h2_cache_valid_2.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train: 2202 valid: 734 test 734\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:73: DeprecationWarning: `imresize` is deprecated!\n",
            "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``skimage.transform.resize`` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[]\n",
            "INFO:tensorflow:Restoring parameters from /content/mobilenet_v2_1/mobilenet_v2_1.0_224.ckpt\n",
            "X.shape (2202, 224, 224, 3)\n",
            "rnd_idx.append [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534, 1535, 1536, 1537, 1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545, 1546, 1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556, 1557, 1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567, 1568, 1569, 1570, 1571, 1572, 1573, 1574, 1575, 1576, 1577, 1578, 1579, 1580, 1581, 1582, 1583, 1584, 1585, 1586, 1587, 1588, 1589, 1590, 1591, 1592, 1593, 1594, 1595, 1596, 1597, 1598, 1599, 1600, 1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 1610, 1611, 1612, 1613, 1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1624, 1625, 1626, 1627, 1628, 1629, 1630, 1631, 1632, 1633, 1634, 1635, 1636, 1637, 1638, 1639, 1640, 1641, 1642, 1643, 1644, 1645, 1646, 1647, 1648, 1649, 1650, 1651, 1652, 1653, 1654, 1655, 1656, 1657, 1658, 1659, 1660, 1661, 1662, 1663, 1664, 1665, 1666, 1667, 1668, 1669, 1670, 1671, 1672, 1673, 1674, 1675, 1676, 1677, 1678, 1679, 1680, 1681, 1682, 1683, 1684, 1685, 1686, 1687, 1688, 1689, 1690, 1691, 1692, 1693, 1694, 1695, 1696, 1697, 1698, 1699, 1700, 1701, 1702, 1703, 1704, 1705, 1706, 1707, 1708, 1709, 1710, 1711, 1712, 1713, 1714, 1715, 1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723, 1724, 1725, 1726, 1727, 1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735, 1736, 1737, 1738, 1739, 1740, 1741, 1742, 1743, 1744, 1745, 1746, 1747, 1748, 1749, 1750, 1751, 1752, 1753, 1754, 1755, 1756, 1757, 1758, 1759, 1760, 1761, 1762, 1763, 1764, 1765, 1766, 1767, 1768, 1769, 1770, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789, 1790, 1791, 1792, 1793, 1794, 1795, 1796, 1797, 1798, 1799, 1800, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810, 1811, 1812, 1813, 1814, 1815, 1816, 1817, 1818, 1819, 1820, 1821, 1822, 1823, 1824, 1825, 1826, 1827, 1828, 1829, 1830, 1831, 1832, 1833, 1834, 1835, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1843, 1844, 1845, 1846, 1847, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1855, 1856, 1857, 1858, 1859, 1860, 1861, 1862, 1863, 1864, 1865, 1866, 1867, 1868, 1869, 1870, 1871, 1872, 1873, 1874, 1875, 1876, 1877, 1878, 1879, 1880, 1881, 1882, 1883, 1884, 1885, 1886, 1887, 1888, 1889, 1890, 1891, 1892, 1893, 1894, 1895, 1896, 1897, 1898, 1899, 1900, 1901, 1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909, 1910, 1911, 1912, 1913, 1914, 1915, 1916, 1917, 1918, 1919, 1920, 1921, 1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 1930, 1931, 1932, 1933, 1934, 1935, 1936, 1937, 1938, 1939, 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2032, 2033, 2034, 2035, 2036, 2037, 2038, 2039, 2040, 2041, 2042, 2043, 2044, 2045, 2046, 2047, 2048, 2049, 2050, 2051, 2052, 2053, 2054, 2055, 2056, 2057, 2058, 2059, 2060, 2061, 2062, 2063, 2064, 2065, 2066, 2067, 2068, 2069, 2070, 2071, 2072, 2073, 2074, 2075, 2076, 2077, 2078, 2079, 2080, 2081, 2082, 2083, 2084, 2085, 2086, 2087, 2088, 2089, 2090, 2091, 2092, 2093, 2094, 2095, 2096, 2097, 2098, 2099, 2100, 2101, 2102, 2103, 2104, 2105, 2106, 2107, 2108, 2109, 2110, 2111, 2112, 2113, 2114, 2115, 2116, 2117, 2118, 2119, 2120, 2121, 2122, 2123, 2124, 2125, 2126, 2127, 2128, 2129, 2130, 2131, 2132, 2133, 2134, 2135, 2136, 2137, 2138, 2139, 2140, 2141, 2142, 2143, 2144, 2145, 2146, 2147, 2148, 2149, 2150, 2151, 2152, 2153, 2154, 2155, 2156, 2157, 2158, 2159, 2160, 2161, 2162, 2163, 2164, 2165, 2166, 2167, 2168, 2169, 2170, 2171, 2172, 2173, 2174, 2175, 2176, 2177, 2178, 2179, 2180, 2181, 2182, 2183, 2184, 2185, 2186, 2187, 2188, 2189, 2190, 2191, 2192, 2193, 2194, 2195, 2196, 2197, 2198, 2199, 2200, 2201]\n",
            "X.shape (734, 224, 224, 3)\n",
            "rnd_idx.append [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733]\n",
            "(2202, 7, 7, 160) (734, 7, 7, 160)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OCjQBLJySv3r",
        "colab_type": "code",
        "outputId": "43d5b03e-3dae-464b-9a36-45cd67a08620",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "print(h2_cache_2.shape, h2_cache_valid_2.shape)\n",
        "#index = initial_list.index(h2_cache_2)\n",
        "h2_cache_2 = np.delete(h2_cache_2, [0, 1], 0)\n",
        "#del other_list[index]\n",
        "print(h2_cache_2.shape, h2_cache_valid_2.shape)\n",
        "print(arr.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2202, 7, 7, 160) (734, 7, 7, 160)\n",
            "(2200, 7, 7, 160) (734, 7, 7, 160)\n",
            "(2200, 7, 7, 160)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "U22zBGoLBC21",
        "colab_type": "code",
        "outputId": "94d2162e-d5fe-4c87-b671-f42caf90ad58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "arr = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\n",
        "print(arr.shape)\n",
        "arr = np.delete(arr, 2, 0)\n",
        "print(arr.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3, 4)\n",
            "(2, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "G0G2UteDDlcB",
        "colab_type": "code",
        "outputId": "ec23250a-f716-46d3-ad97-866e2abc8326",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "arr"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1,  2,  3,  4],\n",
              "       [ 5,  6,  7,  8],\n",
              "       [ 9, 10, 11, 12]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "metadata": {
        "id": "_tVEO2qanJx_",
        "colab_type": "code",
        "outputId": "4280f6cb-a089-4e28-89e7-f72e5ccb09ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "cell_type": "code",
      "source": [
        "b = h2_cache_valid_2.shape\n",
        "a = h2_cache_valid_1.shape\n",
        "print(\"a=\", a, \"b=\", b)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-97af70349aec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh2_cache_valid_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh2_cache_valid_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"a=\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"b=\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'h2_cache_valid_1' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "fs-Xq-Acg2JZ",
        "colab_type": "code",
        "outputId": "03a9f962-0be6-4b18-aac9-edad7f238ce5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "h2_cache_3 = np.empty([0, 7, 7, 160])\n",
        "h2_cache_3"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([], shape=(0, 7, 7, 160), dtype=float64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "DY3VSf6af5G9",
        "colab_type": "code",
        "outputId": "380ccf10-9c66-4b03-d3b9-4c08b0fd1e9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "batch_size = 100\n",
        "filters=256\n",
        "kernel_size=3\n",
        "#inception_saver = tf.train.Saver() \n",
        "n_outputs = len(flower_classes)\n",
        "\n",
        "#y_label = tf.placeholder(tf.int32, (None,10))\n",
        "ys = tf.placeholder(tf.int32, shape=[None])  \n",
        "\n",
        "#with slim.arg_scope(mobilenet_v2.training_scope(is_training=False)): #is_training=False\n",
        "    #logits, end_points = mobilenet_v2.mobilenet(X)\n",
        "with tf.variable_scope(\"finetune_layers\"):  #reuse=tf.AUTO_REUSE\n",
        "    X = tf.placeholder(tf.float32, shape=[None, 7, 7, 160], name=\"X\")\n",
        "    #mobilenet_tensor = end_points[\"layer_17/output\"]\n",
        "    x = slim.conv2d(X, num_outputs=960, kernel_size=[1, 1], stride=1)\n",
        "    x = slim.separable_conv2d(x, num_outputs=None, kernel_size=(3, 3), depth_multiplier=1, stride=1, rate=1,  padding='SAME') #normalizer_fn=slim.batch_norm,\n",
        "    x = slim.conv2d(x, num_outputs=320, kernel_size=[1, 1], activation_fn=tf.identity)\n",
        "    x = slim.conv2d(x, stride=1, kernel_size=[1, 1], num_outputs=1280)\n",
        "    x = tf.nn.avg_pool(x, ksize=[1, 7, 7, 1], strides=[1, 1, 1, 1], padding='VALID')\n",
        "    dropout = tf.nn.dropout(x, 0.5)\n",
        "    #X_2 = tf.placeholder() #shape=[None, 7, 7, 160],\n",
        "    #x1 = tf.layers.Conv2D(mobilenet_tensor, filters, kernel_size)\n",
        "    #x2 = tf.nn.relu(x1)\n",
        "    #x3 = tf.layers.Conv2D(x1, filters, kernel_size)\n",
        "    #x4 = tf.layers.Conv2D(x3, 10, 3)\n",
        "    #print(x4.get_shape())\n",
        "    #predictions = tf.reshape(x4, (-1, 10))\n",
        "    logits = slim.conv2d(dropout, 10, [1, 1], activation_fn=None, normalizer_fn=None, biases_initializer=tf.zeros_initializer())\n",
        "    logits = tf.squeeze(logits, [1, 2])\n",
        "    #logits = tf.reshape(logits, )\n",
        "    predictions = slim.softmax(logits)\n",
        "   # predictions1 = tf.reshape(logits, (-1, 10))\n",
        "  #  print(\"predictions\", predictions)\n",
        "    #x = tf.layers.Conv2D(filters=256,kernel_size=3,name=\"conv2d_1\")(X)\n",
        "    # 观察新层权重是否更新 tf.summary.histogram(\"conv2d_1\",x)\n",
        "   # x = tf.nn.relu(x,name=\"relu_1\")\n",
        "   # x = tf.layers.Conv2D(filters=256,kernel_size=3,name=\"conv2d_2\")(x)\n",
        "   # x = tf.layers.Conv2D(10,3,name=\"conv2d_3\")(x)\n",
        "    #predictions = tf.reshape(x, (-1,10))\n",
        "\n",
        "with tf.name_scope(\"train1\"):\n",
        "   # print(\"logits shape\", logits.get_shape().as_list(), \"label shape\", y.shape )\n",
        "   # xentropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=ys)\n",
        "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=ys)\n",
        "    #xentropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=flower_logits, labels=y)\n",
        "    loss = tf.reduce_mean(xentropy)\n",
        "    #batch = tf.Variable(0, trainable=False)\n",
        "    \n",
        "\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=0.0001, name='Adam6')  # \n",
        "    #train_var = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"finetune_layers\")\n",
        "    #training_op = optimizer.minimize(loss, var_list=train_var)\n",
        "    training_op = optimizer.minimize(loss)\n",
        "\n",
        "with tf.name_scope(\"eval1\"):\n",
        "    correct = tf.nn.in_top_k(logits, ys, 1)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
        "\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver() \n",
        "\n",
        "\n",
        "#target_tensor = \"MobilenetV2/expanded_conv_17/output:0\"\n",
        "#var_list = get_var_list(target_tensor)\n",
        "#saver = tf.train.Saver(var_list=var_list)\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "import time\n",
        "n_epochs = 100000\n",
        "\n",
        "from IPython.display import clear_output\n",
        "max_checks_without_progress = 1000\n",
        "checks_without_progress = 0\n",
        "best_loss = np.infty\n",
        "init = tf.global_variables_initializer()\n",
        "with tf.Session() as sess:\n",
        " \n",
        "\n",
        "    #sess.run(tf.variables_initializer(var_list=train_var))\n",
        "    init.run()\n",
        "    #inception_saver.restore(sess, os.path.join(MODELS_PATH, MODEL_NAME))\n",
        "    #for var in train_var:\n",
        "        #var.initializer.run()\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        rnd_idx = np.random.permutation(2200)\n",
        "        for rnd_indices in np.array_split(rnd_idx, 2200 // batch_size):\n",
        "            h5_batch, y_batch = h2_cache_2[rnd_indices], y_train[rnd_indices]\n",
        "            sess.run(training_op, feed_dict={X: h5_batch, ys: y_batch})\n",
        "            #print(\"h5_batch\", h5_batch.shape, \"y_batch\", y_batch.shape )\n",
        "        loss_val, acc_val = sess.run([loss, accuracy], feed_dict={X: h2_cache_valid_2, ys: y_valid})\n",
        "        #loss_val = loss.eval(feed_dict={X: h2_cache_valid_2[:10], ys: y_valid[:10]})\n",
        "        if loss_val < best_loss:\n",
        "            save_path = saver.save(sess, \"./my_mnist_model_5_to_9_five_frozen\")\n",
        "            best_loss = loss_val\n",
        "            checks_without_progress = 0\n",
        "        else:\n",
        "            checks_without_progress += 1\n",
        "            if checks_without_progress > max_checks_without_progress:\n",
        "                print(\"Early stopping!\")\n",
        "                break\n",
        "        if epoch % 100 == 0:\n",
        "          \n",
        "          \n",
        "          #clear_output(wait=True)\n",
        "          print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n",
        "              epoch, loss_val, best_loss, acc_val * 100))\n",
        "          #print('Learning rate: %f' % (sess.run(training_op ._lr)))\n",
        "          #print(printm())\n",
        "\n",
        "    t1 = time.time()\n",
        "    print(\"Total training time: {:.1f}s\".format(t1 - t0))\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    saver.restore(sess, \"./my_mnist_model_5_to_9_five_frozen\")\n",
        "    acc_test = accuracy.eval(feed_dict={X: X_test, ys: y_test})\n",
        "    print(\"Final test accuracy: {:.2f}%\".format(acc_test * 100))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\tValidation loss: 2.251106\tBest loss: 2.251106\tAccuracy: 21.93%\n",
            "100\tValidation loss: 1.595120\tBest loss: 1.591118\tAccuracy: 23.71%\n",
            "200\tValidation loss: 1.597980\tBest loss: 1.589802\tAccuracy: 24.93%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FOuNWx7FLVvy",
        "colab_type": "code",
        "outputId": "713cffc7-eaf8-4b95-debb-03deb7e8f550",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "print(logits.shape)\n",
        "print(y_batch.shape)\n",
        "print(ys.shape)\n",
        "print(predictions.shape)\n",
        "\n",
        "\n",
        "#d = y_valid.reshape(734)\n",
        "#d.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(?, 10)\n",
            "(20,)\n",
            "(20,)\n",
            "(?, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tpb62qgb0QHB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 10\n",
        "rnd_idx = np.random.permutation(2200)\n",
        "for rnd_indices in np.array_split(rnd_idx, 2200 // batch_size):\n",
        "  print(rnd_indices.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3DW_Ns1r_7uj",
        "colab_type": "code",
        "outputId": "f6b2bb41-5fbc-4f9c-fde2-484c1db84d6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "len(X_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2202"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 123
        }
      ]
    },
    {
      "metadata": {
        "id": "61UU2EIqjhVM",
        "colab_type": "code",
        "outputId": "919a4a0e-c069-433f-c65e-93efb7b4682b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "y_batch"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 0, 1, 4, 4, 1, 4, 3, 1, 1, 1, 1, 3, 4, 0, 2, 0, 2, 0],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 154
        }
      ]
    }
  ]
}