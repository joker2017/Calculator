{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Simple transfer learning with image modules from TensorFlow Hub.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joker2017/Calculator/blob/master/Simple_transfer_learning_with_image_modules_from_TensorFlow_Hub.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "r9hYI7tIy6wo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "https://github.com/tensorflow/hub/tree/master/examples/image_retraining\n",
        "\n",
        "\n",
        "Простое обучение переносу с помощью модулей изображений из TensorFlow Hub.\n",
        "В этом примере показано, как обучить классификатор изображений на основе любого\n",
        "модуля TensorFlow Hub, который вычисляет векторы элементов изображения. По умолчанию,\n",
        "он использует векторы функций, вычисленные Inception V3, обученные в ImageNet.\n",
        "См. Https://github.com/tensorflow/hub/blob/master/docs/modules/image.md.\n",
        "\n",
        "Верхний слой получает в качестве входных данных 2048-мерный вектор (при условии\n",
        "Inception V3) для каждого изображения. Мы тренируем слой softmax поверх этого\n",
        "представления. Если слой softmax содержит N меток, это соответствует \n",
        "изучению N + 2048 * N параметров модели для смещений и весов.\n",
        "Вот пример, который предполагает, что у вас есть папка с именем класса, каждая из которых\n",
        "содержит изображения для каждой метки. Папка с примерами flower_photosдолжна иметь такую структуру:\n",
        "~ / Flower_photos / ромашка / photo1.jpg\n",
        "~ / Flower_photos / ромашка / photo2.jpg\n",
        "...\n",
        "~ / Flower_photos / роза / anotherphoto77.jpg\n",
        "...\n",
        "~ / Flower_photos / Подсолнечник / somepicture.jpg\n",
        "\n",
        "Имена подпапок важны, так как они определяют, к какой метке применяется\n",
        "каждое изображение, но сами имена файлов не имеют значения. (Для рабочего примера\n",
        "скачать http://download.tensorflow.org/example_images/flower_photos.tgz\n",
        "и запустите tar xzf flower_photos.tgz, чтобы распаковать его.)\n",
        "Как только ваши изображения подготовлены, и у вас есть установленный Тензор-хаб и достаточно свежая версия tensenflow, вы можете запустить обучение с команды как эта:\n",
        "```bash\n",
        "python retrain.py --image_dir ~ / flower_photos\n",
        "```\n",
        "Вы можете заменить аргумент image_dir любой папкой, содержащей подпапки изображений. Метка для каждого изображения берется из названия подпапки.\n",
        "Это создает новый файл модели, который может быть загружен и запущен любым TensorFlow запрограммируйте, например, пример кода \n",
        "tenorflow / examples / label_image. По умолчанию этот скрипт будет использовать очень точный, но сравнительно большой и медленная Inception V3 \n",
        "модель архитектуры. Рекомендуется начать с этого чтобы убедиться, что вы собрали хорошие учебные данные, но если вы хотите развернуть\n",
        "на платформах с ограниченными ресурсами вы можете попробовать флаг `--tfhub_module` с модели mobilenet. Для получения дополнительной информации о \n",
        "Mobilenet, см. https://research.googleblog.com/2017/06/mobilenets-open-source-models-for.html\n",
        "Например:\n",
        "Запустите версию Mobilenet с плавающей запятой:\n",
        "`` `Баш\n",
        "python retrain.py --image_dir ~ / flower_photos \\\n",
        "    --tfhub_module https://tfhub.dev/google/imagenet/mobilenet_v1_100_224/feature_vector/1\n",
        "`` `\n",
        "Запустить Mobilenet, инструментированный для квантования:\n",
        "`` `Баш\n",
        "python retrain.py --image_dir ~ / flower_photos / \\\n",
        "    --tfhub_module https://tfhub.dev/google/imagenet/mobilenet_v1_100_224/quantops/feature_vector/1\n",
        "`` `\n",
        "Эти инструментальные модели могут быть преобразованы в полностью квантованные мобильные модели с помощью\n",
        "TensorFlow Lite.\n",
        "На выбор предлагаются разные модели Mobilenet с различными файлами.\n",
        "параметры размера и задержки.\n",
        "  - Первое число может быть «100», «075», «050» или «025» для управления номером.\n",
        "    нейронов (активация скрытых слоев); количество весов (и, следовательно,\n",
        "    в некоторой степени размер файла и скорость) уменьшается с квадратом этого\n",
        "    доля.\n",
        "  - Второе число - размер входного изображения. Вы можете выбрать «224», «192»,\n",
        "    «160» или «128» с меньшими размерами, обеспечивающими более высокие скорости.\n",
        "Для использования с TensorBoard:\n",
        "По умолчанию этот скрипт будет записывать сводки в каталог / tmp / retrain_logs\n",
        "Визуализируйте итоги с помощью этой команды:\n",
        "тензорная доска --logdir / tmp / retrain_logs\n",
        "Чтобы использовать с Tensorflow Serving, запустите этот инструмент с набором --saved_model_dir\n",
        "в какое-либо все более пронумерованное место экспорта под базовым путем модели, например:\n",
        "`` `Баш\n",
        "python retrain.py (... другие аргументы, как и раньше ...) \\\n",
        "    --saved_model_dir = / tmp / save_models / $ (дата +% s) /\n",
        "tenorflow_model_server --port = 9000 --model_name = my_image_classifier \\\n",
        "    --model_base_path = / TMP / saved_models /"
      ]
    },
    {
      "metadata": {
        "id": "uM1hmkK3zi8f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import argparse\n",
        "import collections\n",
        "from datetime import datetime\n",
        "import hashlib\n",
        "import os.path\n",
        "import random\n",
        "import re\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "FLAGS = None\n",
        "\n",
        "MAX_NUM_IMAGES_PER_CLASS = 2 ** 27 - 1  # ~134M\n",
        "\n",
        "# The location where variable checkpoints will be stored.\n",
        "# Место, где будут храниться переменные контрольные точки.\n",
        "CHECKPOINT_NAME = '/tmp/_retrain_checkpoint'\n",
        "\n",
        "# Модуль понимается как инструмент для квантования с TF-Lite если он содержит какие-либо из этих операций.\n",
        "\n",
        "FAKE_QUANT_OPS = ('FakeQuantWithMinMaxVars',\n",
        "                  'FakeQuantWithMinMaxVarsPerChannel')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LxFLpEN1zkX7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **create_image_lists(happy)**\n",
        "Создает список тренировочных образов из файловой системы.\n",
        "   Анализирует вложенные папки в каталоге изображений, разбивает их на стабильные\n",
        "   обучающие, тестирующие и проверочные наборы и возвращающие структуру данных\n",
        "   описание списков изображений для каждой метки и их пути.\n",
        " > ** Args:**\n",
        "   *  *image_dir:* строковый путь к папке, содержащей подпапки изображений.\n",
        "   * *testing_percentage: *целочисленный процент изображений, зарезервированных для тестов.\n",
        "   * *validation_percentage: *целочисленный процент изображений, зарезервированных для проверки.\n",
        "\n",
        "> **  Return:**\n",
        "   * OrderedDict, содержащий запись для каждой подпапки метки с изображениями\n",
        "     разделить на наборы обучения, тестирования и проверки на каждом ярлыке.\n",
        "     Порядок предметов определяет индексы классов.\n",
        "\n",
        "\n",
        "> ссылки:\n",
        "*  [collections.OrderedDict()](https://pythonworld.ru/moduli/modul-collections.html)\n",
        "* [set](https://pythonworld.ru/tipy-dannyx-v-python/mnozhestva-set-i-frozenset.html)\n",
        "* [tf.gfile.Walk](https://www.tensorflow.org/api_docs/python/tf/gfile/Walk)\n",
        "* [tf.gfile.Glob ](https://www.tensorflow.org/api_docs/python/tf/gfile/Glob)\n",
        "* [tf.gfile.Exists](https://www.tensorflow.org/api_docs/python/tf/gfile/Exists)\n",
        "* [os.path.normcase, os.path.basename](https://docs.python.org/2/library/os.path.html)\n",
        "* [re.sub](https://tproger.ru/translations/regular-expression-python/)\n",
        "* [str.lower](http://pythonz.net/references/named/str.lower/)\n",
        "* [hashlib.sha1](https://docs.python.org/3/library/hashlib.html)"
      ]
    },
    {
      "metadata": {
        "id": "8WURLYEf0hht",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_image_lists(image_dir, testing_percentage, validation_percentage):\n",
        " \n",
        "  if not tf.gfile.Exists(image_dir):   #tf.gfile.Exists Определяет, существует ли путь или нет.Истинно, если путь существует, будь то файл или каталог.\n",
        "    tf.logging.error(\"Image directory '\" + image_dir + \"' not found.\")\n",
        "    return None\n",
        "  result = collections.OrderedDict() \n",
        "  #похожий на словарь объект, но он помнит порядок, в котором ему были даны ключи.\n",
        "  \n",
        "  \n",
        "  sub_dirs = sorted(x[0] for x in tf.gfile.Walk(image_dir)) \n",
        "  # tf.gfile.Walk Рекурсивный генератор дерева каталогов для каталогов.\n",
        "  # The root directory comes first, so skip it.\n",
        "  # Корневой каталог стоит первым, поэтому пропустите его\n",
        "  is_root_dir = True       #?????\n",
        "  for sub_dir in sub_dirs: #?????\n",
        "    if is_root_dir:        #?????\n",
        "      is_root_dir = False  #?????\n",
        "      continue             #?????\n",
        "    extensions = sorted(set(os.path.normcase(ext) for ext in ['JPEG', 'JPG', 'jpeg', 'jpg'])) \n",
        "    # аналогично просто написать ['JPEG', 'JPG', 'jpeg', 'jpg']\n",
        "    # os.path.normcase(path) - нормализует регистр пути\n",
        "    # set создает множество удаляет повторения \n",
        "    file_list = []\n",
        "    dir_name = os.path.basename(sub_dir) #os.path.basename(path) - базовое имя пути (эквивалентно os.path.split(path)[1]).\n",
        "    if dir_name == image_dir:\n",
        "      continue   #Оператор continue начинает следующий проход цикла, минуя оставшееся тело цикла (for или while)\n",
        "    tf.logging.info(\"Looking for images in '\" + dir_name + \"'\")\n",
        "    for extension in extensions:\n",
        "      file_glob = os.path.join(image_dir, dir_name, '*.' + extension)\n",
        "      file_list.extend(tf.gfile.Glob(file_glob))   #  tf.gfile.Glob Возвращает список файлов, которые соответствуют заданному шаблону\n",
        "    if not file_list:\n",
        "      tf.logging.warning('No files found')\n",
        "      continue\n",
        "    if len(file_list) < 20:\n",
        "      tf.logging.warning(\n",
        "          'WARNING: Folder has less than 20 images, which may cause issues.')\n",
        "    elif len(file_list) > MAX_NUM_IMAGES_PER_CLASS:\n",
        "      tf.logging.warning(\n",
        "          'WARNING: Folder {} has more than {} images. Some images will '\n",
        "          'never be selected.'.format(dir_name, MAX_NUM_IMAGES_PER_CLASS))\n",
        "    label_name = re.sub(r'[^a-z0-9]+', ' ', dir_name.lower())  #re.sub ищет шаблон в строке и заменяет его на указанную подстроку.\n",
        "                                                                #str.lower возвращает копию исходной строки с символами приведёнными к нижнему регистру.\n",
        "    training_images = []\n",
        "    testing_images = []\n",
        "    validation_images = []\n",
        "    for file_name in file_list:\n",
        "      base_name = os.path.basename(file_name)\n",
        "      # Мы хотим игнорировать что-либо после '_nohash_' в имени файла, когда\n",
        "       # решая, в какой набор поместить изображение, создатель набора данных может\n",
        "       # группировка фотографий, которые являются близкими вариациями друг друга. Например\n",
        "       # это используется в наборе данных о заболеваниях растений для группировки нескольких изображений\n",
        "       # тот же лист.\n",
        "      hash_name = re.sub(r'_nohash_.*$', '', file_name)\n",
        "      # Это выглядит немного волшебно, но мы должны решить, должен ли этот файл\n",
        "       # перейти к обучающим, тестовым или проверочным наборам, и мы хотим сохранить\n",
        "       # существующие файлы в том же наборе, даже если впоследствии будет больше файлов\n",
        "       # добавлено.\n",
        "       # Для этого нам нужен стабильный способ решения, основанный только на имени файла\n",
        "       # сам, поэтому мы делаем это хеш, а затем используем его для генерации\n",
        "       # значение вероятности, которое мы используем для его назначения.\n",
        "      hash_name_hashed = hashlib.sha1(tf.compat.as_bytes(hash_name)).hexdigest() \n",
        "       #hashlib.sha1().hexdigest() хеширует аргумент и возвращает захе-е значение\n",
        "       # tf.compat.as_bytes Converts either bytes or unicode to bytes, using utf-8 encoding for text.\n",
        "      percentage_hash = ((int(hash_name_hashed, 16) %\n",
        "                          (MAX_NUM_IMAGES_PER_CLASS + 1)) *\n",
        "                         (100.0 / MAX_NUM_IMAGES_PER_CLASS))\n",
        "      if percentage_hash < validation_percentage:\n",
        "        validation_images.append(base_name)\n",
        "      elif percentage_hash < (testing_percentage + validation_percentage):\n",
        "        testing_images.append(base_name)\n",
        "      else:\n",
        "        training_images.append(base_name)\n",
        "    result[label_name] = {\n",
        "        'dir': dir_name,\n",
        "        'training': training_images,\n",
        "        'testing': testing_images,\n",
        "        'validation': validation_images,\n",
        "    }\n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hE1K7-3a0jW5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **get_image_path()**\n",
        "Возвращает путь к изображению для метки по указанному индексу.\n",
        "> **  Args:**\n",
        "* *image_lists:* OrderedDict обучающих изображений для каждой метки.\n",
        "* *label_name: *строка метки, для которой мы хотим получить изображение.\n",
        "* *index:* Int смещение изображения, которое мы хотим. Это будет модулировано\n",
        "     Доступно количество изображений для метки, поэтому оно может быть сколь угодно большим.\n",
        "* *image_dir:* строка корневых папок подпапок, содержащих обучение\n",
        "     изображений.\n",
        "* *category:* Имя строки набора для извлечения изображений - обучение, тестирование или проверки.\n",
        " \n",
        " > ** Return**\n",
        "* Строка пути файловой системы к изображению, которое соответствует запрошенным параметрам.\n"
      ]
    },
    {
      "metadata": {
        "id": "lyfaQKul123T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_image_path(image_lists, label_name, index, image_dir, category):\n",
        "  \n",
        "  if label_name not in image_lists:\n",
        "    tf.logging.fatal('Label does not exist %s.', label_name)\n",
        "  label_lists = image_lists[label_name]\n",
        "  if category not in label_lists:\n",
        "    tf.logging.fatal('Category does not exist %s.', category)\n",
        "  category_list = label_lists[category]\n",
        "  if not category_list:\n",
        "    tf.logging.fatal('Label %s has no images in the category %s.',\n",
        "                     label_name, category)\n",
        "  mod_index = index % len(category_list)\n",
        "  base_name = category_list[mod_index]\n",
        "  sub_dir = label_lists['dir']\n",
        "  full_path = os.path.join(image_dir, sub_dir, base_name)\n",
        "  return full_path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FF1HhMl716mo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **get_bottleneck_path()**\n",
        "Возвращает путь к файлу узкого места для метки по указанному индексу.\n",
        ">  **Args:**\n",
        "* *image_lists:* OrderedDict обучающих изображений для каждой метки.\n",
        "* *label_name: *строка метки, для которой мы хотим получить изображение.\n",
        "* *index: *целочисленное смещение изображения, которое мы хотим. Это будет модулировано\n",
        "     Доступно количество изображений для метки, поэтому оно может быть сколь угодно большим.\n",
        "* *bottleneck_dir: *строка папки, содержащая кэшированные файлы значений узких мест.\n",
        "* *category: *Имя строки набора для извлечения изображений - обучение, тестирование или Проверка.\n",
        "* *module_name: *имя используемого модуля изображения.\n",
        "\n",
        "> **Return:**\n",
        "* Строка пути файловой системы к изображению, которое соответствует запрошенным параметрам.\n",
        "\n",
        "> ссылки\n",
        "* [string.replace](http://pythonz.net/references/named/string.replace/)\n"
      ]
    },
    {
      "metadata": {
        "id": "XSm_Z7pu2yDa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_bottleneck_path(image_lists, label_name, index, bottleneck_dir,\n",
        "                        category, module_name):\n",
        "  module_name = (module_name.replace('://', '~')  # .replace Возвращает копию строки, в которой заменены все вхождения указанной строки указанным значением.\n",
        "                 .replace('/', '~')  # URL and Unix paths.\n",
        "                 .replace(':', '~').replace('\\\\', '~'))  # Windows paths.\n",
        "  return get_image_path(image_lists, label_name, index, bottleneck_dir,\n",
        "                        category) + '_' + module_name + '.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DtBEz9AJ20Uc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **create_module_graph()**\n",
        " Создает график и загружает в него модуль-концентратор.\n",
        " >**Args:**\n",
        "* *module_spec:* hub.ModuleSpec для используемого модуля изображения.\n",
        "\n",
        ">**Return:**\n",
        "* *TF.Graph*, который был создан.\n",
        "* *bottleneck_tensor*: значения узких мест, выводимые модулем.\n",
        "* *resized_input_tensor:* входные изображения, размеры которых изменяются в соответствии с ожиданиями модуля.\n",
        "* *want_quanization:* логическое значение, был ли модуль оснащен с поддельной операцией квантования.\n",
        "\n",
        "> ссылки\n",
        "* [hub.get_expected_image_size](https://www.tensorflow.org/hub/api_docs/python/hub/get_expected_image_size)\n",
        "* [hub.Module](https://www.tensorflow.org/hub/api_docs/python/hub/Module)\n",
        "* [any](https://pythonworld.ru/osnovy/vstroennye-funkcii.html)\n",
        "* [op&node](https://www.tensorflow.org/guide/extend/model_files)\n",
        "* [FAKE_QUANT_OPS](https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/fake-quant-with-min-max-vars-per-channel)"
      ]
    },
    {
      "metadata": {
        "id": "-B-YbABE3qFU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_module_graph(module_spec):\n",
        "  \n",
        "  height, width = hub.get_expected_image_size(module_spec)  #Возвращает ожидаемые [высота, ширина] размеры входного изображения.\n",
        "  with tf.Graph().as_default() as graph:\n",
        "    resized_input_tensor = tf.placeholder(tf.float32, [None, height, width, 3])\n",
        "    m = hub.Module(module_spec)  \n",
        "    # hub.Module Модуль представляет собой часть графа TensorFlow, который можно экспортировать на диск (в соответствии с форматом SavedModel), а\n",
        "    # затем повторно загрузить. \n",
        "    bottleneck_tensor = m(resized_input_tensor)\n",
        "    wants_quantization = any(node.op in FAKE_QUANT_OPS\n",
        "                             for node in graph.as_graph_def().node) \n",
        "    # Квантование — разбиение диапазона значений некоторой величины на конечное число уровней и округление этих значений до ближайших к ним уровней.\n",
        "    # op node операция и узел \n",
        "    # any(последовательность) - Возвращает True, если хотя бы один элемент - истина. Для пустой последовательности возвращает False\n",
        "  return graph, bottleneck_tensor, resized_input_tensor, wants_quantization"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RdqgHQo63rV1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **run_bottleneck_on_image()**\n",
        "  Запускает вывод для изображения, чтобы извлечь сводный слой «bottleneck».\n",
        ">**Args:**\n",
        "* sess: Текущий активный сеанс TensorFlow.\n",
        "* image_data: строка необработанных данных JPEG.\n",
        "* image_data_tensor: входной слой данных на графике.\n",
        "* decoded_image_tensor: Вывод исходного изменения размера изображения и предварительной обработки.\n",
        "* resized_input_tensor: входной узел графа распознавания.\n",
        "* bottleneck_tensor: слой перед окончательным softmax.\n",
        "\n",
        ">**Return:**\n",
        "* Numpy массив значений узкого места.\n",
        "\n",
        "> ссылки\n",
        "* [numpy.squeeze](https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.squeeze.html)"
      ]
    },
    {
      "metadata": {
        "id": "CZIXe2Rk4IBM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def run_bottleneck_on_image(sess, image_data, image_data_tensor,\n",
        "                            decoded_image_tensor, resized_input_tensor,\n",
        "                            bottleneck_tensor):\n",
        "\n",
        "  # First decode the JPEG image, resize it, and rescale the pixel values.\n",
        "  # Сначала декодируйте изображение JPEG, измените его размер и измените размеры пикселей.\n",
        "  resized_input_values = sess.run(decoded_image_tensor,\n",
        "                                  {image_data_tensor: image_data})\n",
        "  # Then run it through the recognition network.\n",
        "  # Затем запустите его через сеть распознавания.\n",
        "  bottleneck_values = sess.run(bottleneck_tensor,\n",
        "                               {resized_input_tensor: resized_input_values})\n",
        "  bottleneck_values = np.squeeze(bottleneck_values) # np.squeeze Удалите одномерные записи из формы массива.\n",
        "  return bottleneck_values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0fgBS6eu4Od9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def ensure_dir_exists(dir_name):\n",
        " \"\"\" \n",
        "  Убедитесь, что папка существует на диске.\n",
        "   Args:\n",
        "  dir_name: строка пути к папке, которую мы хотим создать.\n",
        "  \"\"\"\n",
        "  if not os.path.exists(dir_name):\n",
        "    os.makedirs(dir_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8jVrrI6N4W9k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **create_bottleneck_file()**\n",
        "Create a single bottleneck file\n",
        "\n",
        "[bottleneck(oreilly)](https://learning.oreilly.com/library/view/deep-learning-with/9781786469786/9444e43d-d496-4a21-a7b8-7df767c527d6.xhtml) это термин, который TensorFlow использует для ссылки на слой непосредственно перед последним слоем, который фактически отвечает за классификацию. Таким образом, любое изображение в обучающем наборе используется несколько раз во время обучения, и вычисление слоев за узким местом занимает много времени для каждого изображения. Таким образом, кэшируя выходные данные этих нижних уровней на диске, мы избегаем тратить значительное количество времени. По умолчанию узкое место хранится в /tmp/bottleneckкаталоге.\n",
        " [bottleneck](https://ai.stackexchange.com/questions/4864/what-is-the-concept-of-tensorflow-bottlenecks)\n",
        " \n",
        " > ссылки:\n",
        " * [tf.gfile.FastGFile](https://www.tensorflow.org/api_docs/python/tf/gfile/FastGFile)"
      ]
    },
    {
      "metadata": {
        "id": "mainYHUK4WKi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_bottleneck_file(bottleneck_path, image_lists, label_name, index,\n",
        "                           image_dir, category, sess, jpeg_data_tensor,\n",
        "                           decoded_image_tensor, resized_input_tensor,\n",
        "                           bottleneck_tensor):\n",
        "  \"\"\"Create a single bottleneck file.Создайте один файл узкого места.\"\"\"\n",
        "  tf.logging.info('Creating bottleneck at ' + bottleneck_path)\n",
        "  image_path = get_image_path(image_lists, label_name, index,\n",
        "                              image_dir, category)\n",
        "  if not tf.gfile.Exists(image_path):\n",
        "    tf.logging.fatal('File does not exist %s', image_path)\n",
        "  image_data = tf.gfile.FastGFile(image_path, 'rb').read()  #файловый ввод / вывод\n",
        "  try:\n",
        "    bottleneck_values = run_bottleneck_on_image(\n",
        "        sess, image_data, jpeg_data_tensor, decoded_image_tensor,\n",
        "        resized_input_tensor, bottleneck_tensor)\n",
        "  except Exception as e:\n",
        "    raise RuntimeError('Error during processing file %s (%s)' % (image_path,\n",
        "                                                                 str(e)))\n",
        "  bottleneck_string = ','.join(str(x) for x in bottleneck_values)\n",
        "  with open(bottleneck_path, 'w') as bottleneck_file:\n",
        "    bottleneck_file.write(bottleneck_string)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hpTdWQ1C4pHI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#** get_or_create_bottleneck(): **\n",
        "Извлекает или вычисляет значения узкого места для изображения.\n",
        "  Если кэшированная версия данных о узких местах существует на диске, верните ее,\n",
        "  в противном случае рассчитайте данные и сохраните их на диск для дальнейшего использования.\n",
        ">**Args:**\n",
        "* *sess:* текущий активный сеанс TensorFlow.\n",
        "* *image_lists:* OrderedDict обучающих изображений для каждой метки.\n",
        "* *label_name:* строка метки, для которой мы хотим получить изображение.\n",
        "* *index:* целочисленное смещение изображения, которое мы хотим. Это будет по модулю Доступно количество изображений для метки, поэтому оно может быть сколь угодно большим.\n",
        "* *image_dir:* строка корневых папок подпапок, содержащих обучение изображений        \n",
        "* *category:* Строка имени, из которой нужно извлекать изображения - обучение, тестирование или проверка.\n",
        "* *bottleneck_dir: * строка папки, содержащая кэшированные файлы значений узких мест.\n",
        "* *jpeg_data_tensor:* тензор для подачи загруженных данных JPEG в.\n",
        "** decoded_image_tensor:* вывод декодирования и изменения размера изображения.\n",
        "* *resized_input_tensor:* входной узел графа распознавания.\n",
        "* *bottleneck_tensor:* выходной тензор для значений узких мест.\n",
        "* *module_name:* имя используемого модуля изображения.\n",
        "\n",
        ">**Return:**\n",
        "* Numpy массив значений, созданный слоем узкого места для изображения.\n"
      ]
    },
    {
      "metadata": {
        "id": "mehRgFL16XhL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_or_create_bottleneck(sess, image_lists, label_name, index, image_dir,\n",
        "                             category, bottleneck_dir, jpeg_data_tensor,\n",
        "                             decoded_image_tensor, resized_input_tensor,\n",
        "                             bottleneck_tensor, module_name):\n",
        " \n",
        "  label_lists = image_lists[label_name]\n",
        "  sub_dir = label_lists['dir']\n",
        "  sub_dir_path = os.path.join(bottleneck_dir, sub_dir)\n",
        "  ensure_dir_exists(sub_dir_path)\n",
        "  bottleneck_path = get_bottleneck_path(image_lists, label_name, index,\n",
        "                                        bottleneck_dir, category, module_name)\n",
        "  if not os.path.exists(bottleneck_path):\n",
        "    create_bottleneck_file(bottleneck_path, image_lists, label_name, index,\n",
        "                           image_dir, category, sess, jpeg_data_tensor,\n",
        "                           decoded_image_tensor, resized_input_tensor,\n",
        "                           bottleneck_tensor)\n",
        "  with open(bottleneck_path, 'r') as bottleneck_file:\n",
        "    bottleneck_string = bottleneck_file.read()\n",
        "  did_hit_error = False\n",
        "  try:\n",
        "    bottleneck_values = [float(x) for x in bottleneck_string.split(',')]\n",
        "  except ValueError:\n",
        "    tf.logging.warning('Invalid float found, recreating bottleneck')\n",
        "    did_hit_error = True\n",
        "  if did_hit_error:\n",
        "    create_bottleneck_file(bottleneck_path, image_lists, label_name, index,\n",
        "                           image_dir, category, sess, jpeg_data_tensor,\n",
        "                           decoded_image_tensor, resized_input_tensor,\n",
        "                           bottleneck_tensor)\n",
        "    with open(bottleneck_path, 'r') as bottleneck_file:\n",
        "      bottleneck_string = bottleneck_file.read()\n",
        "# Разрешить распространение исключений здесь, так как они не должны произойти после\n",
        "# свежее творение\n",
        "    bottleneck_values = [float(x) for x in bottleneck_string.split(',')]\n",
        "  return bottleneck_values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PNb2HwNG6cvm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **cache_bottlenecks()**\n",
        "\n",
        "Гарантирует, что все узкие места обучения, тестирования и проверки кэшируются.\n",
        "Потому что мы можем прочитать одно и то же изображение несколько раз (если нет искажения, применяемые во время тренировок), если мы рассчитать значения слоя узкого места один раз для каждого изображения во время предварительной обработки, а затем просто несколько раз прочитать эти кэшированные значения во время повышение квалификации. Здесь мы рассмотрим все изображения, которые мы нашли, рассчитать те, значения и сохранить их.\n",
        ">**Args**:\n",
        "* *sess:* текущий активный сеанс TensorFlow.\n",
        "* *image_lists:* OrderedDict обучающих изображений для каждой метки.\n",
        "* *image_dir:* строка корневых папок подпапок, содержащих обучение\n",
        "    изображений.\n",
        "* *bottleneck_dir:* строка папки, содержащая кэшированные файлы значений узких мест.\n",
        "* *jpeg_data_tensor:* входной тензор для данных JPEG из файла.\n",
        "* *decoded_image_tensor:* вывод декодирования и изменения размера изображения.\n",
        "* *resized_input_tensor:* входной узел графа распознавания.\n",
        "* *bottleneck_tensor:* предпоследний выходной слой графика.\n",
        "* *module_name* имя используемого модуля изображения.\n",
        "\n",
        ">**Return**\n",
        "* Ничего такого."
      ]
    },
    {
      "metadata": {
        "id": "E0c9gUye7Owb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def cache_bottlenecks(sess, image_lists, image_dir, bottleneck_dir,\n",
        "                      jpeg_data_tensor, decoded_image_tensor,\n",
        "                      resized_input_tensor, bottleneck_tensor, module_name):\n",
        "\n",
        "  how_many_bottlenecks = 0\n",
        "  ensure_dir_exists(bottleneck_dir)\n",
        "  for label_name, label_lists in image_lists.items():\n",
        "    for category in ['training', 'testing', 'validation']:\n",
        "      category_list = label_lists[category]\n",
        "      for index, unused_base_name in enumerate(category_list):\n",
        "        get_or_create_bottleneck(\n",
        "            sess, image_lists, label_name, index, image_dir, category,\n",
        "            bottleneck_dir, jpeg_data_tensor, decoded_image_tensor,\n",
        "            resized_input_tensor, bottleneck_tensor, module_name)\n",
        "\n",
        "        how_many_bottlenecks += 1\n",
        "        if how_many_bottlenecks % 100 == 0:\n",
        "          tf.logging.info(\n",
        "              str(how_many_bottlenecks) + ' bottleneck files created.')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RIJMZ8Ul7U50",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **def get_random_cached_bottlenecks()**\n",
        "Получает значения узких мест для кэшированных изображений. Если искажения не применяются, эта функция может извлечь кэшированный узкие места значения непосредственно с диска для изображений. Он выбирает случайный набор изображения из указанной категории.\n",
        "> **Args:**\n",
        "* *sess:* Текущая сессия TensorFlow.\n",
        "* *image_lists:* OrderedDict обучающих изображений для каждой метки.\n",
        "* *how_many:*  Если положительный, будет выбрана случайная выборка такого размера.\n",
        "    Если отрицательный, все узкие места будут восстановлены.\n",
        "    Категория: Строка имени, из которой нужно извлечь - обучение, тестирование или проверка.\n",
        "* *bottleneck_dir:* строка папки, содержащая кэшированные файлы значений узких мест.\n",
        "* *image_dir: *строка корневых папок подпапок, содержащих обучение изображений.\n",
        "* *jpeg_data_tensor:* слой для подачи данных изображения jpeg.\n",
        "* *decoded_image_tensor:* вывод декодирования и изменения размера изображения.\n",
        "* *resized_input_tensor:* входной узел графа распознавания.\n",
        "* *bottleneck_tensor:* выходной слой узкого места графа CNN.\n",
        "* *module_name:* имя используемого модуля изображения.\n",
        "\n",
        "> **Return**\n",
        "* Список массивов узких мест, их соответствующие основные истины и\n",
        "    соответствующие имена файлов.\n",
        "    \n",
        "> Ссылки\n",
        "* [random.randrange](https://docs.python.org/3.1/library/random.html)"
      ]
    },
    {
      "metadata": {
        "id": "oNcm03hz8J-B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_random_cached_bottlenecks(sess, image_lists, how_many, category,\n",
        "                                  bottleneck_dir, image_dir, jpeg_data_tensor,\n",
        "                                  decoded_image_tensor, resized_input_tensor,\n",
        "                                  bottleneck_tensor, module_name):\n",
        "\n",
        "  class_count = len(image_lists.keys())\n",
        "  bottlenecks = []\n",
        "  ground_truths = []\n",
        "  filenames = []\n",
        "  if how_many >= 0:\n",
        "    # Retrieve a random sample of bottlenecks.\n",
        "    # Получить случайную выборку узких мест.\n",
        "    for unused_i in range(how_many):\n",
        "      label_index = random.randrange(class_count) # random.randrange Вернуть случайно выбранный элемент из диапазона (начало, остановка, шаг) \n",
        "      label_name = list(image_lists.keys())[label_index]\n",
        "      image_index = random.randrange(MAX_NUM_IMAGES_PER_CLASS + 1)\n",
        "      image_name = get_image_path(image_lists, label_name, image_index,\n",
        "                                  image_dir, category)\n",
        "      bottleneck = get_or_create_bottleneck(\n",
        "          sess, image_lists, label_name, image_index, image_dir, category,\n",
        "          bottleneck_dir, jpeg_data_tensor, decoded_image_tensor,\n",
        "          resized_input_tensor, bottleneck_tensor, module_name)\n",
        "      bottlenecks.append(bottleneck)\n",
        "      ground_truths.append(label_index)\n",
        "      filenames.append(image_name)\n",
        "  else:\n",
        "    # Retrieve all bottlenecks.\n",
        "    # Получить все узкие места.\n",
        "    for label_index, label_name in enumerate(image_lists.keys()):\n",
        "      for image_index, image_name in enumerate(\n",
        "          image_lists[label_name][category]):\n",
        "        image_name = get_image_path(image_lists, label_name, image_index,\n",
        "                                    image_dir, category)\n",
        "        bottleneck = get_or_create_bottleneck(\n",
        "            sess, image_lists, label_name, image_index, image_dir, category,\n",
        "            bottleneck_dir, jpeg_data_tensor, decoded_image_tensor,\n",
        "            resized_input_tensor, bottleneck_tensor, module_name)\n",
        "        bottlenecks.append(bottleneck)\n",
        "        ground_truths.append(label_index)\n",
        "        filenames.append(image_name)\n",
        "  return bottlenecks, ground_truths, filenames\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iXZDxlem8NrB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **def get_random_distorted_bottlenecks()**\n",
        "Извлекает узкие места для тренировочных образов после искажений.\n",
        "Если мы тренируемся с искажениями, такими как crops, scales, или flips, мы должны\n",
        "  пересчитать полную модель для каждого изображения, и мы не можем использовать кэшированные\n",
        "  значения узкого места. Вместо этого мы находим случайные изображения для запрошенной категории,\n",
        "  пропустите их через граф искажений, а затем полный граф, чтобы получить\n",
        "  узкое место результаты для каждого.\n",
        ">**Args:**\n",
        "* *sess:* Текущая сессия TensorFlow.\n",
        "* *image_lists:* OrderedDict обучающих изображений для каждой метки.\n",
        "* *how_many:* целое число возвращаемых значений узкого места.\n",
        "* *category: * Строка имени, набор изображений для извлечения - обучение, тестирование,\n",
        "    или проверка.\n",
        "* *image_dir:* строка корневых папок подпапок, содержащих обучение\n",
        "    изображений.\n",
        "* *input_jpeg_tensor:* входной слой, к которому мы подаем данные изображения.\n",
        "* *distorted_image:* выходной узел графика искажения.\n",
        "* *resized_input_tensor:* входной узел графа распознавания.\n",
        "* *bottleneck_tensor:* выходной слой узкого места графа CNN.\n",
        "\n",
        ">**Return**:\n",
        "* Список массивов узких мест и соответствующие им основные истины.\n",
        " "
      ]
    },
    {
      "metadata": {
        "id": "5Q3xNyQW8q3b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_random_distorted_bottlenecks(\n",
        "    sess, image_lists, how_many, category, image_dir, input_jpeg_tensor,\n",
        "    distorted_image, resized_input_tensor, bottleneck_tensor):\n",
        "\n",
        "  class_count = len(image_lists.keys())\n",
        "  bottlenecks = []\n",
        "  ground_truths = []\n",
        "  for unused_i in range(how_many):\n",
        "    label_index = random.randrange(class_count)\n",
        "    label_name = list(image_lists.keys())[label_index]\n",
        "    image_index = random.randrange(MAX_NUM_IMAGES_PER_CLASS + 1)\n",
        "    image_path = get_image_path(image_lists, label_name, image_index, image_dir,\n",
        "                                category)\n",
        "    if not tf.gfile.Exists(image_path):\n",
        "      tf.logging.fatal('File does not exist %s', image_path)\n",
        "    jpeg_data = tf.gfile.FastGFile(image_path, 'rb').read()\n",
        "    # Обратите внимание, что мы материализовали distorted_image_data как массив numpy до\n",
        "     # отправка рабочего вывода на изображение. Это включает в себя 2 копии памяти и\n",
        "     # может быть оптимизирован в других реализациях.\n",
        "    distorted_image_data = sess.run(distorted_image,\n",
        "                                    {input_jpeg_tensor: jpeg_data})\n",
        "    bottleneck_values = sess.run(bottleneck_tensor,\n",
        "                                 {resized_input_tensor: distorted_image_data})\n",
        "    bottleneck_values = np.squeeze(bottleneck_values)\n",
        "    bottlenecks.append(bottleneck_values)\n",
        "    ground_truths.append(label_index)\n",
        "  return bottlenecks, ground_truths"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "puocnYS28_fi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **def should_distort_images()**\n",
        "Включены ли какие-либо искажения из входных флагов.\n",
        "> **Args:**\n",
        "* *flip_left_right:* Boolean, следует ли случайным образом отражать изображения по горизонтали.\n",
        "* *random_crop:* целочисленный процент, задающий общую маржу, используемую вокруг ящик для обрезки.\n",
        "* *random_scale:* целое число в процентах от того, насколько сильно изменяется масштаб.\n",
        "* *random_brightness:* целочисленный диапазон для случайного умножения значений пикселей на.\n",
        "\n",
        "> **Return:**\n",
        "* Логическое значение, указывающее, следует ли применять какие-либо искажения.\n"
      ]
    },
    {
      "metadata": {
        "id": "FlrOnUBD9Zlu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def should_distort_images(flip_left_right, random_crop, random_scale,\n",
        "                          random_brightness):\n",
        "\n",
        "  return (flip_left_right or (random_crop != 0) or (random_scale != 0) or\n",
        "          (random_brightness != 0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6M3IGVeq9h-f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **def add_input_distortions()**\n",
        "Создает операции для применения указанных искажений.\n",
        "   Во время обучения это может помочь улучшить результаты, если мы запустим изображения\n",
        "   через простые искажения, такие как зерновые культуры, весы и сальто. Они отражают\n",
        "   вид изменений, которые мы ожидаем в реальном мире, и поэтому может помочь в обучении\n",
        "   модель, чтобы справиться с естественными данными более эффективно. Здесь мы берем поставленный\n",
        "   параметры и построить сеть операций, чтобы применить их к изображению.\n",
        "   Урожайность\n",
        "   ~~~~~~~~\n",
        "   Обрезка выполняется путем размещения ограничивающей рамки в случайном положении в полном объеме.\n",
        "   образ. Параметр обрезки определяет размер этого поля относительно\n",
        "   входное изображение. Если это ноль, то поле имеет тот же размер, что и вход, и нет\n",
        "   обрезка выполняется. Если значение составляет 50%, то поле кадрирования будет вдвое меньше\n",
        "   ширина и высота ввода. На диаграмме это выглядит так:\n",
        "  <       width         >\n",
        "  +---------------------+\n",
        "  |                     |\n",
        "  |   width - crop%     |\n",
        "  |    <      >         |\n",
        "  |    +------+         |\n",
        "  |    |      |         |\n",
        "  |    |      |         |\n",
        "  |    |      |         |\n",
        "  |    +------+         |\n",
        "  |                     |\n",
        "  |                     |\n",
        "  +---------------------+\n",
        "  Scaling\n",
        "  ~~~~~~~\n",
        "  Масштабирование очень похоже на обрезку, за исключением того, что ограничивающий прямоугольник всегда\n",
        "   по центру и его размер изменяется случайным образом в пределах данного диапазона. Например, если\n",
        "   масштабный процент равен нулю, тогда ограничивающий прямоугольник имеет тот же размер, что и\n",
        "   вход и масштабирование не применяется. Если это 50%, то ограничительная рамка будет в\n",
        "   случайный диапазон между половиной ширины и высоты и полным размером.\n",
        "> **Args:**\n",
        "* *flip_left_right:* Boolean, следует ли случайным образом отражать изображения по горизонтали.\n",
        "* *random_crop:* целочисленный процент, задающий общую маржу, используемую вокруг\n",
        "     ящик для обрезки.\n",
        "* *random_scale:* целое число в процентах от того, насколько сильно изменяется масштаб.\n",
        "* *random_brightness:* целочисленный диапазон для случайного умножения значений пикселей на.\n",
        "     граф.\n",
        "* *module_spec:* hub.ModuleSpec для используемого модуля изображения.\n",
        "\n",
        ">**Return:**\n",
        "     Входной слой jpeg и тензор искаженного результата.\n"
      ]
    },
    {
      "metadata": {
        "id": "ej2-qF8o-Fep",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def add_input_distortions(flip_left_right, random_crop, random_scale,\n",
        "                          random_brightness, module_spec):\n",
        "\n",
        "  input_height, input_width = hub.get_expected_image_size(module_spec)\n",
        "  input_depth = hub.get_num_image_channels(module_spec)\n",
        "  jpeg_data = tf.placeholder(tf.string, name='DistortJPGInput')\n",
        "  decoded_image = tf.image.decode_jpeg(jpeg_data, channels=input_depth)\n",
        "  # Convert from full range of uint8 to range [0,1] of float32.\n",
        "  decoded_image_as_float = tf.image.convert_image_dtype(decoded_image,\n",
        "                                                        tf.float32)\n",
        "  decoded_image_4d = tf.expand_dims(decoded_image_as_float, 0)\n",
        "  margin_scale = 1.0 + (random_crop / 100.0)\n",
        "  resize_scale = 1.0 + (random_scale / 100.0)\n",
        "  margin_scale_value = tf.constant(margin_scale)\n",
        "  resize_scale_value = tf.random_uniform(shape=[],\n",
        "                                         minval=1.0,\n",
        "                                         maxval=resize_scale)\n",
        "  scale_value = tf.multiply(margin_scale_value, resize_scale_value)\n",
        "  precrop_width = tf.multiply(scale_value, input_width)\n",
        "  precrop_height = tf.multiply(scale_value, input_height)\n",
        "  precrop_shape = tf.stack([precrop_height, precrop_width])\n",
        "  precrop_shape_as_int = tf.cast(precrop_shape, dtype=tf.int32)\n",
        "  precropped_image = tf.image.resize_bilinear(decoded_image_4d,\n",
        "                                              precrop_shape_as_int)\n",
        "  precropped_image_3d = tf.squeeze(precropped_image, axis=[0])\n",
        "  cropped_image = tf.random_crop(precropped_image_3d,\n",
        "                                 [input_height, input_width, input_depth])\n",
        "  if flip_left_right:\n",
        "    flipped_image = tf.image.random_flip_left_right(cropped_image)\n",
        "  else:\n",
        "    flipped_image = cropped_image\n",
        "  brightness_min = 1.0 - (random_brightness / 100.0)\n",
        "  brightness_max = 1.0 + (random_brightness / 100.0)\n",
        "  brightness_value = tf.random_uniform(shape=[],\n",
        "                                       minval=brightness_min,\n",
        "                                       maxval=brightness_max)\n",
        "  brightened_image = tf.multiply(flipped_image, brightness_value)\n",
        "  distort_result = tf.expand_dims(brightened_image, 0, name='DistortResult')\n",
        "  return jpeg_data, distort_result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0UrXlJxu-KdB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def variable_summaries(var):\n",
        "  \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
        "  with tf.name_scope('summaries'):\n",
        "    mean = tf.reduce_mean(var)\n",
        "    tf.summary.scalar('mean', mean)\n",
        "    with tf.name_scope('stddev'):\n",
        "      stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
        "    tf.summary.scalar('stddev', stddev)\n",
        "    tf.summary.scalar('max', tf.reduce_max(var))\n",
        "    tf.summary.scalar('min', tf.reduce_min(var))\n",
        "    tf.summary.histogram('histogram', var)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1auuvmen-LjG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **def add_final_retrain_ops()**\n",
        "Добавляет новый softmax и полностью связанный слой для обучения и оценки.\n",
        "  Нам нужно переобучить верхний слой, чтобы определить наши новые классы, поэтому эта функция\n",
        "  добавляет правильные операции на график, а также некоторые переменные для хранения\n",
        "  весов, а затем устанавливает все градиенты для обратного прохода.\n",
        "  Настройка для softmax и полностью связанных слоев основана на:\n",
        "  https://www.tensorflow.org/tutorials/mnist/beginners/index.html\n",
        ">**Args:**\n",
        "* *class_count:* целое число от количества категорий вещей, которые мы пытаемся признать.\n",
        "* *final_tensor_name:* строка имени для нового конечного узла, который дает результаты.\n",
        "* *bottleneck_tensor:* вывод основного графа CNN.\n",
        "* *quantize_layer:* Boolean, указывающий, должен ли новый добавленный слой быть\n",
        "Инструментарий для квантования с TF-Lite.\n",
        "* *is_training:* Boolean, указывающий, предназначен ли новый слой для обучения\n",
        "или Eval.\n",
        "\n",
        ">**Return:**\n",
        "* Тензоры для результатов обучения и кросс-энтропии, а также тензоры для\n",
        "    вход узкого места и ввод правды земли.\n",
        "> Ссылки\n",
        "* [tf.truncated_normal](https://www.tensorflow.org/api_docs/python/tf/random/truncated_normal)\n",
        "* [про квантование в нейронных сетях](https://petewarden.com/2016/05/03/how-to-quantize-neural-networks-with-tensorflow/)\n",
        "* [tf.contrib.quantize.create_training_graph](https://www.tensorflow.org/api_docs/python/tf/contrib/quantize/create_training_graph)\n",
        "* [tf.contrib.quantize.create_eval_graph](https://www.tensorflow.org/api_docs/python/tf/contrib/quantize/create_eval_graph)"
      ]
    },
    {
      "metadata": {
        "id": "s0_ikdoH-4YO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def add_final_retrain_ops(class_count, final_tensor_name, bottleneck_tensor,\n",
        "                          quantize_layer, is_training):\n",
        "\n",
        "  batch_size, bottleneck_tensor_size = bottleneck_tensor.get_shape().as_list()\n",
        "  assert batch_size is None, 'We want to work with arbitrary batch size.'\n",
        "  with tf.name_scope('input'):\n",
        "    bottleneck_input = tf.placeholder_with_default(\n",
        "        bottleneck_tensor,\n",
        "        shape=[batch_size, bottleneck_tensor_size],\n",
        "        name='BottleneckInputPlaceholder')\n",
        "\n",
        "    ground_truth_input = tf.placeholder(\n",
        "        tf.int64, [batch_size], name='GroundTruthInput')\n",
        "\n",
        "  # Организация следующих операций, чтобы их было легче увидеть в TensorBoard.\n",
        "  layer_name = 'final_retrain_ops'\n",
        "  with tf.name_scope(layer_name):\n",
        "    with tf.name_scope('weights'):\n",
        "      initial_value = tf.truncated_normal(\n",
        "          [bottleneck_tensor_size, class_count], stddev=0.001)\n",
        "      layer_weights = tf.Variable(initial_value, name='final_weights')\n",
        "      variable_summaries(layer_weights)\n",
        "# tf.truncated_normal Вывод случайных значений из усеченного нормального распределения.\n",
        "    with tf.name_scope('biases'):\n",
        "      layer_biases = tf.Variable(tf.zeros([class_count]), name='final_biases')\n",
        "      variable_summaries(layer_biases)\n",
        "\n",
        "    with tf.name_scope('Wx_plus_b'):\n",
        "      logits = tf.matmul(bottleneck_input, layer_weights) + layer_biases\n",
        "      tf.summary.histogram('pre_activations', logits)\n",
        "\n",
        "  final_tensor = tf.nn.softmax(logits, name=final_tensor_name)\n",
        "\n",
        "# Функции tf.contrib.quantize переписывают график на месте для\n",
        "   # квантование. График импортированной модели уже переписан, поэтому после\n",
        "   # вызывая эти перезаписи, будет только добавленный последний слой\n",
        "   # преобразился\n",
        "  if quantize_layer:\n",
        "    if is_training:\n",
        "      tf.contrib.quantize.create_training_graph() #Переписывает обучающий input_graph для имитации квантования.\n",
        "    else:\n",
        "      tf.contrib.quantize.create_eval_graph()     #Переписывает eval input_graph на месте для симулированного квантования.\n",
        "\n",
        "\n",
        "\n",
        "  tf.summary.histogram('activations', final_tensor)\n",
        "\n",
        "  # If this is an eval graph, we don't need to add loss ops or an optimizer.\n",
        "  # Если это график eval, нам не нужно добавлять операции вычисления ошибки или оптимизатор.\n",
        "  if not is_training:\n",
        "    return None, None, bottleneck_input, ground_truth_input, final_tensor\n",
        "\n",
        "  with tf.name_scope('cross_entropy'):\n",
        "    cross_entropy_mean = tf.losses.sparse_softmax_cross_entropy(\n",
        "        labels=ground_truth_input, logits=logits)\n",
        "\n",
        "  tf.summary.scalar('cross_entropy', cross_entropy_mean)\n",
        "\n",
        "  with tf.name_scope('train'):\n",
        "    optimizer = tf.train.GradientDescentOptimizer(FLAGS.learning_rate)\n",
        "    train_step = optimizer.minimize(cross_entropy_mean)\n",
        "\n",
        "  return (train_step, cross_entropy_mean, bottleneck_input, ground_truth_input,\n",
        "          final_tensor)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WZalBRi4-_wB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **add_evaluation_step()**\n",
        "Вставляет операции, которые нам нужны, чтобы оценить точность наших результатов.\n",
        ">**Args:**\n",
        "* *result_tensor: *новый конечный узел, который дает результаты.\n",
        "* *ground_truth_tensor:* узел, который мы передаем данные истинности землив.\n",
        "\n",
        "> **Return**:\n",
        "* Кортеж (шаг оценки, прогноз).\n",
        "\n",
        ">Ссылки:\n",
        "* [tf.equal](https://www.tensorflow.org/api_docs/python/tf/math/equal)\n",
        "  \n",
        "  "
      ]
    },
    {
      "metadata": {
        "id": "yIYMSYK4_S5X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def add_evaluation_step(result_tensor, ground_truth_tensor):\n",
        "\n",
        "  with tf.name_scope('accuracy'):\n",
        "    with tf.name_scope('correct_prediction'):\n",
        "      prediction = tf.argmax(result_tensor, 1)\n",
        "      correct_prediction = tf.equal(prediction, ground_truth_tensor) #Возвращает значение истинности (x == y) поэлементно\n",
        "    with tf.name_scope('accuracy'):\n",
        "      evaluation_step = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "  tf.summary.scalar('accuracy', evaluation_step)\n",
        "  return evaluation_step, prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EauomXiA_WAN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **def run_final_eval()**\n",
        "Запускает окончательную оценку на графике оценки с использованием набора тестовых данных.\n",
        "> **Args:**\n",
        "* *train_session: *сессия для графа поездов с тензорами ниже.\n",
        "* *module_spec: *hub.ModuleSpec для используемого модуля изображения.\n",
        "* *class_count:* количество классов\n",
        "* *image_lists:* OrderedDict обучающих изображений для каждой метки.\n",
        "* *jpeg_data_tensor:* слой для подачи данных изображения jpeg.\n",
        "* *decoded_image_tensor:* вывод декодирования и изменения размера изображения.\n",
        "* *resized_image_tensor:* входной узел графа распознавания.\n",
        "* *bottleneck_tensor:* выходной слой узкого места графа CNN.\n"
      ]
    },
    {
      "metadata": {
        "id": "4A7hH_1O_wLf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def run_final_eval(train_session, module_spec, class_count, image_lists,\n",
        "                   jpeg_data_tensor, decoded_image_tensor,\n",
        "                   resized_image_tensor, bottleneck_tensor):\n",
        "\n",
        "  test_bottlenecks, test_ground_truth, test_filenames = (\n",
        "      get_random_cached_bottlenecks(train_session, image_lists,\n",
        "                                    FLAGS.test_batch_size,\n",
        "                                    'testing', FLAGS.bottleneck_dir,\n",
        "                                    FLAGS.image_dir, jpeg_data_tensor,\n",
        "                                    decoded_image_tensor, resized_image_tensor,\n",
        "                                    bottleneck_tensor, FLAGS.tfhub_module))\n",
        "\n",
        "  (eval_session, _, bottleneck_input, ground_truth_input, evaluation_step,\n",
        "   prediction) = build_eval_session(module_spec, class_count)\n",
        "  test_accuracy, predictions = eval_session.run(\n",
        "      [evaluation_step, prediction],\n",
        "      feed_dict={\n",
        "          bottleneck_input: test_bottlenecks,\n",
        "          ground_truth_input: test_ground_truth\n",
        "      })\n",
        "  tf.logging.info('Final test accuracy = %.1f%% (N=%d)' %\n",
        "                  (test_accuracy * 100, len(test_bottlenecks)))\n",
        "\n",
        "  if FLAGS.print_misclassified_test_images:\n",
        "    tf.logging.info('=== MISCLASSIFIED TEST IMAGES ===')\n",
        "    for i, test_filename in enumerate(test_filenames):\n",
        "      if predictions[i] != test_ground_truth[i]:\n",
        "        tf.logging.info('%70s  %s' % (test_filename,\n",
        "                                      list(image_lists.keys())[predictions[i]]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Qbb2MBDX_xa8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **build_eval_session()**\n",
        "Создает восстановленный сеанс eval без операций с поездами для экспорта.\n",
        "> **Args:**\n",
        "* *module_spec:*  hub.ModuleSpec для используемого модуля изображения.\n",
        "* *class_count:* количество классов\n",
        "\n",
        "> **Return**\n",
        "* Сессия Eval, содержащая восстановленный граф Eval.\n",
        "* Вход узкого места, истинность основания, шаг оценки и тензоры прогноза.\n",
        "\n",
        "  \n"
      ]
    },
    {
      "metadata": {
        "id": "Q_PWAHqwAQ1O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def build_eval_session(module_spec, class_count):\n",
        "\n",
        "  # Если квантовать, нам нужно создать правильный eval-граф для экспорта.\n",
        "  eval_graph, bottleneck_tensor, resized_input_tensor, wants_quantization = (\n",
        "      create_module_graph(module_spec))\n",
        "\n",
        "  eval_sess = tf.Session(graph=eval_graph)\n",
        "  with eval_graph.as_default():\n",
        "  # Добавить новый слой для экспорта.\n",
        "    (_, _, bottleneck_input,\n",
        "     ground_truth_input, final_tensor) = add_final_retrain_ops(\n",
        "         class_count, FLAGS.final_tensor_name, bottleneck_tensor,\n",
        "         wants_quantization, is_training=False)\n",
        "\n",
        "    # Теперь нам нужно восстановить значения из тренировочного графика в граф eval.\n",
        "    tf.train.Saver().restore(eval_sess, CHECKPOINT_NAME)\n",
        "\n",
        "    evaluation_step, prediction = add_evaluation_step(final_tensor,\n",
        "                                                      ground_truth_input)\n",
        "\n",
        "  return (eval_sess, resized_input_tensor, bottleneck_input, ground_truth_input,\n",
        "          evaluation_step, prediction)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xcU-vdlHAW9F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **def save_graph_to_file**\n",
        "Сохраняет график в файл, создавая действительный квантованный в случае необходимости."
      ]
    },
    {
      "metadata": {
        "id": "CB-724jtAepK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def save_graph_to_file(graph_file_name, module_spec, class_count):\n",
        "\n",
        "  sess, _, _, _, _, _ = build_eval_session(module_spec, class_count)\n",
        "  graph = sess.graph\n",
        "\n",
        "  output_graph_def = tf.graph_util.convert_variables_to_constants(\n",
        "      sess, graph.as_graph_def(), [FLAGS.final_tensor_name])\n",
        "\n",
        "  with tf.gfile.FastGFile(graph_file_name, 'wb') as f:\n",
        "    f.write(output_graph_def.SerializeToString())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dqZfKSpHAhml",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def prepare_file_system():\n",
        "  # Set up the directory we'll write summaries to for TensorBoard\n",
        "  if tf.gfile.Exists(FLAGS.summaries_dir):\n",
        "    tf.gfile.DeleteRecursively(FLAGS.summaries_dir)\n",
        "  tf.gfile.MakeDirs(FLAGS.summaries_dir)\n",
        "  if FLAGS.intermediate_store_frequency > 0:\n",
        "    ensure_dir_exists(FLAGS.intermediate_output_graphs_dir)\n",
        "  return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vLaOYUFWAmK2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **def add_jpeg_decoding()**\n",
        "Добавляет операции, которые выполняют JPEG-декодирование и изменение размера к графику.\n",
        "\n",
        "> **Args:**\n",
        "* *module_spec: *hub.ModuleSpec для используемого модуля изображения.\n",
        "\n",
        ">**Return**\n",
        "* Тензор для узла для подачи данных JPEG и вывода этапы предварительной обработки.\n"
      ]
    },
    {
      "metadata": {
        "id": "U_7yztk1A8dc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def add_jpeg_decoding(module_spec):\n",
        "\n",
        "  input_height, input_width = hub.get_expected_image_size(module_spec)\n",
        "  input_depth = hub.get_num_image_channels(module_spec)\n",
        "  jpeg_data = tf.placeholder(tf.string, name='DecodeJPGInput')\n",
        "  decoded_image = tf.image.decode_jpeg(jpeg_data, channels=input_depth)\n",
        "  # Convert from full range of uint8 to range [0,1] of float32.\n",
        "  decoded_image_as_float = tf.image.convert_image_dtype(decoded_image,\n",
        "                                                        tf.float32)\n",
        "  decoded_image_4d = tf.expand_dims(decoded_image_as_float, 0)\n",
        "  resize_shape = tf.stack([input_height, input_width])\n",
        "  resize_shape_as_int = tf.cast(resize_shape, dtype=tf.int32)\n",
        "  resized_image = tf.image.resize_bilinear(decoded_image_4d,\n",
        "                                           resize_shape_as_int)\n",
        "  return jpeg_data, resized_image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "el6lB0qCBRpZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **export_model()**\n",
        "Экспорт модели для сервировки.\n",
        "> **Args:**\n",
        "* *module_spec:* hub.ModuleSpec для используемого модуля изображения.\n",
        "* *class_count:* количество классов.\n",
        "* *save_model_dir:* каталог, в котором сохраняются экспортированная модель и переменные.\n",
        "  "
      ]
    },
    {
      "metadata": {
        "id": "HgzHHPy-BgPQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def export_model(module_spec, class_count, saved_model_dir):\n",
        "\n",
        "# SavedModel должен содержать граф eval.\n",
        "  sess, in_image, _, _, _, _ = build_eval_session(module_spec, class_count)\n",
        "  with sess.graph.as_default() as graph:\n",
        "    tf.saved_model.simple_save(\n",
        "        sess,\n",
        "        saved_model_dir,\n",
        "        inputs={'image': in_image},\n",
        "        outputs={'prediction': graph.get_tensor_by_name('final_result:0')},\n",
        "        legacy_init_op=tf.group(tf.tables_initializer(), name='legacy_init_op')\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2U1REzCTYT7t",
        "colab_type": "code",
        "outputId": "3c11dc1e-a389-4108-93ea-c24cfd7b45e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "def main(_):\n",
        "  # Необходимо убедиться, что вывод журнала виден.\n",
        "  # See https://github.com/tensorflow/tensorflow/issues/3047\n",
        "  tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "  if not FLAGS.image_dir:\n",
        "    tf.logging.error('Must set flag --image_dir.')\n",
        "    return -1\n",
        "\n",
        "# Подготовьте необходимые каталоги, которые можно использовать во время обучения\n",
        "  prepare_file_system()\n",
        "\n",
        "# Посмотрите на структуру папок и создайте списки всех изображений.\n",
        "  image_lists = create_image_lists(FLAGS.image_dir, FLAGS.testing_percentage,\n",
        "                                   FLAGS.validation_percentage)\n",
        "  class_count = len(image_lists.keys())\n",
        "  if class_count == 0:\n",
        "    tf.logging.error('No valid folders of images found at ' + FLAGS.image_dir)\n",
        "    return -1\n",
        "  if class_count == 1:\n",
        "    tf.logging.error('Only one valid folder of images found at ' +\n",
        "                     FLAGS.image_dir +\n",
        "                     ' - multiple classes are needed for classification.')\n",
        "    return -1\n",
        "\n",
        "# Проверьте, означают ли флаги командной строки, что мы применяем какие-либо искажения.\n",
        "  do_distort_images = should_distort_images(\n",
        "      FLAGS.flip_left_right, FLAGS.random_crop, FLAGS.random_scale,\n",
        "      FLAGS.random_brightness)\n",
        "\n",
        "# Настройте предварительно обученный график.\n",
        "  module_spec = hub.load_module_spec(FLAGS.tfhub_module)\n",
        "  graph, bottleneck_tensor, resized_image_tensor, wants_quantization = (\n",
        "      create_module_graph(module_spec))\n",
        "\n",
        "# Добавьте новый слой, который мы будем тренировать.\n",
        "  with graph.as_default():\n",
        "    (train_step, cross_entropy, bottleneck_input,\n",
        "     ground_truth_input, final_tensor) = add_final_retrain_ops(\n",
        "         class_count, FLAGS.final_tensor_name, bottleneck_tensor,\n",
        "         wants_quantization, is_training=True)\n",
        "\n",
        "  with tf.Session(graph=graph) as sess:\n",
        "     # Инициализировать все веса: для модуля их предварительно обученные значения,\n",
        "     # и для вновь добавленного слоя переподготовки к случайным начальным значениям.\n",
        "    init = tf.global_variables_initializer()\n",
        "    sess.run(init)\n",
        "\n",
        "# Настройка подграфа декодирования изображения.\n",
        "    jpeg_data_tensor, decoded_image_tensor = add_jpeg_decoding(module_spec)\n",
        "\n",
        "    if do_distort_images:\n",
        "      # Мы будем применять искажения, поэтому настройте нужные нам операции.\n",
        "      (distorted_jpeg_data_tensor,\n",
        "       distorted_image_tensor) = add_input_distortions(\n",
        "           FLAGS.flip_left_right, FLAGS.random_crop, FLAGS.random_scale,\n",
        "           FLAGS.random_brightness, module_spec)\n",
        "    else:\n",
        "      # Мы позаботимся о том, чтобы мы вычислили краткие изображения и\n",
        "       # кэшировал их на диск.\n",
        "      cache_bottlenecks(sess, image_lists, FLAGS.image_dir,\n",
        "                        FLAGS.bottleneck_dir, jpeg_data_tensor,\n",
        "                        decoded_image_tensor, resized_image_tensor,\n",
        "                        bottleneck_tensor, FLAGS.tfhub_module)\n",
        "\n",
        "    # Создайте операции, которые нам нужны, чтобы оценить точность нашего нового слоя.\n",
        "    evaluation_step, _ = add_evaluation_step(final_tensor, ground_truth_input)\n",
        "\n",
        "    # Объединить все сводки и записать их в файл резюме.\n",
        "    merged = tf.summary.merge_all()\n",
        "    train_writer = tf.summary.FileWriter(FLAGS.summaries_dir + '/train',\n",
        "                                         sess.graph)\n",
        "\n",
        "    validation_writer = tf.summary.FileWriter(\n",
        "        FLAGS.summaries_dir + '/validation')\n",
        "\n",
        "     # Создать заставку поезда, которая используется для восстановления значений в граф Eval\n",
        "     # при экспорте моделей.\n",
        "    train_saver = tf.train.Saver()\n",
        "\n",
        "# Запустите обучение столько раз, сколько требуется в командной строке.\n",
        "    for i in range(FLAGS.how_many_training_steps):\n",
        "# Получить пакет входных значений узких мест, каждый из которых рассчитывается заново каждый раз\n",
        "       # время с примененными искажениями или из кеша, хранящегося на диске.\n",
        "      if do_distort_images:\n",
        "        (train_bottlenecks,\n",
        "         train_ground_truth) = get_random_distorted_bottlenecks(\n",
        "             sess, image_lists, FLAGS.train_batch_size, 'training',\n",
        "             FLAGS.image_dir, distorted_jpeg_data_tensor,\n",
        "             distorted_image_tensor, resized_image_tensor, bottleneck_tensor)\n",
        "      else:\n",
        "        (train_bottlenecks,\n",
        "         train_ground_truth, _) = get_random_cached_bottlenecks(\n",
        "             sess, image_lists, FLAGS.train_batch_size, 'training',\n",
        "             FLAGS.bottleneck_dir, FLAGS.image_dir, jpeg_data_tensor,\n",
        "             decoded_image_tensor, resized_image_tensor, bottleneck_tensor,\n",
        "             FLAGS.tfhub_module)\n",
        "# Вставьте узкие места и правду в график и проведите тренинг\n",
        "       # шаг. Захватите учебные резюме для TensorBoard с опцией `merged`\n",
        "      train_summary, _ = sess.run(\n",
        "          [merged, train_step],\n",
        "          feed_dict={bottleneck_input: train_bottlenecks,\n",
        "                     ground_truth_input: train_ground_truth})\n",
        "      train_writer.add_summary(train_summary, i)\n",
        "\n",
        "# Время от времени распечатывайте, насколько хорошо тренируется график.\n",
        "      is_last_step = (i + 1 == FLAGS.how_many_training_steps)\n",
        "      if (i % FLAGS.eval_step_interval) == 0 or is_last_step:\n",
        "        train_accuracy, cross_entropy_value = sess.run(\n",
        "            [evaluation_step, cross_entropy],\n",
        "            feed_dict={bottleneck_input: train_bottlenecks,\n",
        "                       ground_truth_input: train_ground_truth})\n",
        "        tf.logging.info('%s: Step %d: Train accuracy = %.1f%%' %\n",
        "                        (datetime.now(), i, train_accuracy * 100))\n",
        "        tf.logging.info('%s: Step %d: Cross entropy = %f' %\n",
        "                        (datetime.now(), i, cross_entropy_value))\n",
        "# TODO: используйте этот граф eval, чтобы избежать квантования\n",
        "         # скользящие средние, обновляемые набором проверки, хотя в\n",
        "         # Практика это делает незначительную разницу.\n",
        "        validation_bottlenecks, validation_ground_truth, _ = (\n",
        "            get_random_cached_bottlenecks(\n",
        "                sess, image_lists, FLAGS.validation_batch_size, 'validation',\n",
        "                FLAGS.bottleneck_dir, FLAGS.image_dir, jpeg_data_tensor,\n",
        "                decoded_image_tensor, resized_image_tensor, bottleneck_tensor,\n",
        "                FLAGS.tfhub_module))\n",
        "# Запустите шаг проверки и запишите результаты обучения для TensorBoard\n",
        "         # с \"merged\" операцией\n",
        "        validation_summary, validation_accuracy = sess.run(\n",
        "            [merged, evaluation_step],\n",
        "            feed_dict={bottleneck_input: validation_bottlenecks,\n",
        "                       ground_truth_input: validation_ground_truth})\n",
        "        validation_writer.add_summary(validation_summary, i)\n",
        "        tf.logging.info('%s: Step %d: Validation accuracy = %.1f%% (N=%d)' %\n",
        "                        (datetime.now(), i, validation_accuracy * 100,\n",
        "                         len(validation_bottlenecks)))\n",
        "\n",
        "# Хранить промежуточные результаты\n",
        "      intermediate_frequency = FLAGS.intermediate_store_frequency\n",
        "\n",
        "      if (intermediate_frequency > 0 and (i % intermediate_frequency == 0)\n",
        "          and i > 0):\n",
        "       # Если мы хотим сделать промежуточное сохранение, сохраните контрольную точку поезда\n",
        "       # graph, для восстановления в граф eval.\n",
        "        train_saver.save(sess, CHECKPOINT_NAME)\n",
        "        intermediate_file_name = (FLAGS.intermediate_output_graphs_dir +\n",
        "                                  'intermediate_' + str(i) + '.pb')\n",
        "        tf.logging.info('Save intermediate result to : ' +\n",
        "                        intermediate_file_name)\n",
        "        save_graph_to_file(intermediate_file_name, module_spec,\n",
        "                           class_count)\n",
        "\n",
        "# По окончании тренировки выполните последнее сохранение контрольной точки поезда.\n",
        "    train_saver.save(sess, CHECKPOINT_NAME)\n",
        "\n",
        "# Мы завершили все наши тренинги, поэтому проведите итоговую тестовую оценку\n",
        "     # некоторые новые изображения, которые мы не использовали раньше.\n",
        "    run_final_eval(sess, module_spec, class_count, image_lists,\n",
        "                   jpeg_data_tensor, decoded_image_tensor, resized_image_tensor,\n",
        "                   bottleneck_tensor)\n",
        "\n",
        "# Запишите обученный график и метки с весами, сохраненными как\n",
        "     # константы.\n",
        "    tf.logging.info('Save final result to : ' + FLAGS.output_graph)\n",
        "    if wants_quantization:\n",
        "      tf.logging.info('The model is instrumented for quantization with TF-Lite')\n",
        "    save_graph_to_file(FLAGS.output_graph, module_spec, class_count)\n",
        "    with tf.gfile.FastGFile(FLAGS.output_labels, 'w') as f:\n",
        "      f.write('\\n'.join(image_lists.keys()) + '\\n')\n",
        "\n",
        "    if FLAGS.saved_model_dir:\n",
        "      export_model(module_spec, class_count, FLAGS.saved_model_dir)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  parser = argparse.ArgumentParser()\n",
        "  parser.add_argument(\n",
        "      '--image_dir',\n",
        "      type=str,\n",
        "      default='',\n",
        "      help='Path to folders of labeled images.'\n",
        "  )\n",
        "  parser.add_argument(\n",
        "      '--output_graph',\n",
        "      type=str,\n",
        "      default='/tmp/output_graph.pb',\n",
        "      help='Where to save the trained graph.'\n",
        "  )\n",
        "  parser.add_argument(\n",
        "      '--intermediate_output_graphs_dir',\n",
        "      type=str,\n",
        "      default='/tmp/intermediate_graph/',\n",
        "      help='Where to save the intermediate graphs.'\n",
        "  )\n",
        "  parser.add_argument(\n",
        "      '--intermediate_store_frequency',\n",
        "      type=int,\n",
        "      default=0,\n",
        "      help=\"\"\"\\\n",
        "         How many steps to store intermediate graph. If \"0\" then will not\n",
        "         store.\\\n",
        "      \"\"\"\n",
        "  )\n",
        "  parser.add_argument(\n",
        "      '--output_labels',\n",
        "      type=str,\n",
        "      default='/tmp/output_labels.txt',\n",
        "      help='Where to save the trained graph\\'s labels.'\n",
        "  )\n",
        "  parser.add_argument(\n",
        "      '--summaries_dir',\n",
        "      type=str,\n",
        "      default='/tmp/retrain_logs',\n",
        "      help='Where to save summary logs for TensorBoard.'\n",
        "  )\n",
        "  parser.add_argument(\n",
        "      '--how_many_training_steps',\n",
        "      type=int,\n",
        "      default=4000,\n",
        "      help='How many training steps to run before ending.'\n",
        "  )\n",
        "  parser.add_argument(\n",
        "      '--learning_rate',\n",
        "      type=float,\n",
        "      default=0.01,\n",
        "      help='How large a learning rate to use when training.'\n",
        "  )\n",
        "  parser.add_argument(\n",
        "      '--testing_percentage',\n",
        "      type=int,\n",
        "      default=10,\n",
        "      help='What percentage of images to use as a test set.'\n",
        "  )\n",
        "  parser.add_argument(\n",
        "      '--validation_percentage',\n",
        "      type=int,\n",
        "      default=10,\n",
        "      help='What percentage of images to use as a validation set.'\n",
        "  )\n",
        "  parser.add_argument(\n",
        "      '--eval_step_interval',\n",
        "      type=int,\n",
        "      default=10,\n",
        "      help='How often to evaluate the training results.'\n",
        "  )\n",
        "  parser.add_argument(\n",
        "      '--train_batch_size',\n",
        "      type=int,\n",
        "      default=100,\n",
        "      help='How many images to train on at a time.'\n",
        "  )\n",
        "  parser.add_argument(\n",
        "      '--test_batch_size',\n",
        "      type=int,\n",
        "      default=-1,\n",
        "      help=\"\"\"\\\n",
        "     Сколько изображений для тестирования. Этот набор тестов используется только один раз, чтобы оценить\n",
        "       окончательная точность модели после обучения завершена.\n",
        "       Значение -1 приводит к использованию всего набора тестов, что приводит к увеличению\n",
        "       стабильные результаты по пробегам. \\\n",
        "      \"\"\"\n",
        "  )\n",
        "  parser.add_argument(\n",
        "      '--validation_batch_size',\n",
        "      type=int,\n",
        "      default=100,\n",
        "      help=\"\"\"\\\n",
        " Сколько изображений использовать в оценочной партии. Этот набор проверки\n",
        "       используется гораздо чаще, чем набор тестов, и является ранним индикатором того, как\n",
        "       Точная модель во время обучения.\n",
        "       Значение -1 приводит к использованию всего набора проверки, что приводит к\n",
        "       более стабильные результаты на протяжении итераций обучения, но могут быть медленнее на больших\n",
        "       тренировочные наборы. \\\n",
        "      \"\"\"\n",
        "  )\n",
        "  parser.add_argument(\n",
        "      '--print_misclassified_test_images',\n",
        "      default=False,\n",
        "      help=\"\"\"\\\n",
        "      Whether to print out a list of all misclassified test images.\\\n",
        "      \"\"\",\n",
        "      action='store_true'\n",
        "  )\n",
        "  parser.add_argument(\n",
        "      '--bottleneck_dir',\n",
        "      type=str,\n",
        "      default='/tmp/bottleneck',\n",
        "      help='Path to cache bottleneck layer values as files.'\n",
        "  )\n",
        "  parser.add_argument(\n",
        "      '--final_tensor_name',\n",
        "      type=str,\n",
        "      default='final_result',\n",
        "      help=\"\"\"\\\n",
        "      The name of the output classification layer in the retrained graph.\\\n",
        "      \"\"\"\n",
        "  )\n",
        "  parser.add_argument(\n",
        "      '--flip_left_right',\n",
        "      default=False,\n",
        "      help=\"\"\"\\\n",
        "      Whether to randomly flip half of the training images horizontally.\\\n",
        "      \"\"\",\n",
        "      action='store_true'\n",
        "  )\n",
        "  parser.add_argument(\n",
        "      '--random_crop',\n",
        "      type=int,\n",
        "      default=0,\n",
        "      help=\"\"\"\\\n",
        "      A percentage determining how much of a margin to randomly crop off the\n",
        "      training images.\\\n",
        "      \"\"\"\n",
        "  )\n",
        "  parser.add_argument(\n",
        "      '--random_scale',\n",
        "      type=int,\n",
        "      default=0,\n",
        "      help=\"\"\"\\\n",
        "      A percentage determining how much to randomly scale up the size of the\n",
        "      training images by.\\\n",
        "      \"\"\"\n",
        "  )\n",
        "  parser.add_argument(\n",
        "      '--random_brightness',\n",
        "      type=int,\n",
        "      default=0,\n",
        "      help=\"\"\"\\\n",
        "      A percentage determining how much to randomly multiply the training image\n",
        "      input pixels up or down by.\\\n",
        "      \"\"\"\n",
        "  )\n",
        "  parser.add_argument(\n",
        "      '--tfhub_module',\n",
        "      type=str,\n",
        "      default=(\n",
        "          'https://tfhub.dev/google/imagenet/inception_v3/feature_vector/1'),\n",
        "      help=\"\"\"\\\n",
        "      Which TensorFlow Hub module to use.\n",
        "      See https://github.com/tensorflow/hub/blob/master/docs/modules/image.md\n",
        "      for some publicly available ones.\\\n",
        "      \"\"\")\n",
        "  parser.add_argument(\n",
        "      '--saved_model_dir',\n",
        "      type=str,\n",
        "      default='',\n",
        "      help='Where to save the exported graph.')\n",
        "  FLAGS, unparsed = parser.parse_known_args()\n",
        "  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERROR:tensorflow:Must set flag --image_dir.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m -1\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "bN4Z54QUx753",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    }
  ]
}